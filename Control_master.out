20
Attention! AWS2cosipy contains a temperature lapse rate that is not zero. 
Please be sure the input file is not constructed with a lapse rate.
Attention! AWS2cosipy contains a precipitation lapse rate that is not zero. 
Please be sure the input file is not constructed with a lapse rate.
--------------------------------------------------------------
	 RESTART FROM PREVIOUS STATE
-------------------------------------------------------------- 


 Maximum available time interval from 1999-10-01T00:00 until 2018-09-30T23:00. Time steps: 166560 


Starting from 2010-09-30T01:00:00.000000000 (from restart file) to 2018-09-30T00:00 (from config.py) 

--------------------------------------------------------------
Checking input data .... 

Temperature data (T2) ... ok 
Relative humidity data (RH2) ... ok 
Shortwave data (G) ... ok 
Wind velocity data (U2) ... ok 
Precipitation data (RRR) ... ok 
Pressure data (PRES) ... ok 
Incoming longwave data (LWin) ... ok 
Please check the input data, its seems they are out of range Lwin MAX: 360.93 MIN: 86.83 


 Glacier gridpoints: 1819 




Output dataset ... ok
Restart ddataset ... ok 

--------------------------------------------------------------

Starting run with lapse rates: -0.0061 and: 0.0009
#!/bin/bash

#!/usr/bin/env bash
#SBATCH -J WoEraAbr
#SBATCH -n 1
#SBATCH --cpus-per-task=1
#SBATCH --mem=56G
#SBATCH -t 00:30:00
#SBATCH --qos=short
#SBATCH --output=Output_nodes.output
#SBATCH --error=Error_nodes.err
#SBATCH --time=1-00:00:00
#SBATCH --account=morsanat
JOB_ID=${SLURM_JOB_ID%;*}



/programs/anaconda/2019.07/bin/python -m distributed.cli.dask_worker tcp://192.168.1.9:8786 --nthreads 0 --nprocs 20 --memory-limit 3.00GB --name WoEraAbr--${JOB_ID}-- --death-timeout 60 --local-directory logs/dask-worker-space

You are using SLURM!

SLURMCluster(cores=0, memory=0 B, workers=0/0, jobs=0/0)
--------------------------------------------------------------
	 Starting clients and submit jobs ... 

-------------------------------------------------------------- 

SLURMCluster(cores=0, memory=0 B, workers=0/240, jobs=0/12)
<Client: scheduler='tcp://192.168.1.9:8786' processes=0 cores=0>
5368 240 22
	 Time required to do calculations:   41 minutes 15.1723 seconds 



--------------------------------------------------------------
Write results ...
-------------------------------------------------------------- 

Trying to concatenate files. Requires some time.
['T2', 'RH2', 'U2']
['PRES', 'G', 'RRR']
['N', 'LWin', 'MB']
['SNOWHEIGHT']
ds_merged_1
PRES
G
RRR
ds_merged_2
N
LWin
MB
ds_merged_3
SNOWHEIGHT
LS_DATE
TSLA Observed vs Modelled RMSE: 139.08443658182657; R-squared: 0.5715597598824839
	 Time required to write restart and output files:   15 minutes 34.0573 seconds 

	 Total run duration:   62 minutes 55.7583 seconds 

--------------------------------------------------------------
	 SIMULATION WAS SUCCESSFUL
--------------------------------------------------------------
--------------------------------------------------------------
	 RESTART FROM PREVIOUS STATE
-------------------------------------------------------------- 


 Maximum available time interval from 1999-10-01T00:00 until 2018-09-30T23:00. Time steps: 166560 


Starting from 2010-09-30T01:00:00.000000000 (from restart file) to 2018-09-30T00:00 (from config.py) 

--------------------------------------------------------------
Checking input data .... 

Temperature data (T2) ... ok 
Relative humidity data (RH2) ... ok 
Shortwave data (G) ... ok 
Wind velocity data (U2) ... ok 
Precipitation data (RRR) ... ok 
Pressure data (PRES) ... ok 
Incoming longwave data (LWin) ... ok 
Please check the input data, its seems they are out of range Lwin MAX: 360.93 MIN: 86.83 


 Glacier gridpoints: 1819 




Output dataset ... ok
Restart ddataset ... ok 

--------------------------------------------------------------

Starting run with lapse rates: -0.0061 and: 0.0013
#!/bin/bash

#!/usr/bin/env bash
#SBATCH -J WoEraAbr
#SBATCH -n 1
#SBATCH --cpus-per-task=1
#SBATCH --mem=56G
#SBATCH -t 00:30:00
#SBATCH --qos=short
#SBATCH --output=Output_nodes.output
#SBATCH --error=Error_nodes.err
#SBATCH --time=1-00:00:00
#SBATCH --account=morsanat
JOB_ID=${SLURM_JOB_ID%;*}



/programs/anaconda/2019.07/bin/python -m distributed.cli.dask_worker tcp://192.168.1.9:8786 --nthreads 0 --nprocs 20 --memory-limit 3.00GB --name WoEraAbr--${JOB_ID}-- --death-timeout 60 --local-directory logs/dask-worker-space

You are using SLURM!

SLURMCluster(cores=0, memory=0 B, workers=0/0, jobs=0/0)
--------------------------------------------------------------
	 Starting clients and submit jobs ... 

-------------------------------------------------------------- 

SLURMCluster(cores=0, memory=0 B, workers=0/240, jobs=0/12)
<Client: scheduler='tcp://192.168.1.9:8786' processes=0 cores=0>
5368 240 22
	 Time required to do calculations:   36 minutes 56.2112 seconds 



--------------------------------------------------------------
Write results ...
-------------------------------------------------------------- 

Trying to concatenate files. Requires some time.
['T2', 'RH2', 'U2']
['PRES', 'G', 'RRR']
['N', 'LWin', 'MB']
['SNOWHEIGHT']
ds_merged_1
PRES
G
RRR
ds_merged_2
N
LWin
MB
ds_merged_3
SNOWHEIGHT
LS_DATE
TSLA Observed vs Modelled RMSE: 166.03408672579053; R-squared: 0.5264781790174528
	 Time required to write restart and output files:   15 minutes 28.6514 seconds 

	 Total run duration:  121 minutes 38.0691 seconds 

--------------------------------------------------------------
	 SIMULATION WAS SUCCESSFUL
--------------------------------------------------------------
