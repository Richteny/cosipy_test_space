{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import salem\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "## Script requires a stored archive of the GEE-derived Landsat albedos\n",
    "\n",
    "if 'win' in sys.platform:\n",
    "    path_tif = \"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/HEF_Landsat_Albedo/\"\n",
    "    path_shp = \"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Static/\"\n",
    "    aws_path = \"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/AWS_Obleitner/\"\n",
    "else:\n",
    "    path_tif = \"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/HEF_Landsat_Albedo/\"\n",
    "    path_shp = \"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Static/\"\n",
    "    aws_path = \"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/AWS_Obleitner/\"\n",
    "    \n",
    "generate_file = False\n",
    "\n",
    "if generate_file:\n",
    "    import rioxarray as rxr\n",
    "    from rasterio.enums import Resampling\n",
    "\n",
    "    import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recorded original coordinates could not be converted into lat/lon coordinates d$\n",
    "#Reconstructed positions from later recorded GPS points of AWS locations (+/-100$\n",
    "#HEF lower: 46.813570째 N; 10.788977째 E; 2640 m\n",
    "#HEF upper: 46.790453째 N; 10.747121째 E; 3048 m\n",
    "\n",
    "aws_lower = pd.read_csv(aws_path+\"Fix_HEFlower_01102003_24102004.csv\", parse_dates=True, index_col=\"time\")\n",
    "aws_upper = pd.read_csv(aws_path+\"Fix_HEFupper_01102003_24102004.csv\", parse_dates=True, index_col=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_lower = aws_lower.resample(\"1D\").agg(T=('T', \"mean\"),Dir=('Dir', \"mean\"),U=('U', \"mean\"),SWIsum=('SWI', \"sum\"), SWImean=(\"SWI\",\"mean\"),SWOsum=('SWO', \"sum\"),SWOmean=(\"SWO\",\"mean\"),\n",
    "                                         LWO=('LWO','mean'),LWI=('LWI','mean'),sfc=('sfc', \"mean\"),RH= ('RH', \"mean\"),P=('P', \"mean\"))\n",
    "aws_lower['alpha'] = aws_lower['SWOsum'] / aws_lower['SWIsum']\n",
    "aws_lower['sfc'] =  aws_lower['sfc'] - aws_lower['sfc'][0]\n",
    "\n",
    "aws_upper = aws_upper.resample(\"1D\").agg(T=('T', \"mean\"),Dir=('Dir', \"mean\"),U=('U', \"mean\"),SWIsum=('SWI', \"sum\"), SWImean=(\"SWI\",\"mean\"),SWOsum=('SWO', \"sum\"),SWOmean=(\"SWO\",\"mean\"),\n",
    "                                         LWO=('LWO','mean'),LWI=('LWI','mean'),sfc=('sfc', \"mean\"),RH= ('RH', \"mean\"),P=('P', \"mean\"))\n",
    "aws_upper['alpha'] = aws_upper['SWOsum'] / aws_upper['SWIsum']\n",
    "aws_upper['sfc'] = aws_upper['sfc'] - aws_upper['sfc'][0]\n",
    "aws_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dfs = []\n",
    "## Load MetaDataFile and Merge\n",
    "for fp in pathlib.Path(path_tif).glob('*.csv'):\n",
    "    print(fp)\n",
    "    df = pd.read_csv(fp)\n",
    "    list_dfs.append(df)\n",
    "    \n",
    "metadata = pd.concat(list_dfs, ignore_index=True)\n",
    "metadata\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['time'] = metadata['NAME'].apply(lambda x: pd.to_datetime(x.split('193028_')[-1]))\n",
    "\n",
    "metadata = metadata.set_index('time')\n",
    "metadata_ds = metadata.to_xarray()\n",
    "\n",
    "metadata_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_albedo_file(filename, shape):\n",
    "    alb = salem.open_xr_dataset(filename)\n",
    "    alb = alb.salem.roi(shape=shape)\n",
    "    \n",
    "    return alb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_file:\n",
    "\n",
    "    ## Load Example Albedo file\n",
    "    data = salem.open_xr_dataset(path_tif+\"LE07_193028_20000204.tif\")\n",
    "    print(data)\n",
    "    #data.data.plot()\n",
    "\n",
    "    # Load static file and RGI6.0 shapefile\n",
    "    rgi6 = salem.read_shapefile(path_shp+\"/RGI6/HEF_RGI6.shp\")\n",
    "    print(rgi6.crs)\n",
    "    rgi6_reproj = rgi6.to_crs(data.pyproj_srs)\n",
    "    #rgi6_reproj.plot()\n",
    "\n",
    "    print(data.salem.grid)\n",
    "    print(data)\n",
    "\n",
    "    #reprojected using QGIS!\n",
    "    test = salem.open_xr_dataset(path_shp+\"reproj_nasadem_test.tif\")\n",
    "    nasadem_reproj = test.salem.subset(shape=rgi6_reproj)\n",
    "    nasadem_reproj_crop = nasadem_reproj.interp(x=data.x, y=data.y, method=\"nearest\")\n",
    "    nasadem_reproj_crop = nasadem_reproj_crop.salem.roi(shape=rgi6_reproj)\n",
    "    nasadem_reproj_crop.data.plot()\n",
    "\n",
    "    print(nasadem_reproj_crop.salem.grid)\n",
    "    print(data.salem.grid)\n",
    "\n",
    "    # Mask the dataset using the shapefile\n",
    "    masked_albedo = data.salem.roi(shape=rgi6_reproj)\n",
    "    masked_albedo['HGT'] = nasadem_reproj_crop.data\n",
    "    masked_albedo\n",
    "\n",
    "    datasets = []\n",
    "    data_sources = []\n",
    "\n",
    "    for fp in pathlib.Path(path_tif).glob('*.tif'):\n",
    "        filename = str(fp.stem)\n",
    "        print(filename)\n",
    "        alb = process_albedo_file(fp, shape=rgi6_reproj)\n",
    "        \n",
    "        sensor = filename.split('_')[0]\n",
    "        date = filename.split('_')[-1]\n",
    "        timestamp = date[0:4]+'-'+date[4:6]+'-'+date[6:8]\n",
    "        ts = pd.to_datetime(timestamp)\n",
    "        \n",
    "        alb = alb.expand_dims(time=[ts])\n",
    "        alb = alb.assign_coords(sensor_type=(\"time\", [sensor]))\n",
    "        \n",
    "        datasets.append(alb)\n",
    "        data_sources.append(sensor)\n",
    "        \n",
    "    alb_dataset = xr.concat(datasets, dim=\"time\")\n",
    "\n",
    "    alb_dataset = alb_dataset.rename({\"data\": \"albedo\"})\n",
    "    alb_dataset['month'] = alb_dataset.time.dt.month\n",
    "    alb_dataset['HGT'] = nasadem_reproj_crop.data\n",
    "    alb_dataset['albedo'] = alb_dataset['albedo'].where(alb_dataset['albedo'] != -999, np.nan)\n",
    "    if 'win' in sys.platform:\n",
    "        alb_dataset.to_netcdf(r\"E:\\OneDrive\\PhD\\PhD\\Data\\Hintereisferner\\Climate\\HEF_processed_albedos.nc\")\n",
    "    else:\n",
    "        alb_dataset.to_netcdf(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/HEF_processed_albedos.nc\")\n",
    "else:\n",
    "    if 'win' in sys.platform:\n",
    "        alb_dataset = xr.open_dataset(r\"E:\\OneDrive\\PhD\\PhD\\Data\\Hintereisferner\\Climate\\HEF_processed_albedos.nc\")\n",
    "    else:\n",
    "        alb_dataset = xr.open_dataset(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/HEF_processed_albedos.nc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alb_dataset = alb_dataset.sortby('time')\n",
    "## Merge metadata\n",
    "alb_dataset['CLOUDCOVER'] = metadata_ds['CLOUDCOVER']\n",
    "alb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alb_dataset.HGT.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_histogram(dataset, bins=50):\n",
    "    timestamp = str(dataset.time.values).split('T')[0]\n",
    "    sensor = str(dataset.sensor_type.values)\n",
    "    albedo = dataset.albedo.values.flatten()\n",
    "    albedo = albedo[~np.isnan(albedo)]\n",
    "    \n",
    "    bins = np.linspace(albedo.min(), albedo.max(), 50)  # Adjust the number of bins\n",
    "    hist, bin_edges = np.histogram(albedo, bins=bins)\n",
    "    \n",
    "    fig, ax = plt.subplots(3,1, figsize=(16,9), dpi=300)\n",
    "    ax[0].hist(albedo, bins=bin_edges, edgecolor=\"black\", alpha=0.7)\n",
    "    ax[0].set_title(f'Albedo Values on {sensor} {timestamp}')\n",
    "    ax[0].set_xticks(np.arange(0,1+0.1,0.1))\n",
    "    ax[0].set_xlim(0,1)\n",
    "    ax[0].set_ylabel('Frequency')\n",
    "    ax[0].grid(True)\n",
    "       \n",
    "    # Add CDF plot\n",
    "    sorted_data = np.sort(albedo)\n",
    "    cdf = np.cumsum(sorted_data) / np.sum(sorted_data)\n",
    "\n",
    "    percentile_5 = np.percentile(sorted_data, 5)\n",
    "    percentile_95 = np.percentile(sorted_data, 95)\n",
    "\n",
    "    ax[1].plot(sorted_data, cdf, color='blue', label='CDF')\n",
    "    ax[1].axvline(percentile_5, color='red', linestyle='--', label=f'5th Percentile ({percentile_5:.2f})')\n",
    "    ax[1].axvline(percentile_95, color='green', linestyle='--', label=f'95th Percentile ({percentile_95:.2f})')\n",
    "    ax[1].set_xticks(np.arange(0,1+0.1,0.1))\n",
    "    ax[1].set_xlim(0,1)\n",
    "    ax[1].set_xlabel('Albedo (-)')\n",
    "    ax[1].set_ylabel('Cumulative Probability')\n",
    "    ax[1].legend()\n",
    "    ax[1].grid(True)\n",
    "\n",
    "\n",
    "    c = ax[2].pcolormesh(dataset['x'], dataset['y'], dataset['albedo'].squeeze(), cmap='viridis', shading='auto', vmin=0, vmax=1)\n",
    "    ax[2].set_xlabel('X-Coordinate (m)')\n",
    "    ax[2].set_ylabel('Y-Coordinate (m)')\n",
    "\n",
    "    # Add a colorbar to the pcolormesh plot\n",
    "    fig.colorbar(c, ax=ax[2])\n",
    "    # Adjust layout to avoid overlapping\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.loc[metadata['system:index'] == \"LE07_193028_20000119\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.SCENE_CENTER_TIME.values #basically all scenes taken around 10h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Load HORAYZON MASK\n",
    "if 'win' in sys.platform:\n",
    "    horayzon = salem.open_xr_dataset(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Static/HEF_HORAYZON-LUT_30m.nc\")\n",
    "else:\n",
    "    horayzon = salem.open_xr_dataset(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Static/HEF_HORAYZON-LUT_30m.nc\")\n",
    "horayzon.sel(time=\"2020-01-19T10:00:00\").sw_dir_cor.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "horayzon.rio.set_spatial_dims('lon', 'lat', inplace=True)\n",
    "horayzon.rio.write_crs(horayzon.pyproj_srs, inplace=True)\n",
    "test = horayzon.rio.reproject(alb_dataset.pyproj_srs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.sel(time=\"2020-01-19T10:00:00\").sw_dir_cor.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test horayzon mask\n",
    "img_hrz = test.sel(time=\"2020-01-19T10:00:00\").sw_dir_cor\n",
    "img_alb = alb_dataset.isel(time=[1]).albedo\n",
    "\n",
    "img_hrz_aligned = img_hrz.reindex_like(img_alb, method=\"nearest\")  # or method=\"pad\"\n",
    "img_alb_masked = img_alb.where(img_hrz_aligned != 0, np.nan)\n",
    "\n",
    "img_alb_masked.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_histogram(img_alb_masked.to_dataset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_histogram(alb_dataset.isel(time=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## full histogram over all timesteps\n",
    "albedo_vals_full = alb_dataset.albedo.values.flatten()\n",
    "albedo_vals_full = albedo_vals_full[~np.isnan(albedo_vals_full)]\n",
    "\n",
    "print(albedo_vals_full.min(), albedo_vals_full.max())\n",
    "bins = np.linspace(albedo_vals_full.min(), albedo_vals_full.max(), 100)  # Adjust the number of bins\n",
    "hist, bin_edges = np.histogram(albedo_vals_full, bins=bins)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(16,9), dpi=300)\n",
    "ax.hist(albedo_vals_full, bins=bin_edges, edgecolor=\"black\", alpha=0.7)\n",
    "ax.set_title(f'Albedo Values over all timesteps')\n",
    "ax.set_xticks(np.arange(0,1+0.1,0.1))\n",
    "ax.set_xlim(0,1)\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean Data with HORAYZON and repeat figures\n",
    "hrz_aligned = test.reindex({'y': alb_dataset['y'], 'x': alb_dataset['x']}, method=\"nearest\")\n",
    "hrz_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albedo_copy = alb_dataset.copy()\n",
    "alb_time_shifted = albedo_copy.time + np.timedelta64(10, 'h')\n",
    "\n",
    "alb_dt = pd.to_datetime(alb_time_shifted.values)\n",
    "\n",
    "alb_doy = alb_dt.day_of_year\n",
    "alb_time_of_day = alb_dt.strftime(\"%H:%M:%S\")  # Extract time in HH:MM:SS format\n",
    "print(alb_doy)\n",
    "print(alb_time_of_day)\n",
    "\n",
    "alb_time_2020 = pd.to_datetime(f\"2020-01-01\") + pd.to_timedelta(alb_doy - 1, unit=\"D\") + pd.to_timedelta(alb_time_of_day)\n",
    "alb_time_2020\n",
    "\n",
    "# replace time values in albedo_copy \n",
    "albedo_copy['time'] = alb_time_2020\n",
    "np.testing.assert_allclose(albedo_copy.isel(time=1).albedo, alb_dataset.isel(time=1).albedo)\n",
    "\n",
    "sw_dir_cor_selected = hrz_aligned.sel(time=albedo_copy.time, method=\"nearest\")['sw_dir_cor']\n",
    "print(sw_dir_cor_selected)\n",
    "\n",
    "alb_dataset_filtered = albedo_copy.where(sw_dir_cor_selected != 0, np.nan)\n",
    "alb_dataset_filtered['time'] = alb_dataset.time\n",
    "alb_dataset_filtered['HGT'] = alb_dataset.HGT\n",
    "alb_dataset_filtered\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alb_dataset_filtered_cali2 = alb_dataset_filtered.where(alb_dataset_filtered['CLOUDCOVER'] <= 30, drop=True)\n",
    "print(len(alb_dataset_filtered_cali2.time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Find all cloudy scenes\n",
    "Cloudy:\n",
    "2, 11, 24, (25), (27), (31), (40), (44),  47, 48, 52, 64, 70, 71, 72, 80, 86, 90, 94, 95, ((111))\n",
    "\"\"\"\n",
    "# Total length of the time dimension\n",
    "n_times = alb_dataset_filtered_cali2.dims['time']\n",
    "\n",
    "# Create a boolean mask: True for good indices, False for faulty\n",
    "mask = np.ones(n_times, dtype=bool)\n",
    "faulty_indices = [2, 11, 24, 25, 27, 31, 40, 44,  47, 48, 52, 64, 70, 71, 72, 80, 86, 90, 94, 95, 111]\n",
    "mask[faulty_indices] = False\n",
    "\n",
    "# Select only good indices using isel\n",
    "alb_dataset_filtered_cali = alb_dataset_filtered_cali2.isel(time=mask)\n",
    "alb_dataset_filtered_cali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alb_dataset_filtered_cali = alb_dataset_filtered.where(alb_dataset_filtered['CLOUDCOVER'] <= 20, drop=True) ## find threshold for calibration - 10% used for albedo estimation\n",
    "non_nan_count = alb_dataset_filtered_cali['albedo'].count(dim=['x', 'y'])\n",
    "alb_dataset_filtered_cali['non_nan_count'] = non_nan_count\n",
    "alb_dataset_filtered_cali['median_albedo'] = alb_dataset_filtered_cali['albedo'].mean(dim=['x', 'y']) #median more robust against outliers - but actually we use mean, more consistent with literature\n",
    "alb_dataset_filtered_cali['std_albedo'] = alb_dataset_filtered_cali['albedo'].std(dim=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate N_Eff\n",
    "pixel_size = 30 #30m Landsat footprint\n",
    "pixel_area = pixel_size**2\n",
    "L_corr = 500 #assume 500m corr.\n",
    "A_total = alb_dataset_filtered_cali['non_nan_count'] * pixel_area\n",
    "A_eff = np.pi * L_corr**2  # Effective area per independent sample\n",
    "N_eff = A_total / A_eff\n",
    "alb_dataset_filtered_cali['n_eff'] = N_eff\n",
    "\n",
    "standard_error = alb_dataset_filtered_cali['std_albedo'] / np.sqrt(alb_dataset_filtered_cali['n_eff'])\n",
    "alb_dataset_filtered_cali['sigma_albedo'] = np.sqrt((standard_error)**2 + (0.017)**2)\n",
    "\n",
    "alb_dataset_filtered_cali['sigma_albedo'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'win' in sys.platform:\n",
    "    alb_dataset_filtered_cali.to_netcdf(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/HEF_processed_HRZ-30CC-filter_albedos.nc\")\n",
    "else:\n",
    "    alb_dataset_filtered_cali.to_netcdf(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/HEF_processed_HRZ-30CC-filter_albedos.nc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LIST CLOUDY from 92 points\n",
    "scene 1, little bit but not much\n",
    "scene 2, def. clouds!\n",
    "scene 10, def. clouds\n",
    "scene 20 prob. clouds, 23 prob. 30 prob. 35, 36, 40, 55, 69, 73, 74, 75\n",
    "86\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find all cloudy scenes\n",
    "alb_dataset_filtered_cali['albedo'].isel(time=92).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alb_dataset_filtered_cali['albedo'].isel(time=2).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alb_play_data = alb_dataset_filtered.copy().where(alb_dataset_filtered['CLOUDCOVER'] <= 30, drop=True) #10 or 30\n",
    "alb_play_data['month'] = alb_play_data.time.dt.month\n",
    "#mean winter, mean summer\n",
    "def season_label(month):\n",
    "    if month in [12, 1, 2, 3]:  # Winter months #12,1,2,3\n",
    "        return 'winter'\n",
    "    elif month in [6, 7, 8, 9]:  # Summer months\n",
    "        return 'summer'\n",
    "\n",
    "alb_play_data['season'] = xr.DataArray([season_label(x) for x in alb_play_data['month'].values], dims='time')\n",
    "alb_play_data\n",
    "# Group by glaciological year and season, then compute the mean\n",
    "#seasonal_mean = alb_play_data.groupby('season').mean(skipna=True)\n",
    "#seasonal_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "winter = alb_play_data.where(alb_play_data['season'] == 'winter', drop=True).albedo\n",
    "#winter = seasonal_mean.sel(season='winter')['albedo']\n",
    "vals_winter = winter.data.flatten()\n",
    "vals_winter = vals_winter[~np.isnan(vals_winter)]\n",
    "\n",
    "summer = alb_play_data.where(alb_play_data['season'] == 'summer', drop=True).albedo\n",
    "#summer = seasonal_mean.sel(season='summer')['albedo']\n",
    "vals_summer = summer.data.flatten()\n",
    "vals_summer = vals_summer[~np.isnan(vals_summer)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## statistics for winter\n",
    "vals_winter_filtered = vals_winter\n",
    "vals_winter_filtered = vals_winter_filtered[~np.isnan(vals_winter_filtered)]\n",
    "print(np.nanmean(vals_winter_filtered) - 2*np.nanstd(vals_winter))\n",
    "\n",
    "#cut_winter_vals = np.where(vals_winter_filtered < 0.7, np.nan, vals_winter_filtered)\n",
    "cut_winter_vals = np.where(vals_winter_filtered < 0.75, np.nan, vals_winter_filtered) #cuffey paterson\n",
    "cut_winter_vals = np.where(cut_winter_vals > 0.98, np.nan, cut_winter_vals)\n",
    "cut_winter_vals = cut_winter_vals[~np.isnan(cut_winter_vals)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make selection for\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 8), dpi=300, sharex=True,\n",
    "    gridspec_kw={\"height_ratios\": (.1, .9), \"width_ratios\": (1, 1)}\n",
    ")\n",
    "plt.subplots_adjust(hspace=0.05, wspace=0.2)  # Adjust spacing\n",
    "\n",
    "# Assign axes\n",
    "ax_box = ax[0, 0]  # Top-left (boxplot)\n",
    "ax_main = ax[1, 0]  # Bottom-left (main plot)\n",
    "fig.delaxes(ax[0, 1])  # Delete top-right subplot\n",
    "fig.delaxes(ax[1, 1])  # Delete bottom-right subplot\n",
    "ax_side = fig.add_subplot(1, 2, 2)  # Right-side plot spanning full height\n",
    "\n",
    "bins_winter_cut = np.linspace(cut_winter_vals.min(), cut_winter_vals.max(), 100)  # Adjust the number of bins\n",
    "hist, bin_edges_cut = np.histogram(vals_winter_filtered, bins=bins_winter_cut)\n",
    "\n",
    "_2th = np.quantile(cut_winter_vals, 0.02)\n",
    "_25th = np.quantile(cut_winter_vals, 0.25)\n",
    "_50th = np.quantile(cut_winter_vals, 0.5)\n",
    "_75th = np.quantile(cut_winter_vals, 0.75)\n",
    "_98th = np.quantile(cut_winter_vals, 0.98)\n",
    "#print(_25th,_75th,_95th)\n",
    "#print(_50th, _95th)\n",
    "\n",
    "_mask = np.where((cut_winter_vals > _2th) & (cut_winter_vals < _98th))\n",
    "test_snow = cut_winter_vals[_mask]\n",
    "print(np.nanmin(test_snow))\n",
    "print(_2th)\n",
    "print(np.nanmax(test_snow))\n",
    "print(_98th)\n",
    "#Get upper 50% range\n",
    "print(\"50th percentile of filtered data:\", np.quantile(test_snow, 0.5))\n",
    "print(\"Max of filtered data:\", np.nanmax(test_snow))\n",
    "print(np.nanstd(test_snow[np.where((test_snow > np.quantile(test_snow, 0.5)))]))\n",
    "\n",
    "# the boxplot\n",
    "flierprops = dict(marker='o', markerfacecolor='black', markersize=3,\n",
    "                  markeredgecolor='none')\n",
    "ax_box.boxplot(cut_winter_vals, flierprops=flierprops, vert=False)\n",
    "ax_box.set_ylabel(\"\")\n",
    "ax_box.set_yticklabels(\"\")\n",
    "ax_box.set_yticks([])\n",
    "\n",
    "# removing borders\n",
    "ax_box.spines['top'].set_visible(True)\n",
    "ax_box.spines['right'].set_visible(True)\n",
    "ax_box.spines['left'].set_visible(True)\n",
    "\n",
    "# the histogram\n",
    "ax_main.hist(cut_winter_vals, bins=bin_edges_cut, edgecolor=\"black\", alpha=0.7)\n",
    "ax_main.axvline(x=_98th, linestyle=\"--\", color=\"green\")\n",
    "ax_main.axvline(x=np.quantile(test_snow, 0.5), linestyle=\"--\", color=\"red\")\n",
    "ax_main.set_xlabel(\"Snow albedo (-)\")\n",
    "ax_main.set_ylabel(\"Count\")\n",
    "ax_main.set_xticks(np.arange(0.75, 1+0.05, 0.05))\n",
    "ax_main.set_xlim(0.75, 1)\n",
    "\n",
    "#the cdf plot\n",
    "sorted_data = np.sort(cut_winter_vals)\n",
    "cdf = np.cumsum(sorted_data) / np.sum(sorted_data)\n",
    "\n",
    "\n",
    "ax_side.plot(sorted_data, cdf, color='blue', label='CDF')\n",
    "ax_side.axvline(np.quantile(test_snow, 0.5), color='red', linestyle='--', label=f'50th Percentile ({np.quantile(test_snow, 0.5):.2f})')\n",
    "ax_side.axvline(_98th, color='green', linestyle='--', label=f'98th Percentile ({_98th:.2f})')\n",
    "\n",
    "# --- FIX DUPLICATE Y-AXIS ---\n",
    "#ax_side.yaxis.set_ticks_position(\"left\")  # Ensure y-axis ticks appear only on the left\n",
    "#ax_side.yaxis.set_label_position(\"left\")  # Label stays on the left\n",
    "#ax_side.tick_params(axis=\"y\", which=\"both\", left=True, right=False)  # Hide right y-ticks\n",
    "\n",
    "ax_side.set_xlabel(\"Snow albedo (-)\")\n",
    "ax_side.set_xticks(np.arange(0.75, 1+0.05, 0.05))\n",
    "ax_side.set_xlim(0.75, 1)\n",
    "\n",
    "ax_side.set_xlabel('Snow Albedo (-)')\n",
    "ax_side.set_ylabel('Cumulative Probability')\n",
    "ax_side.legend()\n",
    "ax_side.grid(True)\n",
    "plt.savefig(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/FigS09_freshsnow_satellite_albedo.png\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repeat for ice albedo (from summer scenes in Cuffey and Paterson range )\n",
    "cut_summer_vals = np.where(vals_summer < 0.1, np.nan, vals_summer) #cuffey paterson\n",
    "cut_summer_vals = np.where(cut_summer_vals > 0.46, np.nan, cut_summer_vals)\n",
    "cut_summer_vals = cut_summer_vals[~np.isnan(cut_summer_vals)]\n",
    "mean = np.nanmean(cut_summer_vals)  # Mean of the distribution\n",
    "std_dev = np.nanstd(cut_summer_vals)  # Standard deviation of the distribution\n",
    "print(mean, std_dev)\n",
    "\n",
    "## make selection for\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 8), dpi=300, sharex=True,\n",
    "    gridspec_kw={\"height_ratios\": (.1, .9), \"width_ratios\": (1, 1)}\n",
    ")\n",
    "plt.subplots_adjust(hspace=0.05, wspace=0.2)  # Adjust spacing\n",
    "\n",
    "# Assign axes\n",
    "ax_box = ax[0, 0]  # Top-left (boxplot)\n",
    "ax_main = ax[1, 0]  # Bottom-left (main plot)\n",
    "fig.delaxes(ax[0, 1])  # Delete top-right subplot\n",
    "fig.delaxes(ax[1, 1])  # Delete bottom-right subplot\n",
    "ax_side = fig.add_subplot(1, 2, 2)  # Right-side plot spanning full height\n",
    "\n",
    "bins_summer_cut = np.linspace(cut_summer_vals.min(), cut_summer_vals.max(), 100)  # Adjust the number of bins\n",
    "hist, bin_edges_cut = np.histogram(cut_summer_vals, bins=bins_winter_cut)\n",
    "\n",
    "_2th = np.quantile(cut_summer_vals, 0.02)\n",
    "_25th = np.quantile(cut_summer_vals, 0.25)\n",
    "_75th = np.quantile(cut_summer_vals, 0.75)\n",
    "_98th = np.quantile(cut_summer_vals, 0.98)\n",
    "#print(_5th, _25th,_75th,_95th)\n",
    "\n",
    "\n",
    "_mask = np.where((cut_summer_vals > _2th) & (cut_summer_vals < _98th))\n",
    "test_ice = cut_summer_vals[_mask]\n",
    "print(np.nanmin(test_ice))\n",
    "print(_2th)\n",
    "print(np.nanmax(test_ice))\n",
    "print(_98th)\n",
    "#Get upper 50% range\n",
    "print(\"50th percentile of filtered data:\", np.quantile(test_ice, 0.5))\n",
    "print(\"Max of filtered data:\", np.nanmin(test_ice))\n",
    "print(\"OG. 50th:\", np.quantile(cut_summer_vals, 0.5))\n",
    "\n",
    "# the boxplot\n",
    "flierprops = dict(marker='o', markerfacecolor='black', markersize=3,\n",
    "                  markeredgecolor='none')\n",
    "ax_box.boxplot(cut_summer_vals, flierprops=flierprops, vert=False)\n",
    "ax_box.set_ylabel(\"\")\n",
    "ax_box.set_yticklabels(\"\")\n",
    "ax_box.set_yticks([])\n",
    "\n",
    "# removing borders\n",
    "ax_box.spines['top'].set_visible(True)\n",
    "ax_box.spines['right'].set_visible(True)\n",
    "ax_box.spines['left'].set_visible(True)\n",
    "\n",
    "# the histogram\n",
    "ax_main.hist(cut_summer_vals, bins=bins_summer_cut, edgecolor=\"black\", alpha=0.7)\n",
    "ax_main.axvline(x=_2th, linestyle=\"--\", color=\"red\")\n",
    "#ax_main.axvline(x=_25th, linestyle=\"--\", color=\"black\")\n",
    "#ax_main.axvline(x=_75th, linestyle=\"--\", color=\"black\")\n",
    "ax_main.axvline(x=np.quantile(test_ice, 0.5), linestyle=\"--\", color=\"green\")\n",
    "ax_main.set_xlabel(\"Ice albedo (-)\")\n",
    "ax_main.set_ylabel(\"Count\")\n",
    "ax_side.set_xticks(np.arange(0.1, 0.4+0.05, 0.05))\n",
    "ax_side.set_xlim(0.05, 0.45)\n",
    "\n",
    "#the cdf plot\n",
    "sorted_data = np.sort(cut_summer_vals)\n",
    "cdf = np.cumsum(sorted_data) / np.sum(sorted_data)\n",
    "\n",
    "ax_side.plot(sorted_data, cdf, color='blue', label='CDF')\n",
    "ax_side.axvline(x=_2th, linestyle=\"--\", color=\"red\", label=f'2th Percentile ({_2th:.2f})')\n",
    "#ax_side.axvline(_25th, color='black', linestyle='--', label=f'25th Percentile ({_25th:.2f})')\n",
    "#ax_side.axvline(_75th, color='black', linestyle='--', label=f'75th Percentile ({_75th:.2f})')\n",
    "ax_side.axvline(x=np.quantile(test_ice, 0.5), linestyle=\"--\", color=\"green\", label=f'50th Percentile ({np.quantile(test_ice, 0.5):.2f})')\n",
    "\n",
    "\n",
    "# --- FIX DUPLICATE Y-AXIS ---\n",
    "#ax_side.yaxis.set_ticks_position(\"left\")  # Ensure y-axis ticks appear only on the left\n",
    "#ax_side.yaxis.set_label_position(\"left\")  # Label stays on the left\n",
    "#ax_side.tick_params(axis=\"y\", which=\"both\", left=True, right=False)  # Hide right y-ticks\n",
    "\n",
    "ax_side.set_xlabel(\"Ice albedo (-)\")\n",
    "ax_side.set_xticks(np.arange(0.1, 0.4+0.05, 0.05))\n",
    "ax_side.set_xlim(0.05, 0.45)\n",
    "\n",
    "ax_side.set_xlabel('Ice albedo (-)')\n",
    "ax_side.set_ylabel('Cumulative Probability')\n",
    "ax_side.legend()\n",
    "ax_side.grid(True)\n",
    "plt.savefig(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/FigS07_ice_satellite_albedo.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Firn selection ... \n",
    "## Repeat for ice albedo (from summer scenes in Cuffey and Paterson range )\n",
    "cut_firn_vals = np.where(vals_summer < 0.46, np.nan, vals_summer) #cuffey paterson\n",
    "cut_firn_vals = np.where(cut_firn_vals > 0.75, np.nan, cut_firn_vals)\n",
    "cut_firn_vals = cut_firn_vals[~np.isnan(cut_firn_vals)]\n",
    "mean = np.nanmean(cut_firn_vals)  # Mean of the distribution\n",
    "std_dev = np.nanstd(cut_firn_vals)  # Standard deviation of the distribution\n",
    "print(mean, std_dev)\n",
    "\n",
    "## make selection for\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 8), dpi=300, sharex=True,\n",
    "    gridspec_kw={\"height_ratios\": (.1, .9), \"width_ratios\": (1, 1)}\n",
    ")\n",
    "plt.subplots_adjust(hspace=0.05, wspace=0.2)  # Adjust spacing\n",
    "\n",
    "# Assign axes\n",
    "ax_box = ax[0, 0]  # Top-left (boxplot)\n",
    "ax_main = ax[1, 0]  # Bottom-left (main plot)\n",
    "fig.delaxes(ax[0, 1])  # Delete top-right subplot\n",
    "fig.delaxes(ax[1, 1])  # Delete bottom-right subplot\n",
    "ax_side = fig.add_subplot(1, 2, 2)  # Right-side plot spanning full height\n",
    "\n",
    "bins_firn_cut = np.linspace(cut_firn_vals.min(), cut_firn_vals.max(), 100)  # Adjust the number of bins\n",
    "hist, bin_edges_cut = np.histogram(cut_firn_vals, bins=bins_winter_cut)\n",
    "\n",
    "_2th = np.quantile(cut_firn_vals, 0.02)\n",
    "_25th = np.quantile(cut_firn_vals, 0.25)\n",
    "_75th = np.quantile(cut_firn_vals, 0.75)\n",
    "_98th = np.quantile(cut_firn_vals, 0.98)\n",
    "#print(_5th, _25th,_75th,_95th)\n",
    "\n",
    "\n",
    "_mask = np.where((cut_firn_vals > _2th) & (cut_firn_vals < _98th))\n",
    "test_firn = cut_firn_vals[_mask]\n",
    "print(np.nanmin(test_firn))\n",
    "print(_2th)\n",
    "print(np.nanmax(test_firn))\n",
    "print(_98th)\n",
    "#Get upper 50% range\n",
    "print(\"50th percentile of filtered data:\", np.quantile(test_firn, 0.5))\n",
    "print(\"Min of filtered data:\", np.nanmin(test_firn))\n",
    "\n",
    "# the boxplot\n",
    "flierprops = dict(marker='o', markerfacecolor='black', markersize=3,\n",
    "                  markeredgecolor='none')\n",
    "ax_box.boxplot(cut_firn_vals, flierprops=flierprops, vert=False)\n",
    "ax_box.set_ylabel(\"\")\n",
    "ax_box.set_yticklabels(\"\")\n",
    "ax_box.set_yticks([])\n",
    "\n",
    "# removing borders\n",
    "ax_box.spines['top'].set_visible(True)\n",
    "ax_box.spines['right'].set_visible(True)\n",
    "ax_box.spines['left'].set_visible(True)\n",
    "\n",
    "# the histogram\n",
    "ax_main.hist(cut_firn_vals, bins=bins_firn_cut, edgecolor=\"black\", alpha=0.7)\n",
    "ax_main.axvline(x=_2th, linestyle=\"--\", color=\"red\")\n",
    "#ax_main.axvline(x=_25th, linestyle=\"--\", color=\"black\")\n",
    "#ax_main.axvline(x=_75th, linestyle=\"--\", color=\"black\")\n",
    "ax_main.axvline(x=np.quantile(test_firn, 0.5), linestyle=\"--\", color=\"green\")\n",
    "ax_main.set_xlabel(\"Firn albedo (-)\")\n",
    "ax_main.set_ylabel(\"Count\")\n",
    "ax_side.set_xticks(np.arange(0.4, 0.7+0.05, 0.05))\n",
    "ax_side.set_xlim(0.35, 0.75)\n",
    "\n",
    "#the cdf plot\n",
    "sorted_data = np.sort(cut_firn_vals)\n",
    "cdf = np.cumsum(sorted_data) / np.sum(sorted_data)\n",
    "\n",
    "ax_side.plot(sorted_data, cdf, color='blue', label='CDF')\n",
    "ax_side.axvline(_2th, color='green', linestyle='--', label=f'2th Percentile ({_2th:.2f})')\n",
    "#ax_side.axvline(_25th, color='black', linestyle='--', label=f'25th Percentile ({_25th:.2f})')\n",
    "#ax_side.axvline(_75th, color='black', linestyle='--', label=f'75th Percentile ({_75th:.2f})')\n",
    "ax_side.axvline(x=np.quantile(test_firn, 0.5), linestyle=\"--\", color=\"red\", label=f'50th Percentile ({np.quantile(test_firn, 0.5):.2f})')\n",
    "\n",
    "# --- FIX DUPLICATE Y-AXIS ---\n",
    "#ax_side.yaxis.set_ticks_position(\"left\")  # Ensure y-axis ticks appear only on the left\n",
    "#ax_side.yaxis.set_label_position(\"left\")  # Label stays on the left\n",
    "#ax_side.tick_params(axis=\"y\", which=\"both\", left=True, right=False)  # Hide right y-ticks\n",
    "\n",
    "ax_side.set_xticks(np.arange(0.4, 0.7+0.05, 0.05))\n",
    "ax_side.set_xlim(0.35, 0.75)\n",
    "\n",
    "ax_side.set_xlabel('Firn albedo (-)')\n",
    "ax_side.set_ylabel('Cumulative Probability')\n",
    "ax_side.legend()\n",
    "ax_side.grid(True)\n",
    "plt.savefig(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/FigS08_firn_satellite_albedo.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_winter = np.linspace(vals_winter_filtered.min(), vals_winter_filtered.max(), 100)  # Adjust the number of bins\n",
    "hist, bin_edges = np.histogram(vals_winter_filtered, bins=bins_winter)\n",
    "\n",
    "#50% of data above\n",
    "#quant = np.nanquantile(vals_winter_filtered, 0.5)\n",
    "\n",
    "fig, ax = plt.subplots(2,1, figsize=(16,9), dpi=300, sharex=True)\n",
    "ax[0].hist(vals_winter_filtered, bins=bin_edges, edgecolor=\"black\", alpha=0.7)\n",
    "#ax[0].axvline(x=quant, color=\"black\", linestyle='--')\n",
    "ax[0].set_xticks(np.arange(0,1+0.1,0.1))\n",
    "ax[0].set_xlim(0,1)\n",
    "ax[0].set_ylabel('Winter Albedo Frequency')\n",
    "ax[0].grid(True)\n",
    "#\n",
    "bins_summer = np.linspace(vals_summer.min(), vals_summer.max(), 100)  # Adjust the number of bins\n",
    "hist, bin_edges = np.histogram(vals_summer, bins=bins_summer)\n",
    "\n",
    "ax[1].hist(vals_summer, bins=bin_edges, edgecolor=\"black\", alpha=0.7)\n",
    "ax[1].set_xticks(np.arange(0,1+0.05,0.05))\n",
    "ax[1].set_xlim(0,1)\n",
    "ax[1].set_ylabel('Summer Albedo Frequency')\n",
    "ax[1].grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
