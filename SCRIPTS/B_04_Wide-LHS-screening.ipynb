{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Script requires the output of the first 500 LHS samples (wide). These were generated with the respective spotpy script.\n",
    "\n",
    "if 'win' in sys.platform:\n",
    "    path = \"E:/OneDrive - uibk.ac.at/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/LHS/\"\n",
    "    filepath = \"E:/OneDrive - uibk.ac.at/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/\"\n",
    "    tsla = pd.read_csv(\"E:/OneDrive - uibk.ac.at/PhD/PhD/Data/Hintereisferner/Climate/snowlines/HEF-snowlines-1999-2010_manual_filtered.csv\")\n",
    "    figpath = \"E:/OneDrive - uibk.ac.at/PhD/PhD/Data/Hintereisferner/Figures/\"\n",
    "else:\n",
    "    #path = \"/mnt/C4AEBBABAEBB9500/OneDrive - uibk.ac.at/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/LHS/\"\n",
    "    #temporary solution due to issues with OneDrive!\n",
    "    path = \"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/LHS/\"\n",
    "    figpath = \"/home/niki/Dokumente/NewOneDrive/\"\n",
    "    filepath = \"/home/niki/Dokumente/NewOneDrive/\"\n",
    "    tsla = pd.read_csv(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/snowlines/HEF-snowlines-1999-2010_manual_filtered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path+\"LHS-wide_1D20m_1999_2010_fullprior.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare TSLA data\n",
    "time_start_dt = pd.to_datetime(\"2000-01-01\") #config starts with spinup - need to add 1year\n",
    "time_end_dt = pd.to_datetime(\"2009-12-31\")\n",
    "\n",
    "tsla_true_obs = tsla.copy()\n",
    "tsla_true_obs['LS_DATE'] = pd.to_datetime(tsla_true_obs['LS_DATE'])\n",
    "print(\"Start date:\", time_start_dt)\n",
    "print(\"End date:\", time_end_dt)\n",
    "tsla_true_obs = tsla_true_obs.loc[(tsla_true_obs['LS_DATE'] > time_start_dt) & (tsla_true_obs['LS_DATE'] <= time_end_dt)]\n",
    "tsla_true_obs.set_index('LS_DATE', inplace=True)\n",
    "#Normalize standard deviation if necessary\n",
    "tsla_true_obs['SC_stdev'] = (tsla_true_obs['SC_stdev']) / (tsla_true_obs['glacier_DEM_max'] - tsla_true_obs['glacier_DEM_min'])\n",
    "\n",
    "thres_unc = (20) / (tsla_true_obs['glacier_DEM_max'].iloc[0] - tsla_true_obs['glacier_DEM_min'].iloc[0])\n",
    "print(thres_unc)\n",
    "\n",
    "## Set observational uncertainty where smaller to atleast model resolution (20m) and where larger keep it\n",
    "sc_norm = np.where(tsla_true_obs['SC_stdev'] < thres_unc, thres_unc, tsla_true_obs['SC_stdev'])\n",
    "tsla_true_obs['SC_stdev'] = sc_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load MB data\n",
    "rgi_id = \"RGI60-11.00897\"\n",
    "if 'win' in sys.platform:\n",
    "    geod_ref = pd.read_csv(\"E:/OneDrive/PhD/PhD/Data/Hugonnet_21_MB/dh_11_rgi60_pergla_rates.csv\")\n",
    "else:\n",
    "    geod_ref = pd.read_csv(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hugonnet_21_MB/dh_11_rgi60_pergla_rates.csv\")\n",
    "geod_ref = geod_ref.loc[geod_ref['rgiid'] == rgi_id]\n",
    "geod_ref = geod_ref.loc[geod_ref['period'] == \"2000-01-01_2010-01-01\"]\n",
    "geod_ref = geod_ref[['dmdtda', 'err_dmdtda']]\n",
    "print(geod_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modtsls = df.iloc[:,13:13+58].transpose()\n",
    "print(modtsls)\n",
    "\n",
    "mb_mod = df.iloc[:,[12]].transpose()\n",
    "print(mb_mod)\n",
    "\n",
    "def loglike_tsla_func(sim_tsla, eval_tsla, sigma_tsla):\n",
    "    loglike_tsla = -0.5 * np.sum(np.log(2 * np.pi * sigma_tsla**2) + ((eval_tsla-sim_tsla)**2 / sigma_tsla**2))\n",
    "    avg_loglike_tsla = loglike_tsla / len(eval_tsla)\n",
    "    return avg_loglike_tsla\n",
    "\n",
    "def loglike_mb_func(sim_mb, eval_mb, sigma_mb):\n",
    "    loglike_mb = -0.5 * (np.log(2 * np.pi * sigma_mb**2) + ( ((eval_mb-sim_mb)**2) / sigma_mb**2))\n",
    "    return loglike_mb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_logp = mb_mod.transpose().apply(loglike_mb_func, eval_mb = geod_ref['dmdtda'].values, sigma_mb= geod_ref['err_dmdtda'].values, axis=0)\n",
    "print(mb_logp)\n",
    "\n",
    "tsl_logp = modtsls.apply(loglike_tsla_func, eval_tsla =tsla_true_obs['TSL_normalized'].values, sigma_tsla= tsla_true_obs['SC_stdev'].values, axis=0)\n",
    "print(tsl_logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load ALBEDO observations\n",
    "if 'win' in sys.platform:\n",
    "    alb_obs_data = xr.open_dataset(r\"E:\\OneDrive\\PhD\\PhD\\Data\\Hintereisferner\\Climate\\HEF_processed_HRZ-20CC-filter_albedos.nc\")\n",
    "else:\n",
    "    alb_obs_data = xr.open_dataset(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/HEF_processed_HRZ-20CC-filter_albedos.nc\")\n",
    "#has nans where no glacier -> can build glacier-wide mean albedo for additional logp\n",
    "alb_obs_data = alb_obs_data.sortby(\"time\")\n",
    "alb_obs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tsl_logp = modtsls.apply(loglike_tsla_func, eval_tsla =tsla_true_obs['TSL_normalized'].values, sigma_tsla= tsla_true_obs['SC_stdev'].values, axis=0)\n",
    "#print(tsl_logp)\n",
    "sigma_alb = alb_obs_data.sigma_albedo\n",
    "eval_alb = alb_obs_data.median_albedo\n",
    "\n",
    "list_logp_alb = []\n",
    "list_sim_alb = []\n",
    "for i,r in df.iterrows():\n",
    "    if i % 300 == 0:\n",
    "        print(f\"Processing file {i}/3000\")\n",
    "    rrr_factor = round(r['rrr_factor'],4)\n",
    "    alb_ice = round(r['alb_ice'],4)\n",
    "    alb_snow = round(r['alb_snow'],4)\n",
    "    alb_firn = round(r['alb_firn'],4)\n",
    "    alb_aging = round(r['albedo_aging'],4)\n",
    "    alb_depth = round(r['albedo_depth'],4)\n",
    "    roughness_ice = round(r['roughness_ice'], 4)\n",
    "    filename = f\"HEF_COSMO_1D20m_1999_2010_HORAYZON_IntpPRES_LHS-wide_19990101-20091231_RRR-{rrr_factor}_{alb_snow}_{alb_ice}_{alb_firn}_{alb_aging}_{alb_depth}_0.24_{roughness_ice}_4.0_0.0026_num2.nc\"\n",
    "    if 'win' in sys.platform:\n",
    "        sim_alb = xr.open_dataset(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Output/albedo_files/LHS-wide/\"+\\\n",
    "            filename)[\"ALBEDO_weighted\"].resample(time=\"1D\").mean()\n",
    "    else:\n",
    "        sim_alb = xr.open_dataset(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Output/albedo_files/LHS-wide/\"+\\\n",
    "            filename)[\"ALBEDO_weighted\"].resample(time=\"1D\").mean()\n",
    "\n",
    "    \n",
    "    #sort by time\n",
    "    sim_alb = sim_alb.sel(time=alb_obs_data.time)\n",
    "    sim_alb = sim_alb.sortby(\"time\")\n",
    "        \n",
    "    ## compute logp albedo for file\n",
    "    list_sim_alb.append(sim_alb.data)\n",
    "    logp_alb_all = -0.5 * np.sum(np.log(2 * np.pi * sigma_alb**2) + ((sim_alb.data-eval_alb)**2 / sigma_alb**2))\n",
    "    avg_logp_alb = logp_alb_all / len(sim_alb)\n",
    "    list_logp_alb.append(avg_logp_alb.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_list = [f\"sim{i+1}\" for i in range(79)]\n",
    "print(sim_list)\n",
    "\n",
    "df['mb_logp'] = mb_logp\n",
    "df['tsla_logp'] = tsl_logp\n",
    "df['alb_logp'] = list_logp_alb\n",
    "\n",
    "df['joint_like'] = df['mb_logp'] + df['tsla_logp'] + df['alb_logp']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check ranges of logp to ensure equal weight..\n",
    "print(\"MB Logp Range:\")\n",
    "print(df.mb_logp.min(), df.mb_logp.max())\n",
    "print(\"TSLA Logp Range:\")\n",
    "print(df.tsla_logp.min(), df.tsla_logp.max())\n",
    "print(\"Alb Logp Range:\")\n",
    "print(df.alb_logp.min(), df.alb_logp.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_logps = False\n",
    "\n",
    "if scale_logps:\n",
    "    # Min-Max normalization\n",
    "    logp_alb_min = df.alb_logp.min()\n",
    "    logp_alb_max = df.alb_logp.max()\n",
    "    logp_alb_normalized = (df.alb_logp - logp_alb_min) / (logp_alb_max - logp_alb_min)\n",
    "    df['alb_logp'] = logp_alb_normalized\n",
    "\n",
    "    logp_mb_min = df.mb_logp.min()\n",
    "    logp_mb_max = df.mb_logp.max()\n",
    "    logp_mb_normalized = (df.mb_logp - logp_mb_min) / (logp_mb_max - logp_mb_min)\n",
    "    df['mb_logp'] = logp_mb_normalized\n",
    "\n",
    "    logp_snowline_min = df.tsla_logp.min()\n",
    "    logp_snowline_max = df.tsla_logp.max()\n",
    "    logp_snowline_normalized = (df.tsla_logp - logp_snowline_min) / (logp_snowline_max - logp_snowline_min)\n",
    "    df['tsla_logp'] = logp_snowline_normalized\n",
    "\n",
    "\n",
    "    print(\"New MB Logp Range:\")\n",
    "    print(df.mb_logp.min(), df.mb_logp.max())\n",
    "    print(\"New TSLA Logp Range:\")\n",
    "    print(df.tsla_logp.min(), df.tsla_logp.max())\n",
    "    print(\"New Alb Logp Range:\")\n",
    "    print(df.alb_logp.min(), df.alb_logp.max())\n",
    "\n",
    "    df['joint_like'] = df['mb_logp'] + df['tsla_logp'] + df['alb_logp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_features = [\"rrr_factor\",\"alb_ice\",\"alb_snow\",\"alb_firn\",\"albedo_aging\",\"albedo_depth\",\"roughness_ice\"]\n",
    "## Repeat correlation for only best percent\n",
    "best_ten = df[param_features+['alb_logp', 'tsla_logp', 'mb_logp','joint_like']]\n",
    "best_ten = best_ten.loc[best_ten['joint_like'] >= np.nanpercentile(best_ten['joint_like'], 90)]\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "# Compute correlation matrix (including log-likelihoods)\n",
    "correlation_matrix = best_ten[param_features+['alb_logp', 'tsla_logp', 'mb_logp','joint_like']].corr()\n",
    "\n",
    "# Plot the correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix Between Parameters and Log-Likelihood\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ll_drop_from_model_vs_obs(model_outputs, observations, sigmas, delta_sigma=2):\n",
    "    \"\"\"\n",
    "    Compute average log-likelihood drop corresponding to delta_sigma error increase \n",
    "    for the best-fitting simulation.\n",
    "\n",
    "    Parameters:\n",
    "    - model_outputs: array (n_runs, n_points)\n",
    "    - observations: array (n_points,)\n",
    "    - sigmas: array (n_points,) or (n_runs, n_points)\n",
    "    - delta_sigma: float, e.g., 2 for ±2σ threshold\n",
    "\n",
    "    Returns:\n",
    "    - best_run_index: index of the best run\n",
    "    - best_avg_ll: average log-likelihood of the best run\n",
    "    - drop: drop in average log-likelihood when model moves delta_sigma away\n",
    "    \"\"\"\n",
    "    model_outputs = np.asarray(model_outputs)\n",
    "    observations = np.asarray(observations)\n",
    "    sigmas = np.asarray(sigmas)\n",
    "\n",
    "    n_runs, n_points = model_outputs.shape\n",
    "\n",
    "    if sigmas.ndim == 1:\n",
    "        sigmas = np.tile(sigmas, (n_runs, 1))  # shape (n_runs, n_points)\n",
    "\n",
    "    # Log-likelihood for each model value given observation ± sigma\n",
    "    loglikelihoods = norm.logpdf(model_outputs, loc=observations, scale=sigmas)\n",
    "    avg_ll_per_run = np.mean(loglikelihoods, axis=1)\n",
    "\n",
    "    # Best run\n",
    "    best_run_index = np.argmax(avg_ll_per_run)\n",
    "    best_avg_ll = avg_ll_per_run[best_run_index]\n",
    "\n",
    "    # Apply delta_sigma shift to the model output for best run\n",
    "    shifted_model = model_outputs[best_run_index] + delta_sigma * sigmas[best_run_index]\n",
    "    shifted_loglikelihoods = norm.logpdf(shifted_model, loc=observations, scale=sigmas[best_run_index])\n",
    "    shifted_avg_ll = np.mean(shifted_loglikelihoods)\n",
    "\n",
    "    drop = best_avg_ll - shifted_avg_ll\n",
    "\n",
    "    return best_run_index, best_avg_ll, drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx, best_ll, drop_2sigma = compute_ll_drop_from_model_vs_obs(\n",
    "    modtsls.transpose(), tsla_true_obs['TSL_normalized'], tsla_true_obs['SC_stdev'], delta_sigma=3\n",
    ")\n",
    "\n",
    "print(f\"Best run index: {best_idx}\")\n",
    "print(f\"Best avg log-likelihood: {best_ll:.4f}\")\n",
    "print(f\"Log-likelihood drop at +3σ: {drop_2sigma:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx, best_ll, drop_2sigma = compute_ll_drop_from_model_vs_obs(\n",
    "    mb_mod.transpose(), geod_ref['dmdtda'], geod_ref['err_dmdtda'], delta_sigma=3\n",
    ")\n",
    "\n",
    "print(f\"Best run index: {best_idx}\")\n",
    "print(f\"Best avg log-likelihood: {best_ll:.4f}\")\n",
    "print(f\"Log-likelihood drop at +3σ: {drop_2sigma:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx, best_ll, drop_2sigma = compute_ll_drop_from_model_vs_obs(\n",
    "    np.asarray(list_sim_alb), eval_alb.data, sigma_alb, delta_sigma=3\n",
    ")\n",
    "\n",
    "print(f\"Best run index: {best_idx}\")\n",
    "print(f\"Best avg log-likelihood: {best_ll:.4f}\")\n",
    "print(f\"Log-likelihood drop at +3σ: {drop_2sigma:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create thresholds based on this experiment\n",
    "minimum_mb = 0.4242 - 4.4493\n",
    "minimum_alb = 0.9257 - 5.7602\n",
    "minimum_tsla = -28.0132 + -1.9042\n",
    "\n",
    "minimum_thres = minimum_mb + minimum_alb + minimum_tsla\n",
    "print(\"Conservative cutoff at \", minimum_thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_subset = df.loc[df['joint_like'] >= minimum_thres]\n",
    "print(len(final_subset))\n",
    "final_subset['joint_like'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,2, figsize=(16,9), dpi=150)\n",
    "ax[0,0].hist(final_subset['rrr_factor'], edgecolor=\"black\", alpha=0.7, label=\"Filter\")\n",
    "ax[0,0].hist(df['rrr_factor'], edgecolor=\"black\", alpha=0.7, label=\"Full\", zorder=-1)\n",
    "#\n",
    "ax[0,1].hist(final_subset['alb_ice'], edgecolor=\"black\", alpha=0.7, label=\"Filter\")\n",
    "ax[0,1].hist(df['alb_ice'], edgecolor=\"black\", alpha=0.7, label=\"Full\", zorder=-1)\n",
    "#\n",
    "ax[1,0].hist(final_subset['alb_snow'], edgecolor=\"black\", alpha=0.7, label=\"Filter\")\n",
    "ax[1,0].hist(df['alb_snow'], edgecolor=\"black\", alpha=0.7, label=\"Full\", zorder=-1)\n",
    "#\n",
    "ax[1,1].hist(final_subset['alb_firn'], edgecolor=\"black\", alpha=0.7, label=\"Filter\")\n",
    "ax[1,1].hist(df['alb_firn'], edgecolor=\"black\", alpha=0.7, label=\"Full\", zorder=-1)\n",
    "#\n",
    "ax[2,0].hist(final_subset['albedo_aging'], edgecolor=\"black\", alpha=0.7, label=\"Filter\")\n",
    "ax[2,0].hist(df['albedo_aging'], edgecolor=\"black\", alpha=0.7, label=\"Full\", zorder=-1)\n",
    "#\n",
    "ax[2,1].hist(final_subset['albedo_depth'], edgecolor=\"black\", alpha=0.7, label=\"Filter\")\n",
    "ax[2,1].hist(df['albedo_depth'], edgecolor=\"black\", alpha=0.7, label=\"Full\", zorder=-1)\n",
    "#\n",
    "ax[3,0].hist(final_subset['roughness_ice'], edgecolor=\"black\", alpha=0.7, label=\"Filter\")\n",
    "ax[3,0].hist(df['roughness_ice'], edgecolor=\"black\", alpha=0.7, label=\"Full\", zorder=-1)\n",
    "#\n",
    "ax[3,1].hist(final_subset['joint_like'], edgecolor=\"black\", alpha=0.7, label=\"Filter\")\n",
    "ax[3,1].hist(df['joint_like'], edgecolor=\"black\", alpha=0.7, label=\"Full\", zorder=-1)\n",
    "ax[0,0].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in param_features:\n",
    "    print(f\"Param {param} bounds are: {np.nanmin(final_subset[param])} - {np.nanmax(final_subset[param])}.\")\n",
    "    print(f\"Min/Max Sampled bounds are: {np.nanmin(df[param])} - {np.nanmax(df[param])}.\")\n",
    "    print(\"----------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
