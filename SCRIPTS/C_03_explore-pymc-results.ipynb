{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import arviz as az\n",
    "import pymc as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "import sys\n",
    "import emcee\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score\n",
    "import pathlib\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "from emcee.autocorr import integrated_time\n",
    "\n",
    "## Script to anaylse outputs of emulator-based MCMC runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'win' in sys.platform:\n",
    "    path = \"E:/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/\"\n",
    "else:\n",
    "    path = \"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/\"\n",
    "\n",
    "#ds = az.from_netcdf(path+\"point_demcz_posterior_combined.nc\")\n",
    "#ds = az.from_netcdf(path+\"stage1_demczsyserr_posterior_combined.nc\") #snowline + alb summer sys err\n",
    "#ds = az.from_netcdf(path+\"stage1_albmb_demczsyserr_posterior_combined.nc\")\n",
    "ds = az.from_netcdf(path+\"stage2_final_demczsyserr_posterior_combined.nc\")\n",
    "\n",
    "#ds = az.from_netcdf(path+\"stage1_albmbtimes3_demczsyserr_posterior_combined.nc\")\n",
    "#ds = az.from_netcdf(path+\"full_demczsyserr_posterior_combined.nc\") #snowline + alb summer sys err\n",
    "#ds = az.from_netcdf(path+\"stage1_syserr_demcz_posterior_combined.nc\") #snowline sys err\n",
    "#ds = az.from_netcdf(path+\"demcz_posterior_combined.nc\")\n",
    "#ds = az.from_netcdf(path+\"stage1_demcz_posterior_combined.nc\")\n",
    "#ds = az.from_netcdf(path+\"stage2_demcz_posterior_combined.nc\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normal params\n",
    "try:\n",
    "    param_names = [\"rrrfactor\", \"albice\", \"albsnow\", \"albfirn\", \"albaging\", \"albdepth\", \"iceroughness\",\"centersnow\"]\n",
    "    az.plot_trace(ds, var_names=param_names)\n",
    "except:\n",
    "    param_names = [\"rrrfactor\", \"albice\", \"albsnow\", \"albfirn\", \"albaging\", \"albdepth\", \"iceroughness\"]\n",
    "    az.plot_trace(ds, var_names=param_names)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.posterior.loglike_alb[:,:].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.posterior.loglike_tsl[:,:].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.posterior.loglike_mb[:,:].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.posterior.mu_mb.isel(chain=2, mu_mb_dim_0=0).plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.posterior.total_loglike.isel(chain=2).plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stage1 = az.summary(ds, hdi_prob=0.95)\n",
    "summary_stage1.loc[summary_stage1.index.intersection(param_names + [\"sigma_alb_summer\", \"sigma_tsl_summer\"])] #\"sigma_tsl_summer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Derive init vals for 2nd stage\n",
    "num_chains_stage2 = 20\n",
    "\n",
    "# Get the total number of posterior samples from Stage 1\n",
    "num_draws = ds.posterior.draw.size\n",
    "num_chains_stage1 = ds.posterior.chain.size\n",
    "\n",
    "# --- Generate a list of initial value dictionaries ---\n",
    "initial_values_stage2 = []\n",
    "for _ in range(num_chains_stage2):\n",
    "    # Pick a random chain and a random draw from the Stage 1 trace\n",
    "    rand_chain = np.random.randint(0, num_chains_stage1)\n",
    "    rand_draw = np.random.randint(0, num_draws)\n",
    "    \n",
    "    # Create a dictionary for this chain's initial values\n",
    "    init_dict = {\n",
    "        'rrrfactor': ds.posterior['rrrfactor'].isel(chain=rand_chain, draw=rand_draw).values,\n",
    "        'albsnow': ds.posterior['albsnow'].isel(chain=rand_chain, draw=rand_draw).values,\n",
    "        'albfirn': ds.posterior['albfirn'].isel(chain=rand_chain, draw=rand_draw).values,\n",
    "        'albaging': ds.posterior['albaging'].isel(chain=rand_chain, draw=rand_draw).values,\n",
    "        'albdepth': ds.posterior['albdepth'].isel(chain=rand_chain, draw=rand_draw).values,\n",
    "        'albice': ds.posterior['albice'].isel(chain=rand_chain, draw=rand_draw).values,\n",
    "        'iceroughness': ds.posterior['iceroughness'].isel(chain=rand_chain, draw=rand_draw).values,\n",
    "        #'sigma_tsl_summer': ds.posterior['sigma_tsl_summer'].isel(chain=rand_chain, draw=rand_draw).values,\n",
    "        'sigma_alb_summer': ds.posterior['sigma_alb_summer'].isel(chain=rand_chain, draw=rand_draw).values\n",
    "    }\n",
    "    initial_values_stage2.append(init_dict)\n",
    "    \n",
    "import pickle\n",
    "if 'win' in sys.platform:\n",
    "    with open('E:/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/LHS/stage2_initial_values.pkl', 'wb') as f:\n",
    "        pickle.dump(initial_values_stage2, f)\n",
    "else:\n",
    "    with open('/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/LHS/stage2_initial_values.pkl', 'wb') as f:\n",
    "        pickle.dump(initial_values_stage2, f)     \n",
    "initial_values_stage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unc terms\n",
    "try:\n",
    "    #param_unc_names = [\"sigma_alb_summer\"]\n",
    "    param_unc_names = [\"sigma_tsl_summer\", \"sigma_alb_summer\"]\n",
    "    az.plot_trace(ds, var_names=param_unc_names)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO ####\n",
    "## Integrative autocorrelation time - IAT for all parameters + average, must flatten 25000\n",
    "## ESS \n",
    "## R-hat\n",
    "## Monte Carlo Standard Error\n",
    "ds.posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract total log-likelihood (shape: chains x draws)\n",
    "# Assumes you have this already\n",
    "loglikes = ds.posterior.total_loglike.values[:,:]  # shape (n_chains, n_samples)\n",
    "\n",
    "# Step 2: Compute mean log-likelihood per chain\n",
    "mean_ll = loglikes.mean(axis=1)\n",
    "\n",
    "# Step 3: Get indices of best chains (exclude worst 4)\n",
    "n_cut = 0 #4\n",
    "n_chains_to_keep = loglikes.shape[0] - n_cut\n",
    "best_chain_idx = np.argsort(-mean_ll)[:n_chains_to_keep]  # best â†’ worst\n",
    "print(best_chain_idx)\n",
    "# Step 4: Subset posterior object\n",
    "# Assumes `posterior` is an arviz.InferenceData object\n",
    "posterior_subset = ds.posterior.sel(chain=best_chain_idx)\n",
    "posterior_subset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- CONFIGURATION --\n",
    "import matplotlib.ticker as ticker\n",
    "dpi = 300\n",
    "figsize = (13, 13)  # adjust based on number of parameters\n",
    "font_size = 22\n",
    "sample_size = 20000\n",
    "palette = sns.color_palette(\"crest\", as_cmap=True)\n",
    "\n",
    "# -- SET STYLE --\n",
    "sns.set(style=\"white\", font_scale=1.6)\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": font_size,\n",
    "    \"axes.labelsize\": font_size,\n",
    "    \"axes.titlesize\": font_size,\n",
    "    \"xtick.labelsize\": font_size * 0.8,\n",
    "    \"ytick.labelsize\": font_size * 0.8,\n",
    "    \"figure.dpi\": dpi\n",
    "})\n",
    "\n",
    "# -- EXTRACT POSTERIOR AND SAMPLE --\n",
    "posi = ds.posterior[param_names]\n",
    "posterior_stacked = posi.stack(sample=(\"chain\", \"draw\"))  # collapse chains\n",
    "posi_samples_kde = posterior_stacked.to_dataframe()[param_names].sample(sample_size, random_state=42)\n",
    "posi_samples_kde.rename(columns={'rrrfactor': r'$p_{f}$', 'albice': r'$\\alpha_{ice}$', 'albsnow': r'$\\alpha_{fs}$','albfirn': r'$\\alpha_{firn}$', 'albaging': r'$\\alpha_{aging}$',\n",
    "                'albdepth': r'$\\alpha_{depth}$','iceroughness': r'$z0_{ice}$'}, inplace=True)\n",
    "\n",
    "# -- PAIRPLOT --\n",
    "g = sns.pairplot(posi_samples_kde, kind=\"kde\", diag_kind=\"kde\", plot_kws={\"fill\": True, \"cmap\": \"crest\", \"levels\": 8}, diag_kws={\"fill\": True, \"color\": \"black\"}, corner=True)\n",
    "g.fig.set_size_inches(figsize)\n",
    "for ax in g.axes.flat:\n",
    "    if ax is not None:\n",
    "        ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=2))\n",
    "        ax.yaxis.set_major_locator(ticker.MaxNLocator(nbins=2))\n",
    "\n",
    "# -- SAVE FIGURE --\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/posterior_param_pairplots.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.posterior[\"sigma_alb_summer\"].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare integrated autocorr times\n",
    "comb_full = np.stack([ds.posterior[\"rrrfactor\"].values, ds.posterior[\"albice\"].values, ds.posterior[\"albsnow\"].values,\n",
    "                 ds.posterior[\"albfirn\"].values, ds.posterior[\"albaging\"].values, ds.posterior[\"albdepth\"].values,\n",
    "                 ds.posterior[\"iceroughness\"].values, ds.posterior[\"sigma_tsl_summer\"].values, ds.posterior[\"sigma_alb_summer\"].values])\n",
    "print(comb_full.shape)\n",
    "\n",
    "## For subset without bad chains:\n",
    "comb = np.stack([posterior_subset[\"rrrfactor\"].values, posterior_subset[\"albice\"].values, posterior_subset[\"albsnow\"].values,\n",
    "                 posterior_subset[\"albfirn\"].values, posterior_subset[\"albaging\"].values, posterior_subset[\"albdepth\"].values,\n",
    "                 posterior_subset[\"iceroughness\"].values, posterior_subset[\"sigma_tsl_summer\"].values, posterior_subset[\"sigma_alb_summer\"].values])\n",
    "print(comb.shape)\n",
    "\n",
    "params_in_order = [\"rrrfactor\", \"albice\", \"albsnow\", \"albfirn\", \"albaging\", \"albdepth\", \"iceroughness\", \"sigma_tsl_summer\", \"sigma_alb_summer\"]\n",
    "\n",
    "samples_full = comb_full.transpose(2, 1, 0)\n",
    "samples = comb.transpose(2, 1, 0)  # Now (n_samples, n_chains, n_param)\n",
    "  # shape: (n_samples, n_chains, n_param)\n",
    "print(samples.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples[5000:]\n",
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps, n_walkers, n_param = samples.shape\n",
    "ns = np.arange(5000, n_steps, 5000) #start 10k not 5k\n",
    "taus_by_param = [[] for _ in range(n_param)]  # One list per parameter\n",
    "\n",
    "for n in ns:\n",
    "    try:\n",
    "        tau = integrated_time(samples[:n, :, :], quiet=True)  # shape (n_param,)\n",
    "        for i in range(n_param):\n",
    "            taus_by_param[i].append(tau[i])\n",
    "    except emcee.autocorr.AutocorrError:\n",
    "        for i in range(n_param):\n",
    "            taus_by_param[i].append(np.nan)  # Pad with nan if not computable\n",
    "            \n",
    "## Repeat for full dataset\n",
    "n_steps_full, n_walkers_full, n_param_full = samples_full.shape\n",
    "ns = np.arange(5000, n_steps, 5000)\n",
    "taus_by_param_full = [[] for _ in range(n_param_full)]  # One list per parameter\n",
    "\n",
    "for n in ns:\n",
    "    try:\n",
    "        tau = integrated_time(samples_full[:n, :, :], quiet=True)  # shape (n_param,)\n",
    "        for i in range(n_param_full):\n",
    "            taus_by_param_full[i].append(tau[i])\n",
    "    except emcee.autocorr.AutocorrError:\n",
    "        for i in range(n_param_full):\n",
    "            taus_by_param_full[i].append(np.nan)  # Pad with nan if not computable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first stage results to paste into approach\n",
    "ds_first = az.from_netcdf(path+\"stage1_final_demczsyserr_posterior_combined.nc\")\n",
    "ds_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: Due to 2-stage approach, we need TSLA from this run but MB and ALB should come from first logl. run! \n",
    "# \n",
    "total_loglikes = ds.posterior.total_loglike.values\n",
    "print(total_loglikes.shape)\n",
    "total_loglikes = total_loglikes[:,:]\n",
    "print(total_loglikes.shape)\n",
    "\n",
    "mb_loglikes = ds_first.posterior.loglike_mb.values\n",
    "mb_loglikes = mb_loglikes[:,:,0,0]\n",
    "print(mb_loglikes.shape)\n",
    "\n",
    "alb_loglikes = ds_first.posterior.loglike_alb.values\n",
    "print(alb_loglikes.shape)\n",
    "\n",
    "tsla_loglikes = ds.posterior.loglike_tsl.values\n",
    "print(tsla_loglikes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_loglikes shape: (n_chains, n_steps)\n",
    "n_chains, n_steps = total_loglikes.shape\n",
    "\n",
    "# Compute mean log-likelihood per chain\n",
    "mean_ll = total_loglikes.mean(axis=1)\n",
    "print(mean_ll)\n",
    "\n",
    "# Sort chains from best (highest mean) to worst\n",
    "sorted_idx = np.argsort(-mean_ll)  # negative for descending\n",
    "\n",
    "# Generate colors from colormap (e.g., 'viridis', 'plasma', 'inferno')\n",
    "try:\n",
    "    cmap = cm.cividis_r  # Pick your preferred colormap\n",
    "except:\n",
    "    cmap = cm.viridis\n",
    "colors = [cmap(i / (n_chains - 1)) for i in range(n_chains)]  # evenly spaced colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_loglike_mean = mb_loglikes.mean(axis=0)\n",
    "tsla_loglike_mean = tsla_loglikes.mean(axis=0)\n",
    "alb_loglike_mean = alb_loglikes.mean(axis=0)\n",
    "total_loglike_mean = total_loglikes.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taus_array = np.array(taus_by_param_full)\n",
    "taus_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New plot - idea: 4 panels, Logls (3) + 1 integr. auto corr.\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 14), dpi=300, sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for rank, chain_idx in enumerate(sorted_idx):\n",
    "    if chain_idx < 15:\n",
    "        axes[0].plot(\n",
    "            mb_loglikes[chain_idx, :],\n",
    "            color=colors[rank], alpha=0.7, \n",
    "            label=f\"Chain {chain_idx} (Rank {rank+1})\"\n",
    "        )\n",
    "        #axes[0].axvline(x=5000, linestyle=\"dashed\", color=\"black\", lw=0.5)\n",
    "    \n",
    "    axes[1].plot(\n",
    "        tsla_loglikes[chain_idx, :],\n",
    "        color=colors[rank], alpha=0.7, \n",
    "        label=f\"Chain {chain_idx} (Rank {rank+1})\"\n",
    "    )\n",
    "    #axes[1].axvline(x=5000, linestyle=\"dashed\", color=\"black\", lw=0.5) \n",
    "    \n",
    "    if chain_idx < 15:   \n",
    "        axes[2].plot(\n",
    "            alb_loglikes[chain_idx, :],\n",
    "            color=colors[rank], alpha=0.7, \n",
    "            label=f\"Chain {chain_idx} (Rank {rank+1})\"\n",
    "        )\n",
    "        #axes[2].axvline(x=5000, linestyle=\"dashed\", color=\"black\", lw=0.5)\n",
    "\n",
    "# for each subplot add a red mean line\n",
    "axes[0].plot(mb_loglike_mean, color=\"black\", alpha=0.4, zorder=7)\n",
    "axes[1].plot(tsla_loglike_mean, color=\"black\", alpha=0.4, zorder=7)\n",
    "axes[2].plot(alb_loglike_mean, color=\"black\", alpha=0.4, zorder=7)\n",
    "#axes[3].plot(mb_loglike_mean, color=\"red\", alpha=0.6, zorder=7)\n",
    "\n",
    "\n",
    "# Add axes labels\n",
    "axes[0].set_ylabel(r'$\\mathcal{L}(B_{geod}|\\theta)$')\n",
    "axes[1].set_ylabel(r'$\\mathcal{L}(SLA|\\theta)$')\n",
    "axes[2].set_ylabel(r'$\\mathcal{L}(\\bar{\\alpha}|\\theta)$')\n",
    "\n",
    "for i in range(n_param_full):\n",
    "    axes[3].plot(ns, taus_by_param_full[i], label=f\"{params_in_order[i]}\", marker='o')\n",
    "\n",
    "#mean\n",
    "axes[3].plot(ns, taus_array.mean(axis=0), marker='o', color=\"black\", zorder=7)\n",
    "\n",
    "axes[3].set_xticks(np.arange(0, 100000+10000, 10000))\n",
    "axes[3].set_xticklabels(np.arange(0, 100000+10000, 10000), rotation=30)\n",
    "\n",
    "axes[2].set_xticks(np.arange(0, 100000+10000, 10000))\n",
    "axes[2].set_xticklabels(np.arange(0, 100000+10000, 10000), rotation=30)\n",
    "axes[2].set_xlabel(\"Samples in chains\")\n",
    "\n",
    "axes[3].set_yticks(np.arange(20, 40, 5))\n",
    "axes[3].set_ylabel(\"Integr. Autocorr. Time\")\n",
    "axes[3].set_xlabel(\"Samples in chains\")\n",
    "\n",
    "fig.text(0.01, 0.98, 'a)', transform=fig.transFigure, fontsize=24)\n",
    "fig.text(0.49, 0.98, 'b)', transform=fig.transFigure, fontsize=24)\n",
    "fig.text(0.01, 0.53, 'c)', transform=fig.transFigure, fontsize=24)\n",
    "fig.text(0.49, 0.53, 'd)', transform=fig.transFigure, fontsize=24)\n",
    "#\n",
    "\n",
    "# Add legend (example: fake classes or categories)\n",
    "y_label_dict = {'rrrfactor': r'$p_{f}$', 'albice': r'$\\alpha_{ice}$', 'albsnow': r'$\\alpha_{fs}$','albfirn': r'$\\alpha_{firn}$', 'albaging': r'$\\alpha_{aging}$',\n",
    "                'albdepth': r'$\\alpha_{depth}$','iceroughness': r'$z0_{ice}$', 'mb_logp': r'$\\mathcal{L}(MB|\\theta)$', 'sigma_tsl_summer': r'$\\sigma_{\\eta}^{SLA}$',\n",
    "                'sigma_alb_summer': r'$\\sigma_{\\eta}^{\\bar{\\alpha}}$'}\n",
    "#plt.subplots_adjust(wspace=0.0, hspace=0.4, right=0.9, bottom=0.1)\n",
    "fig.tight_layout()\n",
    "## rename params in legends\n",
    "handles, labels_old = axes[3].get_legend_handles_labels()\n",
    "labels = [y_label_dict[x] for x in labels_old]\n",
    "fig.legend(handles, labels,\n",
    "           loc='lower center', ncol=9, frameon=True, bbox_to_anchor=(0.5, -0.04))\n",
    "\n",
    "if 'win' in sys.platform:\n",
    "    plt.savefig(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/Fig05_mcmc_convergence_plots.png\", bbox_inches=\"tight\")\n",
    "else:\n",
    "    plt.savefig(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/Fig05_mcmc_convergence_plots.png\", bbox_inches=\"tight\")\n",
    "\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ARCHIVED because of two-stage approach now.\n",
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12), dpi=300, sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for rank, chain_idx in enumerate(sorted_idx):\n",
    "    axes[0].plot(\n",
    "        total_loglikes[chain_idx, :],\n",
    "        color=colors[rank], alpha=0.7, \n",
    "        label=f\"Chain {chain_idx} (Rank {rank+1})\"\n",
    "    )\n",
    "    axes[0].axvline(x=5000, linestyle=\"dashed\", color=\"black\", lw=0.5)\n",
    "    \n",
    "    axes[1].plot(\n",
    "        mb_loglikes[chain_idx, :],\n",
    "        color=colors[rank], alpha=0.7, \n",
    "        label=f\"Chain {chain_idx} (Rank {rank+1})\"\n",
    "    )\n",
    "    axes[1].axvline(x=5000, linestyle=\"dashed\", color=\"black\", lw=0.5) \n",
    "       \n",
    "    axes[2].plot(\n",
    "        tsla_loglikes[chain_idx, :],\n",
    "        color=colors[rank], alpha=0.7, \n",
    "        label=f\"Chain {chain_idx} (Rank {rank+1})\"\n",
    "    )\n",
    "    axes[2].axvline(x=5000, linestyle=\"dashed\", color=\"black\", lw=0.5)\n",
    "    \n",
    "    axes[3].plot(\n",
    "        alb_loglikes[chain_idx, :],\n",
    "        color=colors[rank], alpha=0.7, \n",
    "        label=f\"Chain {chain_idx} (Rank {rank+1})\"\n",
    "    )\n",
    "    axes[3].axvline(x=5000, linestyle=\"dashed\", color=\"black\", lw=0.5)\n",
    "# Add axes labels\n",
    "axes[0].set_ylabel(r'$\\mathcal{L}(total|\\theta)$')\n",
    "axes[1].set_ylabel(r'$\\mathcal{L}(MB|\\theta)$')\n",
    "axes[2].set_ylabel(r'$\\mathcal{L}(TSLA|\\theta)$')\n",
    "axes[3].set_ylabel(r'$\\mathcal{L}(ALB|\\theta)$')\n",
    "axes[3].set_yticks(np.arange(1, 1.6+0.2, 0.2))\n",
    "\n",
    "# Add colorbar to show ranking\n",
    "# Shared vertical colorbar for first 4 plots\n",
    "# Position: [left, bottom, width, height]\n",
    "cbar_ax = fig.add_axes([0.92, 0.37, 0.02, 0.52])  # spans top two rows\n",
    "sm = cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=0, vmax=n_chains - 1))\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, cax=cbar_ax)\n",
    "cbar.set_label(\"Chain Performance Rank\")\n",
    "\n",
    "for i in range(n_param_full):\n",
    "    axes[4].plot(ns, taus_by_param_full[i], label=f\"{params_in_order[i]}\", marker='o')\n",
    "\n",
    "for i in range(n_param):\n",
    "    axes[5].plot(ns, taus_by_param[i], label=f\"{params_in_order[i]}\", marker='o')    \n",
    "\n",
    "axes[4].set_xticks(np.arange(0, 100000+10000, 10000))\n",
    "axes[5].set_xticks(np.arange(0, 100000+10000, 10000))\n",
    "axes[4].set_xticklabels(np.arange(0, 100000+10000, 10000), rotation=30)\n",
    "axes[5].set_xticklabels(np.arange(0, 100000+10000, 10000), rotation=30)\n",
    "\n",
    "axes[4].set_yticks(np.arange(20, 65+15, 15))\n",
    "axes[5].set_yticks(np.arange(20, 45+5, 5))\n",
    "axes[4].set_ylabel(\"Integr. Autocorr. Time\")\n",
    "axes[4].set_xlabel(\"Samples in chains\")\n",
    "axes[5].set_xlabel(\"Samples in chains\")\n",
    "# Add legend (example: fake classes or categories)\n",
    "y_label_dict = {'rrrfactor': r'$p_{f}$', 'albice': r'$\\alpha_{ice}$', 'albsnow': r'$\\alpha_{fs}$','albfirn': r'$\\alpha_{firn}$', 'albaging': r'$\\alpha_{aging}$',\n",
    "                'albdepth': r'$\\alpha_{depth}$','iceroughness': r'$z0_{ice}$', 'mb_logp': r'$\\mathcal{L}(MB|\\theta)$', 'sigma_tsl_summer': r'$\\sigma_{tsla}^{sys}$',\n",
    "                'sigma_alb_summer': r'$\\sigma_{alb}^{sys}$'}\n",
    "\n",
    "handles, labels_old = axes[4].get_legend_handles_labels()\n",
    "labels = [y_label_dict[x] for x in labels_old]\n",
    "fig.legend(handles, labels,\n",
    "           loc='lower center', ncol=7, frameon=True, bbox_to_anchor=(0.5, -0.07))\n",
    "#plt.subplots_adjust(wspace=0.0, hspace=0.4, right=0.9, bottom=0.1)\n",
    "#fig.tight_layout()\n",
    "## rename params in legends\n",
    "if 'win' in sys.platform:\n",
    "    plt.savefig(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/mcmc_convergence_plots.png\", bbox_inches=\"tight\")\n",
    "else:\n",
    "    plt.savefig(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/mcmc_convergence_plots.png\", bbox_inches=\"tight\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load priors and add to plot below\n",
    "\"\"\"\n",
    "with pm.Model() as model:\n",
    "    #Stage 1 Params: TSLA + ALB only\n",
    "    rrr = pm.TruncatedNormal('rrrfactor', mu=0.7785, sigma=0.781, lower=0.648, upper=0.946)\n",
    "    snow = pm.TruncatedNormal(\"albsnow\", mu=0.903, sigma=0.1, lower=0.887, upper=0.928)\n",
    "    ice = pm.TruncatedNormal(\"albice\", mu=0.17523, sigma=0.1, lower=0.1182, upper=0.2302)\n",
    "    firn = pm.TruncatedNormal(\"albfirn\", mu=0.6036, sigma=0.1, lower=0.51, upper=0.6747)\n",
    "    aging = pm.TruncatedNormal(\"albaging\", mu=13.82, sigma=5.372, lower=5, upper=24.77)\n",
    "    depth = pm.TruncatedNormal(\"albdepth\", mu=1.776, sigma=0.666, lower=1.0, upper=4)\n",
    "    rough = pm.TruncatedNormal(\"iceroughness\", mu=8.612, sigma=9, lower=1.2, upper=19.65)\n",
    "    sigma_alb_summer = pm.HalfNormal(\"sigma_alb_summer\", sigma=0.02)\n",
    "    sigma_tsl_summer = pm.HalfNormal(\"sigma_tsl_summer\", sigma=0.03)\n",
    "\"\"\"\n",
    "from scipy.stats import truncnorm, halfnorm\n",
    "\n",
    "# Helper function to convert bounds to a, b for truncnorm\n",
    "def get_truncnorm_samples(mu, sigma, lower, upper, size=1000):\n",
    "    a, b = (lower - mu) / sigma, (upper - mu) / sigma\n",
    "    return truncnorm.rvs(a, b, loc=mu, scale=sigma, size=size)\n",
    "\n",
    "# Sample size\n",
    "n_samples = 100000\n",
    "\n",
    "# Define priors and sample\n",
    "priors = {\n",
    "    'rrrfactor': get_truncnorm_samples(0.7785, 0.0781, 0.648, 0.946, n_samples),\n",
    "    'albsnow': get_truncnorm_samples(0.903, 0.1, 0.887, 0.928, n_samples),\n",
    "    'albice': get_truncnorm_samples(0.17523, 0.1, 0.1182, 0.2302, n_samples),\n",
    "    'albfirn': get_truncnorm_samples(0.6036, 0.1, 0.51, 0.6747, n_samples),\n",
    "    'albaging': get_truncnorm_samples(13.82, 5.372, 5, 24.77, n_samples),\n",
    "    'albdepth': get_truncnorm_samples(1.776, 0.666, 1.0, 4, n_samples),\n",
    "    'iceroughness': get_truncnorm_samples(8.612, 9, 1.2, 19.65, n_samples),\n",
    "    'sigma_alb_summer': halfnorm.rvs(scale=0.02, size=n_samples),\n",
    "    'sigma_tsl_summer': halfnorm.rvs(scale=0.03, size=n_samples),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "sns.kdeplot(priors['albsnow'], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom plot for chains based on az.trace\n",
    "chains = comb_full.transpose(1, 2, 0)\n",
    "#select from 5000 onwards\n",
    "chains = chains[:,5000:,:]\n",
    "print(chains.shape)\n",
    "\n",
    "n_chains, n_samples, n_params = chains.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "fig, axes = plt.subplots(n_params, 2, figsize=(12, 2.5 * n_params), dpi=300)\n",
    "if n_params == 1:\n",
    "    axes = np.expand_dims(axes, 0)\n",
    "\n",
    "for i in range(n_params):\n",
    "    ax_trace = axes[i, 0]\n",
    "    ax_kde = axes[i, 1]\n",
    "\n",
    "    # Plot each chain, color-coded by performance rank\n",
    "    for rank, chain_idx in enumerate(sorted_idx):\n",
    "        chain_data = chains[chain_idx, :, i]\n",
    "        ax_trace.plot(\n",
    "            chain_data,\n",
    "            color=colors[rank],\n",
    "            alpha=0.7,\n",
    "            label=f\"Chain {chain_idx} (Rank {rank+1})\" if i == 0 else None\n",
    "        )\n",
    "    \n",
    "    mean_trace = np.mean(chains[:, :, i], axis=0)\n",
    "\n",
    "    # Plot the mean trace line\n",
    "    ax_trace.plot(\n",
    "        mean_trace,\n",
    "        color='black',\n",
    "        alpha=0.4,\n",
    "        #label='Mean Trace' if i == 0 else None\n",
    "    )\n",
    "    # Median and 95% CI from all chains combined\n",
    "    combined_samples = chains[:, :, i].flatten()\n",
    "    median = np.median(combined_samples)\n",
    "    lower, upper = np.percentile(combined_samples, [2.5, 97.5])\n",
    "\n",
    "    # Add horizontal lines to trace plot\n",
    "    #ax_trace.axhline(median, color='black', linestyle='--', label='Median' if i == 0 else None)\n",
    "    #ax_trace.axhline(lower, color='gray', linestyle=':', label='95% CI' if i == 0 else None)\n",
    "    #ax_trace.axhline(upper, color='gray', linestyle=':')\n",
    "    \n",
    "\n",
    "    ax_trace.set_ylabel(y_label_dict[params_in_order[i]])\n",
    "    ax_trace.set_xlabel(\"\")\n",
    "    if i != 8:\n",
    "        ax_trace.set_xticklabels(\"\")\n",
    "\n",
    "\n",
    "    # KDE plot\n",
    "    sns.kdeplot(priors[params_in_order[i]], fill=True, ax=ax_kde, color=\"coral\", alpha=0.7, bw_adjust=0.7, common_norm=False, label=\"Prior\")\n",
    "    sns.kdeplot(combined_samples, fill=True, ax=ax_kde, color=\"slateblue\", alpha=0.7, bw_adjust=0.7, common_norm=False, label=\"Posterior\")\n",
    "    ax_kde.axvline(median, color='black', linestyle='--')\n",
    "    ax_kde.axvline(lower, color='gray', linestyle=':')\n",
    "    ax_kde.axvline(upper, color='gray', linestyle=':')\n",
    "    ax_kde.set_ylabel(\"\")\n",
    "\n",
    "    #if i == n_params - 1:\n",
    "    #    ax_kde.set_xlabel(params_in_order[i])\n",
    "    \n",
    "y_top = 0.990\n",
    "y_bottom = 0.120\n",
    "rows = 9  # number of rows\n",
    "step = (y_top - y_bottom) / (rows - 1)\n",
    "\n",
    "labels = [chr(i) + ')' for i in range(ord('a'), ord('r') + 1)]\n",
    "\n",
    "for i in range(rows):\n",
    "    y = y_top - i * step\n",
    "    # Left column\n",
    "    fig.text(0.02, y, labels[2*i], transform=fig.transFigure, fontsize=22)\n",
    "    # Right column\n",
    "    fig.text(0.52, y, labels[2*i+1], transform=fig.transFigure, fontsize=22)\n",
    "    \n",
    "axes[0, 1].legend()\n",
    "axes[-1,0].set_xlabel(\"Samples in chains\")\n",
    "fig.tight_layout()\n",
    "\n",
    "if 'win' in sys.platform:\n",
    "    plt.savefig(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/FigA01_mcmc_chains_traces_plots.png\", bbox_inches=\"tight\")\n",
    "else:\n",
    "    plt.savefig(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/FigA01_mcmc_chains_traces_plots.png\", bbox_inches=\"tight\")\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "#last ticks necessary, density label off\n",
    "\n",
    "# Add shared legend\n",
    "#handles, labels = ax_trace.get_legend_handles_labels()\n",
    "#fig.legend(handles, labels, loc=\"upper right\", bbox_to_anchor=(0.95, 0.98), frameon=False)\n",
    "\n",
    "#plt.tight_layout(rect=[0, 0, 0.93, 1])\n",
    "#plt.show()\n",
    "\n",
    "#label first y-axis based on plot dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify chains to drop ...\n",
    "az.rhat(ds, var_names=param_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.ess(ds, var_names=param_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_subset.mu_mb[:,:,0].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute stats\n",
    "az.ess(posterior_subset, var_names=param_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.rhat(posterior_subset, var_names=param_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taus = np.array(taus_by_param)\n",
    "np.max(taus, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Thinnen and check...\n",
    "# Thin posterior by keeping every 45th sample\n",
    "#ds.posterior.sel(chain=best_chain_idx)\n",
    "subset = posterior_subset.sel(draw=slice(5000, None)) #discard first 5000\n",
    "idata_thinned = subset.sel(draw=slice(None, None, 40)) #thin by 45\n",
    "idata_thinned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata_thinned, var_names=param_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create size\n",
    "post_param_holder = idata_thinned[\"rrrfactor\"].values\n",
    "flat_param_holder = post_param_holder.reshape(-1)\n",
    "flat_param_size = flat_param_holder.size\n",
    "random_indices = np.random.choice(flat_param_size, size=300, replace=False) #generate 300 random samples\n",
    "random_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample n=200 and check\n",
    "dic_samples = {}\n",
    "for param in param_names + [\"sigma_tsl_summer\",\"sigma_alb_summer\"]:\n",
    "    print(\"Sampling \", param)\n",
    "    post_param = idata_thinned[param].values\n",
    "    print(post_param.shape)\n",
    "    \n",
    "    flat_param = post_param.reshape(-1)  # shape: (n_chains * n_samples,)\n",
    "    print(flat_param.shape)\n",
    "\n",
    "    final_samples = flat_param[random_indices]\n",
    "    dic_samples[param] = final_samples\n",
    "    \n",
    "\n",
    "dic_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dic_samples)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to dict of variables (each as shape (1, 200))\n",
    "df_copy = df.copy()\n",
    "df_copy[\"chain\"] = 0\n",
    "df_copy[\"draw\"] = np.arange(len(df_copy), dtype=int)\n",
    "df_copy = df_copy.set_index([\"chain\", \"draw\"])\n",
    "xdata = xr.Dataset.from_dataframe(df_copy)\n",
    "\n",
    "dataset = az.InferenceData(posterior=xdata)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(dataset, var_names=param_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_for_spotpy = {'rrrfactor': 'parRRR_factor', 'albice': 'paralb_ice', 'albsnow': 'paralb_snow',\n",
    "                           'albfirn': 'paralb_firn', 'albaging': 'paralbedo_aging', 'albdepth': 'paralbedo_depth',\n",
    "                           'iceroughness': 'parroughness_ice'}\n",
    "\n",
    "df = df.rename(columns=column_names_for_spotpy)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "if save != False:\n",
    "    if 'win' in sys.platform:\n",
    "        df.to_csv(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/LHS/final_samples_mcmc.csv\", index=False)\n",
    "    else:\n",
    "        df.to_csv(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/LHS/final_samples_mcmc.csv\", index=False)\n",
    "        \n",
    "## Store these to .csv and run the full COSIPY model with them (no emulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load for vali plots - seed not set, so be careful with saving###\n",
    "try:\n",
    "    del df\n",
    "except:\n",
    "    pass\n",
    "df = pd.read_csv(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/LHS/final_samples_mcmc.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load albedo data\n",
    "if 'win' in sys.platform:\n",
    "    albobs = xr.open_dataset(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/HEF_processed_HRZ-30CC-filter_albedos.nc\")\n",
    "    tsla = pd.read_csv(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/snowlines/HEF-snowlines-1999-2010_manual_filtered.csv\")\n",
    "else:\n",
    "    albobs = xr.open_dataset(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/HEF_processed_HRZ-30CC-filter_albedos.nc\")\n",
    "    tsla = pd.read_csv(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/snowlines/HEF-snowlines-1999-2010_manual_filtered.csv\")\n",
    "albobs = albobs.sortby(\"time\")\n",
    "\n",
    "## Load snowlines\n",
    "time_start_dt = pd.to_datetime(\"2000-01-01\") #config starts with spinup - need to add 1year\n",
    "time_end_dt = pd.to_datetime(\"2009-12-31\")\n",
    "\n",
    "\n",
    "tsla_true_obs = tsla.copy()\n",
    "tsla_true_obs['LS_DATE'] = pd.to_datetime(tsla_true_obs['LS_DATE'])\n",
    "print(\"Start date:\", time_start_dt)\n",
    "print(\"End date:\", time_end_dt)\n",
    "tsla_true_obs = tsla_true_obs.loc[(tsla_true_obs['LS_DATE'] > time_start_dt) & (tsla_true_obs['LS_DATE'] <= time_end_dt)]\n",
    "tsla_true_obs.set_index('LS_DATE', inplace=True)\n",
    "#Normalize standard deviation if necessary\n",
    "tsla_true_obs['SC_stdev'] = (tsla_true_obs['SC_stdev']) / (tsla_true_obs['glacier_DEM_max'] - tsla_true_obs['glacier_DEM_min'])\n",
    "\n",
    "thres_unc = (20) / (tsla_true_obs['glacier_DEM_max'].iloc[0] - tsla_true_obs['glacier_DEM_min'].iloc[0])\n",
    "print(thres_unc)\n",
    "\n",
    "## Set observational uncertainty where smaller to atleast model resolution (20m) and where larger keep it\n",
    "sc_norm = np.where(tsla_true_obs['SC_stdev'] < thres_unc, thres_unc, tsla_true_obs['SC_stdev'])\n",
    "tsla_true_obs['SC_stdev'] = sc_norm\n",
    "\n",
    "## Load MB\n",
    "rgi_id = \"RGI60-11.00897\"\n",
    "if 'win' in sys.platform:\n",
    "    geod_ref = pd.read_csv(\"E:/OneDrive/PhD/PhD/Data/Hugonnet_21_MB/dh_11_rgi60_pergla_rates.csv\")\n",
    "else:\n",
    "    geod_ref = pd.read_csv(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hugonnet_21_MB/dh_11_rgi60_pergla_rates.csv\")\n",
    "geod_ref = geod_ref.loc[geod_ref['rgiid'] == rgi_id]\n",
    "geod_ref = geod_ref.loc[geod_ref['period'] == \"2000-01-01_2010-01-01\"]\n",
    "#geod_ref = geod_ref[['dmdtda', 'err_dmdtda']]\n",
    "print(geod_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tsla_true_obs['SC_stdev'].max())\n",
    "print(albobs.sigma_albedo.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need to run the cirrus scripts before we can do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_lookup = {\n",
    "    12: \"winter\", 1: \"winter\", 2: \"winter\",\n",
    "    3: \"winter\", 4: \"winter\", 5: \"summer\",\n",
    "    6: \"summer\", 7: \"summer\", 8: \"summer\",\n",
    "    9: \"summer\", 10: \"winter\", 11: \"winter\"\n",
    "}\n",
    "\n",
    "months = albobs[\"time\"].dt.month\n",
    "season_str = xr.DataArray([season_lookup[m.item()] for m in months], coords={\"time\": albobs[\"time\"]}, dims=\"time\")\n",
    "albobs = albobs.assign_coords(season=season_str)\n",
    "albobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'win' in sys.platform:\n",
    "    nc_files = sorted(pathlib.Path(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Output/albedo_files/MCMC/\").glob(\"*.nc\"))\n",
    "else:\n",
    "    nc_files = sorted(pathlib.Path(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Output/albedo_files/MCMC/\").glob(\"*.nc\"))\n",
    "\n",
    "# Prepare containers\n",
    "filenames = []\n",
    "mb_annual_means = []\n",
    "ppc_albdata = []\n",
    "alb_data = []\n",
    "albfull_data = []\n",
    "#alb_cilower = []\n",
    "#alb_ciupper = []\n",
    "\n",
    "mb_annual_means = []\n",
    "ppc_mb_means = []\n",
    "mb_cumul_means = []\n",
    "\n",
    "np.random.seed(77)\n",
    "\n",
    "for fp in nc_files:\n",
    "    print(fp)\n",
    "    ds = xr.open_dataset(fp).sel(time=slice(\"2000-01-01\",\"2010-01-01\")) # last timestamp is 2009-12-31T23:00:00\n",
    "    albfull_data.append(ds['ALBEDO_weighted'].data)\n",
    "    fname = str(fp).split('MCMC-ensemble')[-1]\n",
    "    # get num for lookup of sys err #\n",
    "    id = int(fname.split('.nc')[0].split('num')[-1]) - 3\n",
    "    sys_err_alb = df.loc[id, 'sigma_alb_summer']\n",
    "    filenames.append(str(fname))\n",
    "\n",
    "    # MASS BALANCE: mean over full time\n",
    "    mb_mean = ds['MB_weighted'].groupby('time.year').sum('time').mean().item()\n",
    "    daily_mb = ds['MB_weighted'].resample(time=\"1D\").sum().data\n",
    "    mb_annual_means.append(mb_mean)\n",
    "    \n",
    "    # Here we only use the observational uncertainty of the mean annual value\n",
    "    y_pred_mean_annual_mb_i = np.random.normal(loc=mb_mean, scale=geod_ref['err_dmdtda'].item())\n",
    "    ppc_mb_means.append(y_pred_mean_annual_mb_i)\n",
    "\n",
    "    mb_cumul_means.append(np.cumsum(daily_mb))\n",
    "    \n",
    "    # ALBEDO: filter time period & summarize\n",
    "    alb_filtered = ds['ALBEDO_weighted'].sel(time=albobs.time)\n",
    "    #alb_ci975 = np.percentile(alb_filtered, 97.5)\n",
    "    #alb_ci025 = np.percentile(alb_filtered, 2.5)\n",
    "    mu_i = alb_filtered.data\n",
    "    alb_data.append(mu_i)\n",
    "\n",
    "    sigma_obs = albobs['sigma_albedo'].values\n",
    "    is_summer = (albobs['season'].values == \"summer\")\n",
    "\n",
    "    # Build a vector of systematic error sigmas aligned with the timeseries\n",
    "    sigma_sys_vector_i = np.where(is_summer, sys_err_alb, 0)\n",
    "    \n",
    "    # Combine errors by adding variances, then taking the square root\n",
    "    sigma_total_i = np.sqrt(sigma_obs**2 + sigma_sys_vector_i**2)\n",
    "    \n",
    "    # Draw one plausible \"reality\" from the predictive distribution\n",
    "    y_pred_i = np.random.normal(loc=mu_i, scale=sigma_total_i)\n",
    "    ppc_albdata.append(y_pred_i)\n",
    "    #alb_cilower.append(alb_ci025.data)\n",
    "    #alb_ciupper.append(alb_ci975.data)\n",
    "    \n",
    "# --- Post-Processing: Convert lists to NumPy arrays ---\n",
    "# This is your parametric uncertainty\n",
    "model_runs_arr = np.array(alb_data) \n",
    "# This is your total predictive uncertainty\n",
    "simulated_predictions_arr = np.array(ppc_albdata)\n",
    "\n",
    "print(f\"Shape of model runs (mu) array: {model_runs_arr.shape}\")\n",
    "print(f\"Shape of predictive ensemble (Y_pred) array: {simulated_predictions_arr.shape}\")\n",
    "\n",
    "# For VALIDATION (Posterior Predictive Check)\n",
    "print(\"\\nCalculating statistics for validation plot...\")\n",
    "# Central line is the median of the predictive ensemble\n",
    "ppc_median = np.median(simulated_predictions_arr, axis=0)\n",
    "# Uncertainty band is the 95% interval of the predictive ensemble\n",
    "ppc_lower, ppc_upper = np.percentile(simulated_predictions_arr, [2.5, 97.5], axis=0)\n",
    "\n",
    "mb_model_runs_mean_annual_arr = np.array(mb_annual_means)\n",
    "mb_simulated_predictions_mean_annual_arr = np.array(ppc_mb_means)\n",
    "\n",
    "mb_model_runs_cumulative_arr = np.array(mb_cumul_means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print modes and sd\n",
    "#mb_simulated_predictions_mean_annual_arr\n",
    "print(geod_ref['dmdtda'], geod_ref['err_dmdtda'])\n",
    "print(np.mean(mb_model_runs_mean_annual_arr), np.std(mb_model_runs_mean_annual_arr))\n",
    "# cohens d (how large and meaningufl is model bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do some stats testing on the MB distributions. Inspired by ChatGPT\n",
    "\n",
    "observed_values = np.random.normal(loc=geod_ref['dmdtda'].item(), scale=geod_ref['err_dmdtda'].item(), size=300)\n",
    "\n",
    "model_mean = np.mean(mb_model_runs_mean_annual_arr)\n",
    "model_std = np.std(mb_model_runs_mean_annual_arr, ddof=1) # ddof=1 for sample std dev\n",
    "model_skew = stats.skew(mb_model_runs_mean_annual_arr)\n",
    "model_kurtosis = stats.kurtosis(mb_model_runs_mean_annual_arr) # \n",
    "\n",
    "# Calculate stats for the (synthetic) observed distribution\n",
    "synth_obs_mean = np.mean(observed_values)\n",
    "synth_obs_std = np.std(observed_values, ddof=1)\n",
    "synth_obs_skew = stats.skew(observed_values)\n",
    "synth_obs_kurtosis = stats.kurtosis(observed_values)\n",
    "\n",
    "# Print the results in a formatted table\n",
    "print(f\"{'Metric':<20} {'Modelled':<15} {'Observed (Synthetic)':<20}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Mean':<20} {model_mean:<15.3f} {synth_obs_mean:<20.3f}\")\n",
    "print(f\"{'Standard Deviation':<20} {model_std:<15.3f} {synth_obs_std:<20.3f}\")\n",
    "print(f\"{'Skewness':<20} {model_skew:<15.3f} {synth_obs_skew:<20.3f}\")\n",
    "print(f\"{'Kurtosis (Excess)':<20} {model_kurtosis:<15.3f} {synth_obs_kurtosis:<20.3f}\\n\")\n",
    "print(\"Interpretation:\")\n",
    "print(\"- Skewness close to 0 indicates symmetry.\")\n",
    "print(\"- Excess Kurtosis close to 0 indicates a peak/tail profile similar to a normal distribution.\\n\")\n",
    "\n",
    "def calculate_cohens_d(group1, group2):\n",
    "    \"\"\"Calculates Cohen's d for independent samples.\"\"\"\n",
    "    # Number of samples\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    # Means\n",
    "    mean1, mean2 = np.mean(group1), np.mean(group2)\n",
    "    # Sample standard deviations\n",
    "    s1, s2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n",
    "    \n",
    "    # Calculate the pooled standard deviation\n",
    "    pooled_std = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2))\n",
    "    \n",
    "    # Calculate Cohen's d\n",
    "    d = (mean1 - mean2) / pooled_std\n",
    "    return d\n",
    "\n",
    "print(\"--- Quantifying Bias with Cohen's d ---\")\n",
    "cohen_d_value = calculate_cohens_d(mb_model_runs_mean_annual_arr, observed_values)\n",
    "print(f\"Cohen's d: {cohen_d_value:.3f}\")\n",
    "\n",
    "# Interpretation of Cohen's d\n",
    "if abs(cohen_d_value) >= 0.8:\n",
    "    print(\"Interpretation: This is a LARGE effect size, indicating a substantial and meaningful difference between the two means.\\n\")\n",
    "elif abs(cohen_d_value) >= 0.5:\n",
    "    print(\"Interpretation: This is a MEDIUM effect size.\\n\")\n",
    "else:\n",
    "    print(\"Interpretation: This is a SMALL effect size.\\n\")\n",
    "\n",
    "print(\"--- Formal Shape Comparison (K-S Test) ---\")\n",
    "# Standardize both datasets to have a mean of 0 and std of 1.\n",
    "# This removes the effect of location and scale, leaving only the shape.\n",
    "modelled_standardized = (mb_model_runs_mean_annual_arr - model_mean) / model_std\n",
    "observed_standardized = (observed_values - synth_obs_mean) / synth_obs_std\n",
    "\n",
    "# Perform the two-sample Kolmogorov-Smirnov test\n",
    "ks_statistic, p_value = stats.ks_2samp(modelled_standardized, observed_standardized)\n",
    "\n",
    "print(f\"K-S Statistic: {ks_statistic:.3f}\")\n",
    "print(f\"P-value: {p_value:.3f}\")\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"The K-S test checks if the two (standardized) samples come from the same distribution.\")\n",
    "if p_value < 0.05:\n",
    "    print(\"Result: The p-value is less than 0.05. We REJECT the null hypothesis.\")\n",
    "    print(\"This suggests the underlying shapes of the distributions are statistically different.\")\n",
    "else:\n",
    "    print(\"Result: The p-value is greater than 0.05. We FAIL to reject the null hypothesis.\")\n",
    "    print(\"This provides evidence that the underlying shapes are NOT statistically different, supporting the visual observation that 'the shapes match'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare data into a dataframe\n",
    "mb_df = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'value': mb_annual_means\n",
    "})\n",
    "mb_df.value.hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_rng = pd.date_range(\"2000-01-01\", \"2009-12-31\", freq='D')\n",
    "cumulative_mb_ensemble = xr.DataArray(\n",
    "    data=mb_model_runs_cumulative_arr,\n",
    "    dims=(\"simulation\", \"time\"),\n",
    "    coords={\n",
    "        \"simulation\": np.arange(300),\n",
    "        \"time\": time_rng\n",
    "    },\n",
    "    name=\"cumulative_mass_balance\"\n",
    ")\n",
    "\n",
    "cummb_median = cumulative_mb_ensemble.median(dim='simulation')\n",
    "cummb_cilower = cumulative_mb_ensemble.quantile(0.025, dim=\"simulation\")\n",
    "cummb_ciupper = cumulative_mb_ensemble.quantile(0.975, dim=\"simulation\")\n",
    "\n",
    "cumulative_mb_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_true_obs['month'] = tsla_true_obs.index.month\n",
    "tsla_true_obs['season'] = np.where(tsla_true_obs['month'].isin([5,6,7,8,9]), 'summer', 'winter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load snowlines\n",
    "# Get all filenames\n",
    "if 'win' in sys.platform:\n",
    "    csv_files = sorted(pathlib.Path(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Output/snowlines/bestfiles\").glob(\"*.csv\"))  # sort to keep order consistent\n",
    "else:\n",
    "    csv_files = sorted(pathlib.Path(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Output/snowlines/bestfiles\").glob(\"*.csv\"))  # sort to keep order consistent\n",
    "\n",
    "# Make sure filenames are clean column names\n",
    "#filenames = [fp.name for fp in csv_files]\n",
    "\n",
    "# Load all CSVs into a list of DataFrames, extracting the same column (e.g., 'snowline')\n",
    "# Assume 'time' column is the same in all files and you want 'snowline' values\n",
    "# Prepare containers\n",
    "snowline_model_runs = []\n",
    "snowline_simulated_predictions = []\n",
    "filenames_list = []\n",
    "\n",
    "for i, fp in enumerate(csv_files):\n",
    "    tslsub = pd.read_csv(fp, parse_dates=True, index_col='time')  # assumes time is the index\n",
    "    id = int(str(fp.stem).split('num')[-1]) - 3 \n",
    "    sys_err_offset = df.loc[id, 'sigma_tsl_summer']\n",
    "    tslsub = tslsub.loc[tslsub.index.isin(tsla_true_obs.index)]\n",
    "    \n",
    "    # Store the original data Series\n",
    "    mu_tsl_i = tslsub['Med_TSL'].values\n",
    "    snowline_model_runs.append(mu_tsl_i)\n",
    "    \n",
    "    sigma_obs_tsl = tsla_true_obs['SC_stdev'].values\n",
    "    is_summer_tsl = (tsla_true_obs['season'].values == \"summer\")\n",
    "    \n",
    "    sigma_sys_tsl_vector_i = np.where(is_summer_tsl, sys_err_offset, 0)\n",
    "    \n",
    "    # Combine errors by adding variances, then taking the square root\n",
    "    sigma_total_tsl_i = np.sqrt(sigma_obs_tsl**2 + sigma_sys_tsl_vector_i**2)\n",
    "    y_pred_tsl_i = np.random.normal(loc=mu_tsl_i, scale=sigma_total_tsl_i)\n",
    "    snowline_simulated_predictions.append(y_pred_tsl_i)\n",
    "    filenames_list.append(fp.name)\n",
    "\n",
    "snowline_model_runs_arr = np.array(snowline_model_runs) \n",
    "# This is your total predictive uncertainty\n",
    "snowline_simulated_predictions_arr = np.array(snowline_simulated_predictions)\n",
    "\n",
    "print(f\"Shape of snowline model runs (mu) array: {snowline_model_runs_arr.shape}\")\n",
    "print(f\"Shape of snowline predictive ensemble (Y_pred) array: {snowline_simulated_predictions_arr.shape}\")\n",
    "\n",
    "# For VALIDATION (Posterior Predictive Check)\n",
    "print(\"\\nCalculating statistics for snowline validation plot...\")\n",
    "# Central line is the median of the predictive ensemble\n",
    "ppc_tsl_median = np.median(snowline_simulated_predictions_arr, axis=0)\n",
    "# Uncertainty band is the 95% interval of the predictive ensemble\n",
    "ppc_tsl_lower, ppc_tsl_upper = np.percentile(snowline_simulated_predictions_arr, [2.5, 97.5], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load WGMS for cumulative MB\n",
    "if 'win' in sys.platform:\n",
    "    wgms_path = \"E:/OneDrive/PhD/PhD/Data/DOI-WGMS-FoG-2022-09/data/\"\n",
    "else:\n",
    "    wgms_path = \"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/DOI-WGMS-FoG-2022-09/data/\"\n",
    "\n",
    "wgms = pd.read_csv(wgms_path+\"mass_balance.csv\")\n",
    "wgms = wgms.loc[(wgms['NAME'] == \"HINTEREIS F.\") & (wgms['YEAR'] > 2001) & (wgms['YEAR'] <= 2009)]\n",
    "print(wgms.NAME.iloc[0], np.unique(wgms.WGMS_ID))\n",
    "wgms.drop(['POLITICAL_UNIT', 'NAME','REMARKS'], axis=1, inplace=True)\n",
    "wgms = wgms.loc[wgms['LOWER_BOUND'] == 9999]\n",
    "wgms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare data\n",
    "# Define WGMS hydrological year-end as Sept 30\n",
    "wgms['hydro_date'] = pd.to_datetime(wgms['YEAR'].astype(str)) + pd.DateOffset(months=8, days=30)\n",
    "wgms['CUM_BALANCE'] = wgms['ANNUAL_BALANCE'].cumsum()\n",
    "wgms['CUM_BALANCE'] = wgms['CUM_BALANCE'] / 1000\n",
    "wgms['ANNUAL_BALANCE_UNC'] = wgms['ANNUAL_BALANCE_UNC'] / 1000\n",
    "\n",
    "klug_etal_geod = np.array([-0.685,-2.713,-0.654,-1.028,-2.091,-1.363,-1.252,-1.209])\n",
    "klug_etal_unc = np.array([0.062, 0.183, 0.063, 0.056, 0.1, 0.041, 0.046, 0.06])\n",
    "wgms['klug_mb'] = klug_etal_geod\n",
    "wgms['klug_unc'] = klug_etal_unc\n",
    "wgms['CUM_KLUG'] = wgms['klug_mb'].cumsum()\n",
    "wgms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10), dpi=300)\n",
    "\n",
    "# 1. Histogram of Mass Balance\n",
    "#axes[0, 0].hist(mb_vals, bins=20, color='skyblue', edgecolor='black')\n",
    "sns.kdeplot(mb_vals, fill=True, color=\"skyblue\", ax=axes[0,0])\n",
    "#\n",
    "mu_obs = geod_ref['dmdtda'].item()  # mean observed MB in m w.e.\n",
    "sigma_obs = geod_ref['err_dmdtda'].item()\n",
    "\n",
    "# Plot PDF\n",
    "x = np.linspace(mu_obs - 4*sigma_obs, mu_obs + 4*sigma_obs, 200)\n",
    "pdf = norm.pdf(x, loc=mu_obs, scale=sigma_obs)\n",
    "axes[0, 0].plot(x, pdf, linestyle=\"dashed\", color=\"black\")\n",
    "\n",
    "#axes[0, 0].axvline(x=geod_ref['dmdtda'].item(), color=\"black\")\n",
    "#axes[0, 0].axvline(x=(geod_ref['dmdtda'] - geod_ref['err_dmdtda']).item(), linestyle=\"dashed\", color=\"black\")\n",
    "#axes[0, 0].axvline(x=(geod_ref['dmdtda'] + geod_ref['err_dmdtda']).item(), linestyle=\"dashed\", color=\"black\")\n",
    "axes[0, 0].set_xlabel('MB (m w.e. a$^{-1}$)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# 2. Albedo scatter\n",
    "axes[0, 1].errorbar(median_alb, albobs.median_albedo.values,\n",
    "                    xerr=alb_err, yerr=albobs.sigma_albedo.values,\n",
    "                    fmt='o', alpha=0.6, capsize=3, color='orange')\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(albobs.median_albedo.values, median_alb)\n",
    "rmse = root_mean_squared_error(albobs.median_albedo.values, median_alb)\n",
    "mae = mean_absolute_error(albobs.median_albedo.values, median_alb)\n",
    "\n",
    "axes[0, 1].text(0.05, 0.95, f\"RÂ² = {r2:.2f}\\nRMSE = {rmse:.2f}\\nMAE = {mae:.2f}\",\n",
    "                transform=axes[0, 1].transAxes,\n",
    "                verticalalignment='top',\n",
    "                fontsize=16,\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "axes[0, 1].axline((0, 0), slope=1, linestyle='--', color='gray')\n",
    "axes[0, 1].set_xlabel('Mod. Albedo (-)')\n",
    "axes[0, 1].set_ylabel('Obs. Albedo (-)')\n",
    "axes[0, 1].set_xlim(0, 1)\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "axes[0, 1].grid()\n",
    "\n",
    "# 3. Snowline scatter\n",
    "axes[0, 2].errorbar(median_tsla, tsla_true_obs['TSL_normalized'].values,\n",
    "                    xerr=tsla_err, yerr=tsla_true_obs['SC_stdev'].values,\n",
    "                    fmt='o', alpha=0.6, capsize=3, color='green')\n",
    "\n",
    "# Plot the highlighted points\n",
    "#axes[0, 2].errorbar(highlighted_model_tsla, highlighted_obs_tsla,\n",
    "#                    xerr=highlighted_model_sigma, yerr=highlighted_obs_sigma,\n",
    "#                    fmt='o', alpha=0.6, capsize=3, color='green')\n",
    "                \n",
    "\n",
    "r2 = r2_score(tsla_true_obs['TSL_normalized'].values, median_tsla)\n",
    "rmse = root_mean_squared_error(tsla_true_obs['TSL_normalized'].values, median_tsla)\n",
    "mae = mean_absolute_error(tsla_true_obs['TSL_normalized'].values, median_tsla)\n",
    "\n",
    "axes[0, 2].text(0.05, 0.95, f\"RÂ² = {r2:.2f}\\nRMSE = {rmse:.2f}\\nMAE = {mae:.2f}\",\n",
    "                transform=axes[0, 2].transAxes,\n",
    "                verticalalignment='top',\n",
    "                fontsize=16,\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "axes[0, 2].axline((0, 0), slope=1, linestyle='--', color='gray')\n",
    "axes[0, 2].set_xlabel('Mod. TSLA (-)')\n",
    "axes[0, 2].set_ylabel('Obs. TSLA (-)')\n",
    "axes[0, 2].set_xlim(0, 1)\n",
    "axes[0, 2].set_ylim(0, 1)\n",
    "axes[0, 2].grid()\n",
    "\n",
    "# 4. Cumulative MB plot (find MB observations) #taken it times 0.77 works wonders\n",
    "axes[1, 0].plot(cummb_median.time, cummb_median.values, label='Modelled', color='blue')\n",
    "axes[1, 0].fill_between(cummb_median.time,\n",
    "                 cummb_cilower.values,\n",
    "                 cummb_ciupper.values,\n",
    "                 color='blue',\n",
    "                 alpha=0.3,\n",
    "                 label='95% CI')\n",
    "axes[1, 0].errorbar(wgms['hydro_date'], wgms['CUM_BALANCE'], yerr=wgms['ANNUAL_BALANCE_UNC'], fmt='o', color='black', label='WGMS')\n",
    "\n",
    "# Add geod. MB (roughly)\n",
    "geod_mb_rate = geod_ref['dmdtda'].item()\n",
    "geod_uncertainty = geod_ref['err_dmdtda'].item()  # Â±m w.e./yr\n",
    "geod_start = pd.Timestamp(\"2000-01-01\")\n",
    "geod_end = pd.Timestamp(\"2010-01-01\")\n",
    "# Compute duration in years\n",
    "years = (geod_end - geod_start).days / 365.25\n",
    "geod_cum = geod_mb_rate * years\n",
    "geod_cum_uncert = geod_uncertainty * years\n",
    "\n",
    "# Plot as a line between start and end\n",
    "axes[1, 0].plot([geod_start, geod_end], [0, geod_cum], color='black', linestyle='--', label='Obs.')\n",
    "\n",
    "# Optional: Add uncertainty ribbon\n",
    "axes[1, 0].fill_between([geod_start, geod_end],\n",
    "                [0 - geod_cum_uncert, geod_cum - geod_cum_uncert],\n",
    "                [0 + geod_cum_uncert, geod_cum + geod_cum_uncert],\n",
    "                color='gray', alpha=0.3)\n",
    "\n",
    "\n",
    "axes[1, 0].set_xlabel('Time')\n",
    "axes[1, 0].set_ylabel('Cum. MB (m.w.e.)')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "axes[1, 0].set_xlim(pd.to_datetime(\"2000-01-01\"),pd.to_datetime(\"2010-01-01\"))\n",
    "axes[1, 0].xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1)))\n",
    "axes[1, 0].xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f\"{mdates.num2date(x).year}\"))\n",
    "axes[1, 0].xaxis.set_minor_locator(mdates.MonthLocator(bymonth=(7)))\n",
    "axes[1, 0].tick_params(axis='x', rotation=30)\n",
    "axes[1, 0].grid()\n",
    "\n",
    "\n",
    "# 5. Albedo time series\n",
    "axes[1, 1].errorbar(albobs.time.values, albobs.median_albedo.values,\n",
    "                    yerr=albobs.sigma_albedo.values,\n",
    "                    fmt='o', label='Observed', alpha=0.6)\n",
    "axes[1, 1].plot(albobs.time.values, median_alb, label='Modelled', color='orange')\n",
    "axes[1, 1].fill_between(albobs.time.values, alb_cilower, alb_ciupper, color='orange', alpha=0.4, label='Confidence Interval')\n",
    "axes[1, 1].set_xlabel('Time')\n",
    "axes[1, 1].set_ylabel('Albedo (-)')\n",
    "#axes[1, 1].legend()\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "\n",
    "axes[1, 1].set_xlim(pd.to_datetime(\"2000-01-01\"),pd.to_datetime(\"2010-01-01\"))\n",
    "axes[1, 1].xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1)))\n",
    "axes[1, 1].xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f\"{mdates.num2date(x).year}\"))\n",
    "axes[1, 1].xaxis.set_minor_locator(mdates.MonthLocator(bymonth=(7)))\n",
    "axes[1, 1].tick_params(axis='x', rotation=30)\n",
    "axes[1, 1].grid()\n",
    "\n",
    "# 6. Snowline time series\n",
    "axes[1, 2].errorbar(tsla_true_obs.index, tsla_true_obs['TSL_normalized'],\n",
    "                    yerr=tsla_true_obs['SC_stdev'],\n",
    "                    fmt='o', color=\"red\", label=\"Observed\", alpha=0.6)\n",
    "#axes[1, 2].errorbar(filtered_tsl.index, filtered_tsl['TSL_normalized'],\n",
    "#                    yerr=filtered_tsl['SC_stdev'],\n",
    "#                    fmt='o', label='Observed')\n",
    "axes[1, 2].plot(tsla_true_obs.index, median_tsla, label='Modelled', color='green')\n",
    "axes[1, 2].fill_between(tsla_true_obs.index, tsla_cilower, tsla_ciupper, color='green', alpha=0.4, label='Confidence Interval')\n",
    "axes[1, 2].set_xlabel('Time')\n",
    "axes[1, 2].set_ylabel('TSLA (-)')\n",
    "#axes[1, 2].legend()\n",
    "axes[1, 2].set_ylim(0, 1)\n",
    "\n",
    "axes[1, 2].set_xlim(pd.to_datetime(\"2000-01-01\"),pd.to_datetime(\"2010-01-01\"))\n",
    "axes[1, 2].xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1)))\n",
    "axes[1, 2].xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f\"{mdates.num2date(x).year}\"))\n",
    "axes[1, 2].xaxis.set_minor_locator(mdates.MonthLocator(bymonth=(7)))\n",
    "axes[1, 2].tick_params(axis='x', rotation=30)\n",
    "axes[1, 2].grid()\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "if 'win' in sys.platform:\n",
    "    plt.savefig(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/mcmc_ensemble_eval.png\", bbox_inches=\"tight\")\n",
    "else:\n",
    "    plt.savefig(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/mcmc_ensemble_eval.png\", bbox_inches=\"tight\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albobs['sigma_albedo'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "\n",
    "ppc_median = np.median(simulated_predictions_arr, axis=0)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12), dpi=300)\n",
    "\n",
    "# PPC for Mean Annual Mass Balance \n",
    "# We compare the distribution of simulated mean values to the observed mean.\n",
    "sns.kdeplot(mb_simulated_predictions_mean_annual_arr, fill=True, color=\"#D81B1B\", ax=axes[0,0], label=\"Posterior Predictive\")\n",
    "mu_obs = geod_ref['dmdtda']\n",
    "sigma_obs = geod_ref['err_dmdtda']\n",
    "x_obs = np.linspace(mu_obs - 4*sigma_obs, mu_obs + 4*sigma_obs, 200)\n",
    "pdf_obs = norm.pdf(x_obs, loc=mu_obs, scale=sigma_obs)\n",
    "axes[0, 0].plot(x_obs, pdf_obs, color=\"black\", linestyle=\"--\", label=\"Observed\")\n",
    "axes[0, 0].set_xlabel(r'$B_{geod}$'+ ' (m w.e. a$^{-1}$)')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "\n",
    "# Albedo scatter of PPCs\n",
    "x_err_alb = [ppc_median - ppc_lower, ppc_upper - ppc_median]\n",
    "axes[0, 1].errorbar(ppc_median, albobs['median_albedo'].values,\n",
    "                    xerr=x_err_alb, yerr=albobs['sigma_albedo'].values,\n",
    "                    fmt='o', alpha=0.6, capsize=3, color='#1E80E5', ecolor='gray')\n",
    "\n",
    "# Metrics calculated against the median of the predictive distribution.\n",
    "r2 = r2_score(albobs['median_albedo'].values, ppc_median)\n",
    "rmse = root_mean_squared_error(albobs['median_albedo'].values, ppc_median)\n",
    "mae = mean_absolute_error(albobs['median_albedo'].values, ppc_median)\n",
    "\n",
    "axes[0, 1].text(0.05, 0.95, f\"RÂ²={r2:.2f}\\nRMSE={rmse:.2f}\\nMAE={mae:.2f}\",\n",
    "                transform=axes[0, 1].transAxes, verticalalignment='top', fontsize=16,\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "axes[0, 1].axline((0, 0), slope=1, linestyle='--', color='black')\n",
    "axes[0, 1].set_xlabel(r'Modelled $\\bar{\\alpha}$ (-)')\n",
    "axes[0, 1].set_ylabel(r'Observed $\\bar{\\alpha}$ (-)')\n",
    "axes[0, 1].set_xlim(0, 1)\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "axes[0, 1].set_xticks(np.arange(0,1+0.2,0.2))\n",
    "axes[0, 1].set_yticks(np.arange(0,1+0.2,0.2))\n",
    "axes[0, 1].grid(True, zorder=-1)\n",
    "\n",
    "\n",
    "# Snowline scatter of PPCs\n",
    "x_err_tsl = [ppc_tsl_median - ppc_tsl_lower, ppc_tsl_upper - ppc_tsl_median]\n",
    "axes[0, 2].errorbar(ppc_tsl_median, tsla_true_obs['TSL_normalized'].values,\n",
    "                    xerr=x_err_tsl, yerr=tsla_true_obs['SC_stdev'].values,\n",
    "                    fmt='o', alpha=0.6, capsize=3, color='#A5781B', ecolor='gray')\n",
    "\n",
    "# Metrics calculated against the median of the predictive distribution.\n",
    "r2 = r2_score(tsla_true_obs['TSL_normalized'].values, ppc_tsl_median)\n",
    "rmse = root_mean_squared_error(tsla_true_obs['TSL_normalized'].values, ppc_tsl_median)\n",
    "mae = mean_absolute_error(tsla_true_obs['TSL_normalized'].values, ppc_tsl_median)\n",
    "\n",
    "axes[0, 2].text(0.05, 0.95, f\"RÂ²={r2:.2f}\\nRMSE={rmse:.2f}\\nMAE={mae:.2f}\",\n",
    "                transform=axes[0, 2].transAxes, verticalalignment='top', fontsize=16,\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "axes[0, 2].set_xlim(-0.2, 1)\n",
    "axes[0, 2].set_ylim(-0.2, 1)\n",
    "axes[0, 2].axline((-0.2, -0.2), slope=1, linestyle='--', color='black')\n",
    "axes[0, 2].set_xticks(np.arange(-0.2,1+0.2,0.2))\n",
    "axes[0, 2].set_yticks(np.arange(-0.2,1+0.2,0.2))\n",
    "axes[0, 2].set_xlabel('Modelled Norm. SLA (-)')\n",
    "axes[0, 2].set_ylabel('Observed Norm. SLA (-)')\n",
    "axes[0, 2].grid(True, zorder=-1)\n",
    "\n",
    "\n",
    "# Cum. MB test\n",
    "cummb_median = np.median(mb_model_runs_cumulative_arr, axis=0)\n",
    "cummb_lower, cummb_upper = np.percentile(mb_model_runs_cumulative_arr, [2.5, 97.5], axis=0)\n",
    "axes[1, 0].errorbar(wgms['hydro_date'], wgms['CUM_BALANCE'], yerr=wgms['ANNUAL_BALANCE_UNC'], fmt='o', color='darkgreen', label='WGMS')\n",
    "axes[1, 0].errorbar(wgms['hydro_date'], wgms['CUM_KLUG'], yerr=wgms['klug_unc'], fmt='o', color='steelblue', label='Klug et al., 2018')\n",
    "axes[1, 0].plot(time_rng, cummb_median, label='Posterior Ens. Median', color='#D81B1B')\n",
    "axes[1, 0].fill_between(time_rng, cummb_lower, cummb_upper, color='#D81B1B',\n",
    "                 alpha=0.3, label='95% CI')\n",
    "geod_mb_rate = geod_ref['dmdtda'].item()\n",
    "geod_uncertainty = geod_ref['err_dmdtda'].item()  # Â±m w.e./yr\n",
    "geod_start = pd.Timestamp(\"2000-01-01\")\n",
    "geod_end = pd.Timestamp(\"2010-01-01\")\n",
    "# Compute duration in years\n",
    "years = (geod_end - geod_start).days / 365.25\n",
    "geod_cum = geod_mb_rate * years\n",
    "geod_cum_uncert = geod_uncertainty * years\n",
    "\n",
    "# Plot as a line between start and end\n",
    "axes[1, 0].plot([geod_start, geod_end], [0, geod_cum], color='black', linestyle='--', label='Obs.')\n",
    "\n",
    "axes[1, 0].fill_between([geod_start, geod_end],\n",
    "                [0 - geod_cum_uncert, geod_cum - geod_cum_uncert],\n",
    "                [0 + geod_cum_uncert, geod_cum + geod_cum_uncert],\n",
    "                color='gray', alpha=0.3)\n",
    "axes[1, 0].set_xlabel('Time')\n",
    "axes[1, 0].set_ylabel('Cumulative MB (m w.e.)')\n",
    "axes[1, 0].legend(prop = { \"size\": 18 })\n",
    "axes[1, 0].grid(True, zorder=-1)\n",
    "\n",
    "\n",
    "# Albedo time series\n",
    "axes[1, 1].errorbar(pd.to_datetime(albobs.time), albobs['median_albedo'].values,\n",
    "                    yerr=albobs['sigma_albedo'].values,\n",
    "                    fmt='o', ms=5, label='Observed', alpha=0.6, color='black', ecolor='gray')\n",
    "# Plot the full predictive distribution\n",
    "axes[1, 1].plot(pd.to_datetime(albobs.time), ppc_median, label='Modelled Median', color='#1E80E5', marker='o', ms=4)\n",
    "axes[1, 1].fill_between(pd.to_datetime(albobs.time), ppc_lower, ppc_upper, color='#1E80E5',\n",
    "                 alpha=0.4, label='95% Prediction Interval')\n",
    "axes[1, 1].set_xlabel('Time')\n",
    "axes[1, 1].set_ylabel(r'$\\bar{\\alpha}$ (-)')\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "axes[1, 1].grid(True, zorder=-1)\n",
    "\n",
    "\n",
    "# Snowline time series\n",
    "axes[1, 2].errorbar(tsla_true_obs.index, tsla_true_obs['TSL_normalized'].values,\n",
    "                    yerr=tsla_true_obs['SC_stdev'].values,\n",
    "                    fmt='o', ms=5, color=\"black\", label=\"Observed\", alpha=0.6, ecolor='gray')\n",
    "# Plot the full predictive distribution\n",
    "axes[1, 2].plot(tsla_true_obs.index, ppc_tsl_median, label='Modelled Median', color='#A5781B', marker='o', ms=4)\n",
    "axes[1, 2].fill_between(tsla_true_obs.index, np.maximum(-0.2, ppc_tsl_lower), ppc_tsl_upper, color='#A5781B',\n",
    "                 alpha=0.4, label='95% Prediction Interval')\n",
    "axes[1, 2].set_xlabel('Time')\n",
    "axes[1, 2].set_ylim(-0.2, 1.0)\n",
    "axes[1, 2].set_yticks(np.arange(-0.2, 1+0.2, 0.2))\n",
    "axes[1, 2].set_ylabel('Norm. SLA (-)')\n",
    "axes[1, 2].grid(True, zorder=-1)\n",
    "\n",
    "for ax in [axes[1, 0], axes[1, 1], axes[1, 2]]:\n",
    "    ax.set_xlim(pd.to_datetime(\"2000-01-01\"), pd.to_datetime(\"2010-01-01\"))\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator(2))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax.tick_params(axis='x', rotation=30)\n",
    "    \n",
    "fig.text(0.01, 0.95, 'a)', transform=fig.transFigure, fontsize=24)\n",
    "fig.text(0.34, 0.95, 'b)', transform=fig.transFigure, fontsize=24)\n",
    "fig.text(0.66, 0.95, 'c)', transform=fig.transFigure, fontsize=24)\n",
    "#\n",
    "fig.text(0.01, 0.5, 'd)', transform=fig.transFigure, fontsize=24)\n",
    "fig.text(0.34, 0.5, 'e)', transform=fig.transFigure, fontsize=24)\n",
    "fig.text(0.66, 0.5, 'f)', transform=fig.transFigure, fontsize=24)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "if 'win' in sys.platform:\n",
    "    plt.savefig(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/mcmc_ensemble_eval.pdf\", bbox_inches=\"tight\")\n",
    "else:\n",
    "    plt.savefig(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/mcmc_ensemble_eval.pdf\", bbox_inches=\"tight\")\n",
    "\"\"\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interpolated lines for models - need to show full time series"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
