{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, TimeDistributed, Reshape, Bidirectional, Conv1D, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import xarray as xr \n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data from previous model runs \n",
    "# need to have pairs of parameters and modelled MB and snowlines\n",
    "# try with modelled MB for now!\n",
    "if 'win' in sys.platform:\n",
    "    path = \"E:/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/LHS/\"\n",
    "    tsla = pd.read_csv(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/snowlines/HEF-snowlines-1999-2010_manual_filtered.csv\")\n",
    "else:\n",
    "    path = \"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/LHS/\"\n",
    "    tsla = pd.read_csv(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/snowlines/HEF-snowlines-1999-2010_manual_filtered.csv\")\n",
    "# params = pd.read_csv(path+\"cosipy_synthetic_params_lhs.csv\", index_col=0)\n",
    "params = pd.read_csv(path+\"LHS-narrow_1D20m_1999_2010_fullprior.csv\", index_col=0)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start_dt = pd.to_datetime(\"2000-01-01\") #config starts with spinup - need to add 1year\n",
    "time_end_dt = pd.to_datetime(\"2009-12-31\")\n",
    "\n",
    "tsla_true_obs = tsla.copy()\n",
    "tsla_true_obs['LS_DATE'] = pd.to_datetime(tsla_true_obs['LS_DATE'])\n",
    "print(\"Start date:\", time_start_dt)\n",
    "print(\"End date:\", time_end_dt)\n",
    "tsla_true_obs = tsla_true_obs.loc[(tsla_true_obs['LS_DATE'] > time_start_dt) & (tsla_true_obs['LS_DATE'] <= time_end_dt)]\n",
    "tsla_true_obs.set_index('LS_DATE', inplace=True)\n",
    "#Normalize standard deviation if necessary\n",
    "tsla_true_obs['SC_stdev'] = (tsla_true_obs['SC_stdev']) / (tsla_true_obs['glacier_DEM_max'] - tsla_true_obs['glacier_DEM_min'])\n",
    "\n",
    "thres_unc = (20) / (tsla_true_obs['glacier_DEM_max'].iloc[0] - tsla_true_obs['glacier_DEM_min'].iloc[0])\n",
    "print(thres_unc)\n",
    "\n",
    "## Set observational uncertainty where smaller to atleast model resolution (20m) and where larger keep it\n",
    "sc_norm = np.where(tsla_true_obs['SC_stdev'] < thres_unc, thres_unc, tsla_true_obs['SC_stdev'])\n",
    "tsla_true_obs['SC_stdev'] = sc_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_true_obs[['TSL_normalized']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load albedo reference\n",
    "if 'win' in sys.platform:\n",
    "    albobs = xr.open_dataset(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/HEF_processed_HRZ-30CC-filter_albedos.nc\")\n",
    "else:\n",
    "    albobs = xr.open_dataset(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/HEF_processed_HRZ-30CC-filter_albedos.nc\")\n",
    "albobs = albobs.sortby(\"time\")\n",
    "albobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get rid of redudant variables\n",
    "params = params.drop(['center_snow_transfer', 'spread_snow_transfer','roughness_fresh_snow', 'roughness_firn','aging_factor_roughness'], axis=1)\n",
    "params_full = params.copy()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## randomly select n samples\n",
    "#size = 500\n",
    "#params = params_full.sample(n=500, random_state=77)\n",
    "#params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[[\"rrr_factor\", \"alb_ice\", \"alb_snow\", \"alb_firn\", \"albedo_aging\", \"albedo_depth\", \"roughness_ice\"]].plot.hist(subplots=True)\n",
    "#Show that samples really are covering full space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get time variable for snowline points - time dependency added to training\n",
    "doy = tsla_true_obs.index.dayofyear\n",
    "# Define time features\n",
    "time_sin = np.sin(2 * np.pi * doy / 365)\n",
    "time_cos = np.cos(2 * np.pi * doy / 365)\n",
    "time_features = np.stack([time_sin, time_cos], axis=-1)  # Shape: (62, 2)\n",
    "time_features\n",
    "#params['DOY'] = doy\n",
    "\n",
    "### Repeat for albedo\n",
    "#get time variable for snowline points - time dependency added to training\n",
    "doy_alb = albobs.time.dt.dayofyear.data\n",
    "# Define time features\n",
    "time_sin_alb = np.sin(2 * np.pi * doy_alb / 365)\n",
    "time_cos_alb = np.cos(2 * np.pi * doy_alb / 365)\n",
    "time_features_alb = np.stack([time_sin_alb, time_cos_alb], axis=-1)  # Shape: (62, 2)\n",
    "time_features_alb\n",
    "#params['DOY'] = doy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load albedo observations\n",
    "list_sim_alb = []\n",
    "\n",
    "for i,r in params.iterrows():\n",
    "    if i % 250 == 0:\n",
    "        print(f\"Processing file {i}/2500\")\n",
    "    rrr_factor = round(r['rrr_factor'],4)\n",
    "    alb_ice = round(r['alb_ice'],4)\n",
    "    alb_snow = round(r['alb_snow'],4)\n",
    "    alb_firn = round(r['alb_firn'],4)\n",
    "    alb_aging = round(r['albedo_aging'],4)\n",
    "    alb_depth = round(r['albedo_depth'],4)\n",
    "    roughness_ice = round(r['roughness_ice'], 4)\n",
    "\n",
    "    filename = f\"HEF_COSMO_1D20m_1999_2010_HORAYZON_IntpPRES_LHS-narrow_19990101-20091231_RRR-{rrr_factor}_{alb_snow}_{alb_ice}_{alb_firn}_{alb_aging}_{alb_depth}_0.24_{roughness_ice}_4.0_0.0026_num2.nc\"\n",
    "    if 'win' in sys.platform:\n",
    "        sim_alb = xr.open_dataarray(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Output/albedo_files/LHS/alb_only/\"+\\\n",
    "            filename)\n",
    "    else:\n",
    "        sim_alb = xr.open_dataarray(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Output/albedo_files/LHS/alb_only/\"+\\\n",
    "            filename)\n",
    "\n",
    "    \n",
    "    #sort by time \n",
    "    sim_alb = sim_alb.sortby(\"time\")\n",
    "    list_sim_alb.append(sim_alb.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_albedo = pd.DataFrame(list_sim_alb, columns=[f'alb{i+1}' for i in range(len(sim_alb.data))])\n",
    "df_albedo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select albedo data\n",
    "# drop index from dataframe - since we loop over it, dfs are aligned\n",
    "params.reset_index(drop=True, inplace=True)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_merged = pd.concat([params, df_albedo], axis=1)\n",
    "params_merged "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## randomly select n subsamples of params (because 3000 might lead to autocorrelation/overfitting)\n",
    "#random indices\n",
    "#n_random_samples = 3000\n",
    "#create random numbers from 0 to 2999\n",
    "#_idx = np.random.randint(low=0, high=2999, size=n_random_samples)\n",
    "#params_subset = params.iloc[_idx]\n",
    "params_subset = params_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create train test split\n",
    "train_dataset, validation_dataset = train_test_split(params_subset.index, \n",
    "                                               train_size=0.8, #0.7 and 0.3?\n",
    "                                               test_size=0.2, random_state=11)\n",
    "\n",
    "df_train = params_subset.loc[train_dataset]\n",
    "df_validation = params_subset.loc[validation_dataset]\n",
    "df_train.shape, df_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sims = [x for x in params_subset.columns if 'sim' in x]\n",
    "list_albs = [x for x in params_subset.columns if 'alb' in x if x not in ['alb_ice','alb_snow','alb_firn','albedo_aging','albedo_depth']] ##make sure to exclude important albeod fields\n",
    "#list_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit scalers using training data\n",
    "features_to_drop = ['mb'] + list_sims + list_albs\n",
    "df_train_X = df_train.drop(features_to_drop, axis=1)\n",
    "df_train_y_mb = df_train[['mb']].values\n",
    "df_train_y_tsla = df_train[list_sims].values\n",
    "df_train_y_alb = df_train[list_albs].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train_X.values)\n",
    "\n",
    "# Transform inputs and outputs\n",
    "X_train = scaler.transform(df_train_X.values)  # Shape: (n_samples, 6)\n",
    "\n",
    "# Repeat the input features for time points\n",
    "X_train_expanded = np.repeat(X_train[:, None, :], len(doy), axis=1)  # Snowlines\n",
    "X_train_expanded_alb = np.repeat(X_train[:, None, :], len(doy_alb), axis=1) # Albedo\n",
    "\n",
    "# Add time features without scaling\n",
    "X_train_with_time = np.concatenate([X_train_expanded, np.tile(time_features, (X_train.shape[0], 1, 1))], axis=-1)\n",
    "X_train_with_time_alb = np.concatenate([X_train_expanded_alb, np.tile(time_features_alb, (X_train.shape[0], 1, 1))], axis=-1)\n",
    "# Final shape: (n_samples, 62, 8)\n",
    "\n",
    "\n",
    "# Repeat the same for validation data\n",
    "df_validation_X = df_validation.drop(features_to_drop, axis=1)\n",
    "df_validation_y_mb = df_validation[['mb']].values\n",
    "df_validation_y_tsla = df_validation[list_sims].values\n",
    "df_validation_y_alb = df_validation[list_albs].values\n",
    "\n",
    "X_validation = scaler.transform(df_validation_X.values)  # Shape: (n_samples, 6)\n",
    "\n",
    "X_validation_expanded = np.repeat(X_validation[:, None, :], len(doy), axis=1)  # Shape: (n_samples, 62, 6)\n",
    "X_validation_with_time = np.concatenate(\n",
    "    [X_validation_expanded, np.tile(time_features, (X_validation.shape[0], 1, 1))], axis=-1\n",
    ")\n",
    "# Final shape: (n_samples, 62, 8)\n",
    "X_validation_expanded_alb = np.repeat(X_validation[:, None, :], len(doy_alb), axis=1)  # Shape: (n_samples, 62, 6)\n",
    "X_validation_with_time_alb = np.concatenate(\n",
    "    [X_validation_expanded_alb, np.tile(time_features_alb, (X_validation.shape[0], 1, 1))], axis=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(df_train_y_mb.shape)\n",
    "print(df_train_y_tsla.shape)\n",
    "print(df_train_y_alb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Layers\n",
    "mass_balance_input = Input(shape=(7,), name=\"mass_balance_input\")\n",
    "snowlines_input = Input(shape=(58, 9), name=\"snowlines_input\")\n",
    "alb_input = Input(shape=(98, 9), name=\"alb_input\")\n",
    "\n",
    "# Shared Layers for Mass Balance\n",
    "shared_mb = Dense(64, activation='relu')(mass_balance_input)\n",
    "shared_mb = Dense(128, activation='relu')(shared_mb)\n",
    "shared_mb = Dropout(0.1)(shared_mb)\n",
    "\n",
    "# Mass Balance Branch\n",
    "mb_branch = Dense(64, activation='relu')(shared_mb)\n",
    "mb_output = Dense(1, name=\"mass_balance_output\")(mb_branch)\n",
    "\n",
    "# Shared Layers for Snowlines\n",
    "# LSTM Layers (Stacked)\n",
    "lstm_out = Bidirectional(LSTM(64, return_sequences=True))(snowlines_input)\n",
    "lstm_out = Bidirectional(LSTM(64, return_sequences=True))(lstm_out)\n",
    "\n",
    "# Shared Layers for Albedo\n",
    "# LSTM Layers (Stacked)\n",
    "lstm_alb = Bidirectional(LSTM(64, return_sequences=True))(alb_input)\n",
    "lstm_alb = Bidirectional(LSTM(64, return_sequences=True))(lstm_alb)\n",
    "\n",
    "# Multi-Scale CNN (Inception Style)\n",
    "#conv1 = Conv1D(filters=64, kernel_size=2, padding=\"same\", activation=\"relu\")(lstm_alb) #3 months, 6 months, 12 months\n",
    "#conv2 = Conv1D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\")(lstm_out)\n",
    "\n",
    "# Combine Multi-Scale Features\n",
    "#x = Concatenate()([conv1, conv2])\n",
    "#x = BatchNormalization()(x)\n",
    "\n",
    "#shared_sl = LSTM(64, return_sequences=True)(snowlines_input)\n",
    "shared_sl = Dense(128, activation='relu')(lstm_out)\n",
    "\n",
    "# Snowlines Branch\n",
    "sl_branch = Dense(64, activation='relu')(shared_sl)\n",
    "#sl_branch = Dropout(0.1)(sl_branch)  # Add Dropout here for snowlines branch\n",
    "snowlines_output = Dense(1, activation='sigmoid')(sl_branch)  # Predict one value per time point\n",
    "snowlines_output = Reshape((58,), name=\"snowlines_output\")(snowlines_output)  # Adjust shape to (batch_size, 62)\n",
    "\n",
    "### Albedo\n",
    "#shared_alb = Dense(128, activation='relu')(conv1)\n",
    "shared_alb = Dense(128, activation='relu')(lstm_alb)\n",
    "\n",
    "# Snowlines Branch\n",
    "alb_branch = Dense(64, activation='relu')(shared_alb)\n",
    "#sl_branch = Dropout(0.1)(sl_branch)  # Add Dropout here for snowlines branch\n",
    "alb_output = Dense(1, activation='sigmoid')(alb_branch)  # Predict one value per time point\n",
    "alb_output = Reshape((98,), name=\"alb_output\")(alb_output)  # Adjust shape to (batch_size, 62)\n",
    "\n",
    "\"\"\"\n",
    "@keras.saving.register_keras_serializable(package=\"CustomLosses\")\n",
    "def weighted_huber_loss(y_true, y_pred, delta=0.1):\n",
    "    residual = tf.abs(y_true - y_pred)\n",
    "    weights = tf.where(y_true > 0, 50.0, 1.0)\n",
    "    weights = tf.cast(weights, dtype=tf.float32)\n",
    "    weights = tf.expand_dims(weights, axis=-1)\n",
    "    huber_loss = tf.keras.losses.huber(y_true, y_pred, delta=delta)\n",
    "    return tf.reduce_mean(weights * huber_loss)\n",
    "\"\"\"\n",
    "\n",
    "# Combine into a Model\n",
    "model = Model(\n",
    "    inputs=[mass_balance_input, snowlines_input, alb_input],\n",
    "    outputs=[mb_output, snowlines_output, alb_output]\n",
    ")\n",
    "\n",
    "loss_type = \"huber\"\n",
    "# Compile the model with a combined loss function\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss={\n",
    "        'mass_balance_output': \"mse\",\n",
    "        'snowlines_output': loss_type,\n",
    "        'alb_output': loss_type\n",
    "    },\n",
    "    loss_weights={\n",
    "        'mass_balance_output': 1.0,  # You can tune this\n",
    "        'snowlines_output': 1.0,      # You can tune this\n",
    "        'alb_output': 1.0\n",
    "    },\n",
    "    metrics=[keras.metrics.RootMeanSquaredError(),keras.metrics.RootMeanSquaredError(),keras.metrics.RootMeanSquaredError()]\n",
    ")\n",
    "\n",
    "# Early Stopping and Learning Rate Scheduler\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True, verbose=1)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    x={\n",
    "        'mass_balance_input': X_train, \n",
    "        'snowlines_input': X_train_with_time,\n",
    "        'alb_input': X_train_with_time_alb\n",
    "    },\n",
    "    y={\n",
    "        'mass_balance_output': df_train_y_mb, \n",
    "        'snowlines_output': df_train_y_tsla,\n",
    "        'alb_output': df_train_y_alb\n",
    "    },\n",
    "    validation_data=(\n",
    "        {\n",
    "            'mass_balance_input': X_validation,\n",
    "            'snowlines_input': X_validation_with_time,\n",
    "            'alb_input': X_validation_with_time_alb\n",
    "        },\n",
    "        {\n",
    "            'mass_balance_output': df_validation_y_mb,\n",
    "            'snowlines_output': df_validation_y_tsla,\n",
    "            'alb_output': df_validation_y_alb\n",
    "        }\n",
    "    ),\n",
    "    epochs=300,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mb_loss, sl_loss, alb_loss, mb_mae, sl_mae, alb_mae = model.evaluate(\n",
    "    x={\n",
    "        'mass_balance_input': X_validation, \n",
    "        'snowlines_input': X_validation_with_time,\n",
    "        'alb_input': X_validation_with_time_alb\n",
    "    },\n",
    "    y={\n",
    "        'mass_balance_output': df_validation_y_mb, \n",
    "        'snowlines_output': df_validation_y_tsla,\n",
    "        'alb_output': df_validation_y_alb\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Total Loss: {loss}\")\n",
    "print(f\"Mass Balance Loss: {mb_loss}, MAE: {mb_mae}\")\n",
    "print(f\"Snowlines Loss: {sl_loss}, MAE: {sl_mae}\")\n",
    "print(f\"Albedo Loss: {alb_loss}, MAE: {alb_mae}\")\n",
    "\n",
    "#batch size default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "def plot_loss(history, loss_var):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss for mass balance and snowlines.\n",
    "    \n",
    "    Parameters:\n",
    "    history: History object returned by the Keras model.fit() method.\n",
    "    \"\"\"\n",
    "    # Plot loss for mass balance output\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Plot training and validation loss for mass balance\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history['mass_balance_output_loss'], label='Train Mass Balance Loss')\n",
    "    plt.plot(history.history['val_mass_balance_output_loss'], label='Val Mass Balance Loss')\n",
    "    plt.title('Mass Balance Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(loss_var)\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training and validation loss for snowlines\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history['snowlines_output_loss'], label='Train Snowlines Loss')\n",
    "    plt.plot(history.history['val_snowlines_output_loss'], label='Val Snowlines Loss')\n",
    "    plt.title('Snowlines Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(loss_var)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot training and validation loss for albedo\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history.history['alb_output_loss'], label='Train Albedo Loss')\n",
    "    plt.plot(history.history['val_alb_output_loss'], label='Val Albedo Loss')\n",
    "    plt.title('Albedo Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(loss_var)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_scatter(y_train, y_pred, savefig=False):\n",
    "    idxs = np.arange(len(y_train))\n",
    "    np.random.shuffle(idxs)\n",
    "\n",
    "    y_expected = y_train.reshape(-1)[idxs[:]]\n",
    "    y_predicted = y_pred.reshape(-1)[idxs[:]]\n",
    "\n",
    "    xy = np.vstack([y_expected, y_predicted])\n",
    "    z = gaussian_kde(xy)(xy)\n",
    "    print(xy.shape)\n",
    "    # Sort the points by density, so that the densest points are plotted last\n",
    "    idx = z.argsort()\n",
    "    y_plt, ann_plt, z = y_expected[idx], y_predicted[idx], z[idx]\n",
    "\n",
    "    plt.figure(figsize=(12,12), dpi=300)\n",
    "    #plt.title(\"Model Evaluation\", fontsize=17)\n",
    "    plt.ylabel('Emulated Mass Balance (m w.e a$^{-1}$)', fontsize=22)\n",
    "    plt.xlabel('COSIPY Mass Balance (m w.e a$^{-1}$)', fontsize=22)\n",
    "    sc = plt.scatter(y_plt, ann_plt, s=20)\n",
    "    #plt.clim(0,0.4)\n",
    "    plt.tick_params(labelsize=22)\n",
    "    #plt.colorbar(sc) \n",
    "    lineStart = -4.5\n",
    "    lineEnd = 1.5\n",
    "    plt.plot([lineStart, lineEnd], [lineStart, lineEnd], 'k-')\n",
    "    plt.axvline(0.0, ls='-.', c='k')\n",
    "    plt.axhline(0.0, ls='-.', c='k')\n",
    "    plt.xlim(lineStart, lineEnd)\n",
    "    plt.ylim(lineStart, lineEnd)\n",
    "    plt.gca().set_box_aspect(1)\n",
    "    plt.grid()\n",
    "\n",
    "    rmse_score = root_mean_squared_error(y_expected, y_predicted)\n",
    "    textstr = '\\n'.join((\n",
    "        r'$R^2=%.2f$' % (r2_score(y_expected, y_predicted), ),\n",
    "        r'$MAE=%.2f$' % (mean_absolute_error(y_expected, y_predicted), ),\n",
    "        r'$RMSE=%.2f$'% (rmse_score), ))\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    # place a text box in upper left in axes coords\n",
    "    plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=22,\n",
    "            verticalalignment='top', bbox=props)\n",
    "    if savefig:\n",
    "        plt.savefig(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/FigS10_mb_emulator.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history, loss_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test sample for other script\n",
    "print(X_validation[0])\n",
    "print(X_validation_with_time[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict({\n",
    "    'mass_balance_input': X_validation,\n",
    "    'snowlines_input': X_validation_with_time,\n",
    "    'alb_input': X_validation_with_time_alb\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_mass_balance = predictions[0]  # Shape: (N, 1)\n",
    "predicted_snowlines = predictions[1]     # Shape: (N, 62)\n",
    "predicted_albedos = predictions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_mass_balance[0,:].flatten())\n",
    "print(predicted_snowlines[0,:].flatten())\n",
    "print(predicted_albedos[0,:].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation_y_mb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(df_validation_y_mb, predicted_mass_balance, savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_glacier_elev = 3700.0\n",
    "min_glacier_elev = 2440.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute point density\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "xy = np.vstack([df_validation_y_tsla.flatten(), predicted_snowlines.flatten()])\n",
    "density = gaussian_kde(xy)(xy)\n",
    "\n",
    "# Sort the points by density to ensure denser points appear on top\n",
    "idx = density.argsort()\n",
    "x_sorted, y_sorted, density_sorted = df_validation_y_tsla.flatten()[idx], predicted_snowlines.flatten()[idx], density[idx]\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,12), dpi=300)\n",
    "#ax.set_title(\"Model Evaluation\", fontsize=17)\n",
    "ax.set_ylabel('Emulated normalised SLA', fontsize=22)\n",
    "ax.set_xlabel('COSIPY normalised SLA', fontsize=22)\n",
    "lineStart = 0.0\n",
    "lineEnd = 1.0\n",
    "ax.plot([lineStart, lineEnd], [lineStart, lineEnd], 'k-')\n",
    "ax.set_xlim(lineStart, lineEnd)\n",
    "ax.set_ylim(lineStart, lineEnd)\n",
    "plt.gca().set_box_aspect(1)\n",
    "\n",
    "# Compute error metrics\n",
    "mae_score = mean_absolute_error(df_validation_y_tsla.flatten(), predicted_snowlines.flatten())\n",
    "r2_scores = r2_score(df_validation_y_tsla.flatten(), predicted_snowlines.flatten())\n",
    "mae_in_meter = mae_score * (max_glacier_elev - min_glacier_elev)\n",
    "rmse_score = root_mean_squared_error(df_validation_y_tsla.flatten(), predicted_snowlines.flatten())\n",
    "rmse_in_meter = rmse_score * (max_glacier_elev - min_glacier_elev)\n",
    "\n",
    "textstr = '\\n'.join((\n",
    "    r'$R^2=%.4f$' % (r2_scores, ),\n",
    "    r'$MAE=%.4f$' % (mae_score, ),\n",
    "    r'$MAE\\,(m)=%.4f$' % (mae_in_meter, ),\n",
    "    r'$RMSE=%.4f$' % (rmse_score, ),\n",
    "    r'$RMSE\\,(m)=%.4f$' % (rmse_in_meter, )))\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=22,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "# Secondary axes\n",
    "def custom_ticks(y):\n",
    "    orig_max = max_glacier_elev\n",
    "    orig_min = min_glacier_elev\n",
    "    return y * (orig_max - orig_min) + orig_min\n",
    "\n",
    "ax2y = ax.secondary_yaxis(-0.14, functions=(custom_ticks, custom_ticks))\n",
    "ax2x = ax.secondary_xaxis(-0.1, functions=(custom_ticks, custom_ticks))\n",
    "list_labels = [custom_ticks(x) for x in np.arange(0,1+0.1,0.1)]\n",
    "ax2x.set_xticks(list_labels)\n",
    "ax2y.set_yticks(list_labels)\n",
    "ax2x.set_xticklabels([round(x) for x in ax2x.get_xticks()], rotation=30)\n",
    "\n",
    "# Scatter plot with density coloring\n",
    "sc = ax.scatter(x_sorted, y_sorted, c=density_sorted, s=20, cmap='plasma', alpha=0.7)\n",
    "#cb = plt.colorbar(sc, ax=ax, label=\"Density\")\n",
    "\n",
    "ax.set_xticks(np.arange(0,1+0.1,0.1))\n",
    "ax.set_yticks(np.arange(0,1+0.1,0.1))\n",
    "ax.grid(True)\n",
    "plt.savefig(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/tsla_emulator.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12,12), dpi=300)\n",
    "#ax.set_title(\"Model Evaluation\", fontsize=17)\n",
    "ax.set_ylabel('Modeled Norm. TSLA', fontsize=22)\n",
    "ax.set_xlabel('Reference Norm. TSLA', fontsize=22)\n",
    "lineStart = 0.0\n",
    "lineEnd = 1.0\n",
    "ax.plot([lineStart, lineEnd], [lineStart, lineEnd], 'k-')\n",
    "#plt.axvline(0.0, ls='-.', c='k')\n",
    "#plt.axhline(0.0, ls='-.', c='k')\n",
    "ax.set_xlim(lineStart, lineEnd)\n",
    "ax.set_ylim(lineStart, lineEnd)\n",
    "plt.gca().set_box_aspect(1)\n",
    "ax.grid()\n",
    "\n",
    "mae_list = []\n",
    "r2_scores = []\n",
    "for j in range(predicted_snowlines.shape[0]):\n",
    "    idxs = np.arange(len(predicted_snowlines[j,:]))\n",
    "    #j = 0\n",
    "    \n",
    "    np.random.shuffle(idxs)\n",
    "    y_expected = df_validation_y_tsla[j,:].reshape(-1)[idxs[:]]\n",
    "    y_predicted = predicted_snowlines[j,:].reshape(-1)[idxs[:]]\n",
    "    #xy = np.vstack([y_expected, y_predicted])\n",
    "    #z = gaussian_kde(xy)(xy)\n",
    "    # Sort the points by density, so that the densest points are plotted last\n",
    "    #idx = z.argsort()\n",
    "    #y_plt, ann_plt, z = y_expected[idx], y_predicted[idx], z[idx]\n",
    "    y_plt, ann_plt = y_expected, y_predicted\n",
    "\n",
    "    sc = ax.scatter(y_plt, ann_plt, s=20, alpha=0.5, label=j)\n",
    "    \n",
    "    #plt.clim(0,0.4)\n",
    "    plt.tick_params(labelsize=14)\n",
    "    #plt.colorbar(sc) \n",
    "\n",
    "    mae_list.append(mean_absolute_error(y_expected, y_predicted))\n",
    "    r2_scores.append(r2_score(y_expected, y_predicted))\n",
    "    '''\n",
    "    '''\n",
    "textstr = '\\n'.join((\n",
    "    r'Avg. $MAE=%.2f$' % (np.nanmean(mae_list), ),\n",
    "    r'Avg. $R^2=%.2f$' % (np.nanmean(r2_scores), )))\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "# place a text box in upper left in axes coords\n",
    "plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=14,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "# Add a secondary x-axis below the primary x-axis\n",
    "def custom_ticks(y):\n",
    "    #convert back to original values\n",
    "    orig_max = 3697.2958984375\n",
    "    orig_min = 2417.2958984375\n",
    "    return y * (orig_max - orig_min) + orig_min  # Define your custom transformation for tick labels\n",
    "\n",
    "ax2y = ax.secondary_yaxis(-0.14, functions=(custom_ticks, custom_ticks))\n",
    "ax2x = ax.secondary_xaxis(-0.1, functions=(custom_ticks, custom_ticks))\n",
    "list_labels = [custom_ticks(x) for x in [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]]\n",
    "ax2x.set_xticks(list_labels)\n",
    "ax2y.set_yticks(list_labels)\n",
    "ax2x.set_xticklabels([round(x) for x in ax2x.get_xticks()], rotation=30)\n",
    "#ax2x.set_xlabel(var)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute point density\n",
    "xy = np.vstack([df_validation_y_alb.flatten(), predicted_albedos.flatten()])\n",
    "density = gaussian_kde(xy)(xy)\n",
    "\n",
    "# Sort the points by density to ensure denser points appear on top\n",
    "idx = density.argsort()\n",
    "x_sorted, y_sorted, density_sorted = df_validation_y_alb.flatten()[idx], predicted_albedos.flatten()[idx], density[idx]\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,12), dpi=300)\n",
    "#ax.set_title(\"Model Evaluation\", fontsize=17)\n",
    "ax.set_ylabel(r'Emulated $\\bar{\\alpha}$', fontsize=22)\n",
    "ax.set_xlabel(r'COSIPY $\\bar{\\alpha}$', fontsize=22)\n",
    "lineStart = 0.0\n",
    "lineEnd = 1.0\n",
    "ax.plot([lineStart, lineEnd], [lineStart, lineEnd], 'k-')\n",
    "ax.set_xlim(lineStart, lineEnd)\n",
    "ax.set_ylim(lineStart, lineEnd)\n",
    "plt.gca().set_box_aspect(1)\n",
    "\n",
    "# Compute error metrics\n",
    "mae_score = mean_absolute_error(df_validation_y_alb.flatten(), predicted_albedos.flatten())\n",
    "r2_scores = r2_score(df_validation_y_alb.flatten(), predicted_albedos.flatten())\n",
    "rmse_score = root_mean_squared_error(df_validation_y_alb.flatten(), predicted_albedos.flatten())\n",
    "\n",
    "textstr = '\\n'.join((\n",
    "    r'$R^2=%.4f$' % (r2_scores, ),\n",
    "    r'$MAE=%.4f$' % (mae_score, ),\n",
    "    r'$RMSE=%.4f$' % (rmse_score, )))\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=22,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "# Scatter plot with density coloring\n",
    "sc = ax.scatter(x_sorted, y_sorted, c=density_sorted, s=20, cmap='plasma', alpha=0.7)\n",
    "#cb = plt.colorbar(sc, ax=ax, label=\"Density\")\n",
    "\n",
    "ax.set_xticks(np.arange(0,1+0.1,0.1))\n",
    "ax.set_yticks(np.arange(0,1+0.1,0.1))\n",
    "ax.grid(True)\n",
    "plt.savefig(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/alb_emulator.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to file\n",
    "if 'win' in sys.platform:\n",
    "    model.save(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/first_test.keras\")\n",
    "else:\n",
    "    model.save(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/first_test.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
