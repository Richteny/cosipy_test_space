{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, TimeDistributed, Reshape, Bidirectional, Conv1D, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "from scipy.spatial.distance import cdist\n",
    "import pickle\n",
    "import gc\n",
    "import xarray as xr \n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data from previous model runs \n",
    "# need to have pairs of parameters and modelled MB and snowlines\n",
    "# try with modelled MB for now!\n",
    "if 'win' in sys.platform:\n",
    "    path = \"E:/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/LHS/\"\n",
    "else:\n",
    "    path = \"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/LHS/\"\n",
    "    tsla = pd.read_csv(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/snowlines/HEF-snowlines-1999-2010_manual_filtered.csv\")\n",
    "# params = pd.read_csv(path+\"cosipy_synthetic_params_lhs.csv\", index_col=0)\n",
    "params = pd.read_csv(path+\"LHS-narrow_1D20m_1999_2010_fullprior.csv\", index_col=0)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start_dt = pd.to_datetime(\"2000-01-01\") #config starts with spinup - need to add 1year\n",
    "time_end_dt = pd.to_datetime(\"2009-12-31\")\n",
    "\n",
    "tsla_true_obs = tsla.copy()\n",
    "tsla_true_obs['LS_DATE'] = pd.to_datetime(tsla_true_obs['LS_DATE'])\n",
    "print(\"Start date:\", time_start_dt)\n",
    "print(\"End date:\", time_end_dt)\n",
    "tsla_true_obs = tsla_true_obs.loc[(tsla_true_obs['LS_DATE'] > time_start_dt) & (tsla_true_obs['LS_DATE'] <= time_end_dt)]\n",
    "tsla_true_obs.set_index('LS_DATE', inplace=True)\n",
    "#Normalize standard deviation if necessary\n",
    "tsla_true_obs['SC_stdev'] = (tsla_true_obs['SC_stdev']) / (tsla_true_obs['glacier_DEM_max'] - tsla_true_obs['glacier_DEM_min'])\n",
    "\n",
    "thres_unc = (20) / (tsla_true_obs['glacier_DEM_max'].iloc[0] - tsla_true_obs['glacier_DEM_min'].iloc[0])\n",
    "print(thres_unc)\n",
    "\n",
    "## Set observational uncertainty where smaller to atleast model resolution (20m) and where larger keep it\n",
    "sc_norm = np.where(tsla_true_obs['SC_stdev'] < thres_unc, thres_unc, tsla_true_obs['SC_stdev'])\n",
    "tsla_true_obs['SC_stdev'] = sc_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_true_obs[['TSL_normalized']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load albedo reference\n",
    "albobs = xr.open_dataset(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/HEF_processed_HRZ-20CC-filter_albedos.nc\") #old for prev. version - adjusted version coming\n",
    "albobs = albobs.sortby(\"time\")\n",
    "albobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get rid of redudant variables\n",
    "params = params.drop(['center_snow_transfer', 'spread_snow_transfer','roughness_fresh_snow', 'roughness_firn','aging_factor_roughness'], axis=1)\n",
    "params_full = params.copy()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## randomly select n samples\n",
    "#size = 500\n",
    "#params = params_full.sample(n=500, random_state=77)\n",
    "#params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[[\"rrr_factor\", \"alb_ice\", \"alb_snow\", \"alb_firn\", \"albedo_aging\", \"albedo_depth\", \"roughness_ice\"]].plot.hist(subplots=True)\n",
    "#Show that samples really are covering full space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "time_normalized = time / 365.0  # Assuming time is in days of the year\n",
    "time_sin = np.sin(2 * np.pi * time_normalized)\n",
    "time_cos = np.cos(2 * np.pi * time_normalized)\n",
    "X_train_scaled = np.concatenate([X_train_scaled, time_sin, time_cos], axis=1)\n",
    "\"\"\"\n",
    "#get time variable for snowline points - time dependency added to training\n",
    "doy = tsla_true_obs.index.dayofyear\n",
    "# Define time features\n",
    "time_sin = np.sin(2 * np.pi * doy / 365)\n",
    "time_cos = np.cos(2 * np.pi * doy / 365)\n",
    "time_features = np.stack([time_sin, time_cos], axis=-1)  # Shape: (62, 2)\n",
    "time_features\n",
    "#params['DOY'] = doy\n",
    "\n",
    "### Repeat for albedo\n",
    "#get time variable for snowline points - time dependency added to training\n",
    "doy_alb = albobs.time.dt.dayofyear.data\n",
    "# Define time features\n",
    "time_sin_alb = np.sin(2 * np.pi * doy_alb / 365)\n",
    "time_cos_alb = np.cos(2 * np.pi * doy_alb / 365)\n",
    "time_features_alb = np.stack([time_sin_alb, time_cos_alb], axis=-1)  # Shape: (62, 2)\n",
    "time_features_alb\n",
    "#params['DOY'] = doy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load albedo observations\n",
    "list_sim_alb = []\n",
    "\n",
    "for i,r in params.iterrows():\n",
    "    if i % 250 == 0:\n",
    "        print(f\"Processing file {i}/2500\")\n",
    "    rrr_factor = round(r['rrr_factor'],4)\n",
    "    alb_ice = round(r['alb_ice'],4)\n",
    "    alb_snow = round(r['alb_snow'],4)\n",
    "    alb_firn = round(r['alb_firn'],4)\n",
    "    alb_aging = round(r['albedo_aging'],4)\n",
    "    alb_depth = round(r['albedo_depth'],4)\n",
    "    roughness_ice = round(r['roughness_ice'], 4)\n",
    "\n",
    "    filename = f\"HEF_COSMO_1D20m_1999_2010_HORAYZON_IntpPRES_LHS-narrow_19990101-20091231_RRR-{rrr_factor}_{alb_snow}_{alb_ice}_{alb_firn}_{alb_aging}_{alb_depth}_0.24_{roughness_ice}_4.0_0.0026_num2.nc\"\n",
    "    if 'win' in sys.platform:\n",
    "        sim_alb = xr.open_dataset(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Output/albedo_files/LHS/alb_only/\"+\\\n",
    "            filename)\n",
    "    else:\n",
    "        sim_alb = xr.open_dataset(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Output/albedo_files/LHS/alb_only/\"+\\\n",
    "            filename)\n",
    "\n",
    "    #sort by time\n",
    "    #sim_alb = sim_alb.sel(time=albobs.time) \n",
    "    #sim_alb = sim_alb.sortby(\"time\")\n",
    "    list_sim_alb.append(sim_alb.ALBEDO_weighted.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_albedo = pd.DataFrame(list_sim_alb, columns=[f'alb{i+1}' for i in range(len(sim_alb.ALBEDO_weighted.data))])\n",
    "df_albedo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select albedo data\n",
    "# drop index from dataframe - since we loop over it, dfs are aligned\n",
    "params.reset_index(drop=True, inplace=True)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_merged = pd.concat([params, df_albedo], axis=1)\n",
    "params_merged "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_subset = params_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_emulator(seed=None):\n",
    "    if seed is not None:\n",
    "        tf.keras.utils.set_random_seed(seed)\n",
    "        tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "    mass_balance_input = Input(shape=(7,), name=\"mass_balance_input\")\n",
    "    snowlines_input = Input(shape=(58, 9), name=\"snowlines_input\")\n",
    "    alb_input = Input(shape=(98, 9), name=\"alb_input\")\n",
    "\n",
    "    # Shared Layers for Mass Balance\n",
    "    shared_mb = Dense(64, activation='relu')(mass_balance_input)\n",
    "    shared_mb = Dense(128, activation='relu')(shared_mb)\n",
    "    shared_mb = Dropout(0.1)(shared_mb)\n",
    "\n",
    "    # Mass Balance Branch\n",
    "    mb_branch = Dense(64, activation='relu')(shared_mb)\n",
    "    mb_output = Dense(1, name=\"mass_balance_output\")(mb_branch)\n",
    "\n",
    "    # Shared Layers for Snowlines\n",
    "    # LSTM Layers (Stacked)\n",
    "    lstm_out = Bidirectional(LSTM(64, return_sequences=True))(snowlines_input)\n",
    "    lstm_out = Bidirectional(LSTM(64, return_sequences=True))(lstm_out)\n",
    "\n",
    "    # Shared Layers for Albedo\n",
    "    # LSTM Layers (Stacked)\n",
    "    lstm_alb = Bidirectional(LSTM(64, return_sequences=True))(alb_input)\n",
    "    lstm_alb = Bidirectional(LSTM(64, return_sequences=True))(lstm_alb)\n",
    "\n",
    "    # Multi-Scale CNN (Inception Style)\n",
    "    #conv1 = Conv1D(filters=64, kernel_size=2, padding=\"same\", activation=\"relu\")(lstm_alb) #3 months, 6 months, 12 months\n",
    "    #conv2 = Conv1D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\")(lstm_out)\n",
    "\n",
    "    # Combine Multi-Scale Features\n",
    "    #x = Concatenate()([conv1, conv2])\n",
    "    #x = BatchNormalization()(x)\n",
    "\n",
    "    #shared_sl = LSTM(64, return_sequences=True)(snowlines_input)\n",
    "    shared_sl = Dense(128, activation='relu')(lstm_out)\n",
    "\n",
    "    # Snowlines Branch\n",
    "    sl_branch = Dense(64, activation='relu')(shared_sl)\n",
    "    #sl_branch = Dropout(0.1)(sl_branch)  # Add Dropout here for snowlines branch\n",
    "    snowlines_output = Dense(1, activation='sigmoid')(sl_branch)  # Predict one value per time point\n",
    "    snowlines_output = Reshape((58,), name=\"snowlines_output\")(snowlines_output)  # Adjust shape to (batch_size, 62)\n",
    "\n",
    "    ### Albedo\n",
    "    #shared_alb = Dense(128, activation='relu')(conv1)\n",
    "    shared_alb = Dense(128, activation='relu')(lstm_alb)\n",
    "\n",
    "    # Snowlines Branch\n",
    "    alb_branch = Dense(64, activation='relu')(shared_alb)\n",
    "    #sl_branch = Dropout(0.1)(sl_branch)  # Add Dropout here for snowlines branch\n",
    "    alb_output = Dense(1, activation='sigmoid')(alb_branch)  # Predict one value per time point\n",
    "    alb_output = Reshape((98,), name=\"alb_output\")(alb_output)  # Adjust shape to (batch_size, 62)\n",
    "\n",
    "    # Combine\n",
    "    model = Model(inputs=[mass_balance_input, snowlines_input, alb_input],\n",
    "                  outputs=[mb_output, snowlines_output, alb_output])\n",
    "\n",
    "    loss_type = \"huber\"\n",
    "    # Compile the model with a combined loss function\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss={\n",
    "            'mass_balance_output': \"mse\",\n",
    "            'snowlines_output': loss_type,\n",
    "            'alb_output': loss_type\n",
    "        },\n",
    "        loss_weights={\n",
    "            'mass_balance_output': 1.0,  # You can tune this\n",
    "            'snowlines_output': 1.0,      # You can tune this\n",
    "            'alb_output': 1.0\n",
    "        },\n",
    "        metrics=[keras.metrics.RootMeanSquaredError(),keras.metrics.RootMeanSquaredError(),keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "def plot_loss(history, loss_var):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss for mass balance and snowlines.\n",
    "    \n",
    "    Parameters:\n",
    "    history: History object returned by the Keras model.fit() method.\n",
    "    \"\"\"\n",
    "    # Plot loss for mass balance output\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Plot training and validation loss for mass balance\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history['mass_balance_output_loss'], label='Train Mass Balance Loss')\n",
    "    plt.plot(history.history['val_mass_balance_output_loss'], label='Val Mass Balance Loss')\n",
    "    plt.title('Mass Balance Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(loss_var)\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training and validation loss for snowlines\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history['snowlines_output_loss'], label='Train Snowlines Loss')\n",
    "    plt.plot(history.history['val_snowlines_output_loss'], label='Val Snowlines Loss')\n",
    "    plt.title('Snowlines Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(loss_var)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot training and validation loss for albedo\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history.history['alb_output_loss'], label='Train Albedo Loss')\n",
    "    plt.plot(history.history['val_alb_output_loss'], label='Val Albedo Loss')\n",
    "    plt.title('Albedo Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(loss_var)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_scatter(y_train, y_pred, savefig=False):\n",
    "    idxs = np.arange(len(y_train))\n",
    "    np.random.shuffle(idxs)\n",
    "\n",
    "    y_expected = y_train.reshape(-1)[idxs[:]]\n",
    "    y_predicted = y_pred.reshape(-1)[idxs[:]]\n",
    "\n",
    "    xy = np.vstack([y_expected, y_predicted])\n",
    "    z = gaussian_kde(xy)(xy)\n",
    "    print(xy.shape)\n",
    "    # Sort the points by density, so that the densest points are plotted last\n",
    "    idx = z.argsort()\n",
    "    y_plt, ann_plt, z = y_expected[idx], y_predicted[idx], z[idx]\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title(\"Model Evaluation\", fontsize=17)\n",
    "    plt.ylabel('Emulated MB (m.w.e)', fontsize=16)\n",
    "    plt.xlabel('Reference MB (m.w.e)', fontsize=16)\n",
    "    sc = plt.scatter(y_plt, ann_plt, s=20)\n",
    "    #plt.clim(0,0.4)\n",
    "    plt.tick_params(labelsize=14)\n",
    "    #plt.colorbar(sc) \n",
    "    lineStart = -4.5\n",
    "    lineEnd = 1.5\n",
    "    plt.plot([lineStart, lineEnd], [lineStart, lineEnd], 'k-')\n",
    "    plt.axvline(0.0, ls='-.', c='k')\n",
    "    plt.axhline(0.0, ls='-.', c='k')\n",
    "    plt.xlim(lineStart, lineEnd)\n",
    "    plt.ylim(lineStart, lineEnd)\n",
    "    plt.gca().set_box_aspect(1)\n",
    "    plt.grid()\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_expected, y_predicted)\n",
    "    r2 = r2_score(y_expected, y_predicted)\n",
    "    rmse = root_mean_squared_error(y_expected, y_predicted)\n",
    "    # Calculate quantile edges (4 bins)\n",
    "    quantiles = np.quantile(y_expected, [0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "    bin_labels = ['Q1 (lowest)', 'Q2', 'Q3', 'Q4 (highest)']\n",
    "\n",
    "    # Assign each obs value to a quantile bin\n",
    "    bin_indices = pd.cut(y_expected, bins=quantiles, labels=bin_labels, include_lowest=True)\n",
    "\n",
    "    # Compute RMSE per bin\n",
    "    rmse_by_bin = {}\n",
    "    for label in bin_labels:\n",
    "        mask = bin_indices == label\n",
    "        if np.any(mask):\n",
    "            rmse = np.sqrt(np.mean((y_predicted[mask] - y_expected[mask])**2))\n",
    "            rmse_by_bin[label] = rmse\n",
    "\n",
    "    # Print results\n",
    "    for label, rmse in rmse_by_bin.items():\n",
    "        print(f\"{label}: RMSE = {rmse:.4f}\")\n",
    "    \"\"\"\n",
    "    textstr = '\\n'.join((\n",
    "        r'$MAE=%.2f$' % (mae, ),\n",
    "        r'$R^2=%.2f$' % (r2, ), \n",
    "        r'$RMSE=%.2f$' % (rmse, )))\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    # place a text box in upper left in axes coords\n",
    "    plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=14,\n",
    "            verticalalignment='top', bbox=props)\n",
    "    if savefig:\n",
    "        plt.savefig(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/mb_emulator.png\")\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    return (mae, r2, rmse, rmse_by_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [300, 400, 500, 1000, 1500, 2000]\n",
    "num_repeats = 10\n",
    "\n",
    "results = {}\n",
    "\n",
    "for n in sample_sizes:\n",
    "    print(f\"Starting NN generation with {n} samples.\")\n",
    "    results[n] = []\n",
    "    for repeat in range(num_repeats):\n",
    "        print(\"N. repetetion: \", repeat)\n",
    "        seed = repeat  # Different seed per run\n",
    "\n",
    "        # Random sample from the full training data\n",
    "        train_dataset, validation_dataset = train_test_split(params_subset.index, \n",
    "                                                    train_size=0.8,\n",
    "                                                    test_size=0.2, random_state=seed)\n",
    "\n",
    "        df_train = params_subset.loc[train_dataset]\n",
    "        df_validation = params_subset.loc[validation_dataset]\n",
    "        list_sims = [x for x in params_subset.columns if 'sim' in x]\n",
    "        list_albs = [x for x in params_subset.columns if 'alb' in x if x not in ['alb_ice','alb_snow','alb_firn','albedo_aging','albedo_depth']]\n",
    "        \n",
    "        # Fit scalers using training data\n",
    "        features_to_drop = ['mb'] + list_sims + list_albs\n",
    "        df_train_X = df_train.drop(features_to_drop, axis=1)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df_train_X.values)\n",
    "\n",
    "        # Transform inputs and outputs\n",
    "        X_train_scaled = scaler.transform(df_train_X.values)  # Shape: (n_samples, 6)\n",
    "        \n",
    "        \n",
    "        # Now run maximin sampling on this scaled space\n",
    "        # i.e. maximize the minimum distance between the selected points.\n",
    "        def maximin_subset(X, n_samples):\n",
    "            selected = [np.random.randint(0, X.shape[0])]\n",
    "            while len(selected) < n_samples:\n",
    "                dists = cdist(X[selected], X)\n",
    "                min_dist = np.min(dists, axis=0)\n",
    "                min_dist[selected] = -np.inf\n",
    "                next_idx = np.argmax(min_dist)\n",
    "                selected.append(next_idx)\n",
    "            return selected\n",
    "\n",
    "        # Sample sizes     \n",
    "        subsets = {}\n",
    "        idx = maximin_subset(X_train_scaled, n)\n",
    "\n",
    "        df_subset = df_train.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "        # Inputs: same features used in full training input\n",
    "        X = df_subset.drop(features_to_drop, axis=1).values\n",
    "        X_scaled = scaler.transform(X)\n",
    "\n",
    "        # Outputs\n",
    "        y_mb = df_subset[['mb']].values\n",
    "        y_tsla = df_subset[list_sims].values\n",
    "        y_alb = df_subset[list_albs].values\n",
    "\n",
    "        # Store everything\n",
    "        subsets[n] = {\n",
    "            'X_scaled': X_scaled,\n",
    "            'y_mb': y_mb,\n",
    "            'y_tsla': y_tsla,\n",
    "            'y_alb': y_alb,\n",
    "        }\n",
    "        # load results\n",
    "        n_samples = n\n",
    "        subset = subsets[n_samples]\n",
    "\n",
    "        X_train = subset['X_scaled']\n",
    "        df_train_y_mb = subset['y_mb']\n",
    "        df_train_y_tsla = subset['y_tsla']\n",
    "        df_train_y_alb = subset['y_alb']\n",
    "\n",
    "        # Repeat the input features for time points\n",
    "        X_train_expanded = np.repeat(X_train[:, None, :], len(doy), axis=1)  # Snowlines\n",
    "        X_train_expanded_alb = np.repeat(X_train[:, None, :], len(doy_alb), axis=1) # Albedo\n",
    "\n",
    "        # Add time features without scaling\n",
    "        X_train_with_time = np.concatenate([X_train_expanded, np.tile(time_features, (X_train.shape[0], 1, 1))], axis=-1)\n",
    "        X_train_with_time_alb = np.concatenate([X_train_expanded_alb, np.tile(time_features_alb, (X_train.shape[0], 1, 1))], axis=-1)\n",
    "        \n",
    "        # Repeat the same for validation data\n",
    "        df_validation_X = df_validation.drop(features_to_drop, axis=1)\n",
    "        df_validation_y_mb = df_validation[['mb']].values\n",
    "        df_validation_y_tsla = df_validation[list_sims].values\n",
    "        df_validation_y_alb = df_validation[list_albs].values\n",
    "\n",
    "        X_validation = scaler.transform(df_validation_X.values)  # Shape: (n_samples, 6)\n",
    "\n",
    "        X_validation_expanded = np.repeat(X_validation[:, None, :], len(doy), axis=1)  # Shape: (n_samples, 62, 6)\n",
    "        X_validation_with_time = np.concatenate(\n",
    "            [X_validation_expanded, np.tile(time_features, (X_validation.shape[0], 1, 1))], axis=-1\n",
    "        )\n",
    "        # Final shape: (n_samples, 62, 8)\n",
    "        X_validation_expanded_alb = np.repeat(X_validation[:, None, :], len(doy_alb), axis=1)  # Shape: (n_samples, 62, 6)\n",
    "        X_validation_with_time_alb = np.concatenate(\n",
    "            [X_validation_expanded_alb, np.tile(time_features_alb, (X_validation.shape[0], 1, 1))], axis=-1\n",
    "        )\n",
    "        \n",
    "        print(X_train.shape)\n",
    "        print(df_train_y_mb.shape)\n",
    "        print(df_train_y_tsla.shape)\n",
    "        print(df_train_y_alb.shape)\n",
    "        \n",
    "        \n",
    "        # Build and train\n",
    "        model = build_emulator(seed=seed)\n",
    "        # Early Stopping and Learning Rate Scheduler\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, verbose=1)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, min_lr=1e-6, verbose=1)\n",
    "        # Fit the model\n",
    "        history = model.fit(\n",
    "            x={\n",
    "                'mass_balance_input': X_train, \n",
    "                'snowlines_input': X_train_with_time,\n",
    "                'alb_input': X_train_with_time_alb\n",
    "            },\n",
    "            y={\n",
    "                'mass_balance_output': df_train_y_mb, \n",
    "                'snowlines_output': df_train_y_tsla,\n",
    "                'alb_output': df_train_y_alb\n",
    "            },\n",
    "            validation_data=(\n",
    "                {\n",
    "                    'mass_balance_input': X_validation,\n",
    "                    'snowlines_input': X_validation_with_time,\n",
    "                    'alb_input': X_validation_with_time_alb\n",
    "                },\n",
    "                {\n",
    "                    'mass_balance_output': df_validation_y_mb,\n",
    "                    'snowlines_output': df_validation_y_tsla,\n",
    "                    'alb_output': df_validation_y_alb\n",
    "                }\n",
    "            ),\n",
    "            epochs=300,\n",
    "            batch_size=32,\n",
    "            callbacks=[early_stopping, lr_scheduler],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Evaluate the model\n",
    "        loss, mb_loss, sl_loss, alb_loss, mb_mae, sl_mae, alb_mae = model.evaluate(\n",
    "            x={\n",
    "                'mass_balance_input': X_validation, \n",
    "                'snowlines_input': X_validation_with_time,\n",
    "                'alb_input': X_validation_with_time_alb\n",
    "            },\n",
    "            y={\n",
    "                'mass_balance_output': df_validation_y_mb, \n",
    "                'snowlines_output': df_validation_y_tsla,\n",
    "                'alb_output': df_validation_y_alb\n",
    "            },\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        print(f\"Total Loss: {loss}\")\n",
    "        print(f\"Mass Balance Loss: {mb_loss}, MAE: {mb_mae}\")\n",
    "        print(f\"Snowlines Loss: {sl_loss}, MAE: {sl_mae}\")\n",
    "        print(f\"Albedo Loss: {alb_loss}, MAE: {alb_mae}\")\n",
    "        \n",
    "        \n",
    "        #\n",
    "        #plot_loss(history, \"huber\")\n",
    "        #\n",
    "        predictions = model.predict({\n",
    "            'mass_balance_input': X_validation,\n",
    "            'snowlines_input': X_validation_with_time,\n",
    "            'alb_input': X_validation_with_time_alb\n",
    "            })\n",
    "        predicted_mass_balance = predictions[0]  # Shape: (N, 1)\n",
    "        predicted_snowlines = predictions[1]     # Shape: (N, 62)\n",
    "        predicted_albedos = predictions[2]\n",
    "        \n",
    "        metrics_dict = {\n",
    "            \"MB\": {\n",
    "                \"MAE\": -9999,\n",
    "                \"RMSE\": -9999,\n",
    "                \"R2\": -9999,\n",
    "            },\n",
    "            \"TSLA\": {\n",
    "                \"MAE\": -9999,\n",
    "                \"RMSE\": -9999,\n",
    "                \"R2\": -9999,\n",
    "            },\n",
    "            \"ALB\": {\n",
    "                \"MAE\": -9999,\n",
    "                \"RMSE\": -9999,\n",
    "                \"R2\": -9999,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        mae_mb, r2_mb, rmse_mb, rmse_by_bin_mb = plot_scatter(df_validation_y_mb, predicted_mass_balance, savefig=False)\n",
    "        metrics_dict[\"MB\"][\"MAE\"] = mae_mb\n",
    "        metrics_dict[\"MB\"][\"RMSE\"] = rmse_mb\n",
    "        metrics_dict[\"MB\"][\"R2\"] = r2_mb\n",
    "        \n",
    "        max_glacier_elev = 3700.0\n",
    "        min_glacier_elev = 2440.0\n",
    "        \n",
    "        # Compute point density\n",
    "        \"\"\"\n",
    "        plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "        xy = np.vstack([df_validation_y_tsla.flatten(), predicted_snowlines.flatten()])\n",
    "        density = gaussian_kde(xy)(xy)\n",
    "\n",
    "        # Sort the points by density to ensure denser points appear on top\n",
    "        \n",
    "        idx = density.argsort()\n",
    "        x_sorted, y_sorted, density_sorted = df_validation_y_tsla.flatten()[idx], predicted_snowlines.flatten()[idx], density[idx]\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Create the plot\n",
    "        fig, ax = plt.subplots(1,1, figsize=(8,8))\n",
    "        ax.set_title(\"Model Evaluation\", fontsize=17)\n",
    "        ax.set_ylabel('Emulated Norm. TSLA', fontsize=16)\n",
    "        ax.set_xlabel('Reference Norm. TSLA', fontsize=16)\n",
    "        lineStart = 0.0\n",
    "        lineEnd = 1.0\n",
    "        ax.plot([lineStart, lineEnd], [lineStart, lineEnd], 'k-')\n",
    "        ax.set_xlim(lineStart, lineEnd)\n",
    "        ax.set_ylim(lineStart, lineEnd)\n",
    "        plt.gca().set_box_aspect(1)\n",
    "        \"\"\"\n",
    "        # Compute error metrics\n",
    "        mae_score_tsla = mean_absolute_error(df_validation_y_tsla.flatten(), predicted_snowlines.flatten())\n",
    "        r2_scores_tsla = r2_score(df_validation_y_tsla.flatten(), predicted_snowlines.flatten())\n",
    "        mae_in_meter = mae_score_tsla * (max_glacier_elev - min_glacier_elev)\n",
    "        rmse_score_tsla = root_mean_squared_error(df_validation_y_tsla.flatten(), predicted_snowlines.flatten())\n",
    "        rmse_in_meter = rmse_score_tsla * (max_glacier_elev - min_glacier_elev)\n",
    "        \"\"\"\n",
    "        textstr = '\\n'.join((\n",
    "            r'$MAE=%.4f$' % (mae_score_tsla, ),\n",
    "            r'$MAE (m)=%.4f$' % (mae_in_meter, ),\n",
    "            r'$RMSE=%.4f$' % (rmse_score_tsla, ),\n",
    "            r'$RMSE (m)=%.4f$' % (rmse_in_meter, ),\n",
    "            r'$R^2=%.4f$' % (r2_scores_tsla, )))\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=14,\n",
    "                verticalalignment='top', bbox=props)\n",
    "        #\n",
    "        \"\"\"\n",
    "        # Compute quantiles and remove duplicates\n",
    "        quantiles = np.quantile(df_validation_y_tsla.flatten(), [0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "        quantiles = np.unique(quantiles)  # ensures unique edges\n",
    "\n",
    "        # Dynamically generate labels\n",
    "        bin_labels = [f\"Q{i+1}\" for i in range(len(quantiles)-1)]\n",
    "\n",
    "        # Assign to bins using the valid (unique) quantiles and labels\n",
    "        bin_indices = pd.cut(\n",
    "            df_validation_y_tsla.flatten(),\n",
    "            bins=quantiles,\n",
    "            labels=bin_labels,\n",
    "            include_lowest=True\n",
    "        )\n",
    "\n",
    "        # Compute RMSE per bin\n",
    "        rmse_by_bin = {}\n",
    "        for label in bin_labels:\n",
    "            mask = bin_indices == label\n",
    "            if np.any(mask):\n",
    "                rmse = np.sqrt(np.mean((predicted_snowlines.flatten()[mask] - df_validation_y_tsla.flatten()[mask])**2))\n",
    "                rmse_by_bin[label] = rmse\n",
    "\n",
    "        # Print results\n",
    "        for label, rmse in rmse_by_bin.items():\n",
    "            rmse_m = rmse * (max_glacier_elev - min_glacier_elev)\n",
    "            print(f\"{label}: RMSE = {rmse_m:.4f}\")\n",
    "        \"\"\"\n",
    "        # Secondary axes\n",
    "        def custom_ticks(y):\n",
    "            orig_max = max_glacier_elev\n",
    "            orig_min = min_glacier_elev\n",
    "            return y * (orig_max - orig_min) + orig_min\n",
    "\n",
    "        ax2y = ax.secondary_yaxis(-0.14, functions=(custom_ticks, custom_ticks))\n",
    "        ax2x = ax.secondary_xaxis(-0.1, functions=(custom_ticks, custom_ticks))\n",
    "        list_labels = [custom_ticks(x) for x in np.arange(0,1+0.1,0.1)]\n",
    "        ax2x.set_xticks(list_labels)\n",
    "        ax2y.set_yticks(list_labels)\n",
    "        ax2x.set_xticklabels([round(x) for x in ax2x.get_xticks()], rotation=30)\n",
    "\n",
    "        # Scatter plot with density coloring\n",
    "        sc = ax.scatter(x_sorted, y_sorted, c=density_sorted, s=20, cmap='plasma', alpha=0.7)\n",
    "        #cb = plt.colorbar(sc, ax=ax, label=\"Density\")\n",
    "\n",
    "        ax.set_xticks(np.arange(0,1+0.1,0.1))\n",
    "        ax.set_yticks(np.arange(0,1+0.1,0.1))\n",
    "        ax.grid(True)\n",
    "        #plt.savefig(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/tsla_emulator.png\")\n",
    "        \"\"\"\n",
    "        metrics_dict[\"TSLA\"][\"MAE\"] = mae_score_tsla\n",
    "        metrics_dict[\"TSLA\"][\"RMSE\"] = rmse_score_tsla\n",
    "        metrics_dict[\"TSLA\"][\"R2\"] = r2_scores_tsla\n",
    "        \n",
    "        ## ALbedo\n",
    "        # Compute point density\n",
    "        \"\"\"\n",
    "        xy = np.vstack([df_validation_y_alb.flatten(), predicted_albedos.flatten()])\n",
    "        density = gaussian_kde(xy)(xy)\n",
    "\n",
    "        # Sort the points by density to ensure denser points appear on top\n",
    "        idx = density.argsort()\n",
    "        x_sorted, y_sorted, density_sorted = df_validation_y_alb.flatten()[idx], predicted_albedos.flatten()[idx], density[idx]\n",
    "        \"\"\"\n",
    "        # Create the plot\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(1,1, figsize=(8,8))\n",
    "        ax.set_title(\"Model Evaluation\", fontsize=17)\n",
    "        ax.set_ylabel('Modeled Mean Albedo', fontsize=16)\n",
    "        ax.set_xlabel('Reference Mean Albedo', fontsize=16)\n",
    "        lineStart = 0.0\n",
    "        lineEnd = 1.0\n",
    "        ax.plot([lineStart, lineEnd], [lineStart, lineEnd], 'k-')\n",
    "        ax.set_xlim(lineStart, lineEnd)\n",
    "        ax.set_ylim(lineStart, lineEnd)\n",
    "        plt.gca().set_box_aspect(1)\n",
    "        \"\"\"\n",
    "        # Compute error metrics\n",
    "        mae_score_alb = mean_absolute_error(df_validation_y_alb.flatten(), predicted_albedos.flatten())\n",
    "        r2_scores_alb = r2_score(df_validation_y_alb.flatten(), predicted_albedos.flatten())\n",
    "        rmse_score_alb = root_mean_squared_error(df_validation_y_alb.flatten(), predicted_albedos.flatten())\n",
    "        \"\"\"\n",
    "        textstr = '\\n'.join((\n",
    "            r'$MAE=%.4f$' % (mae_score_alb, ),\n",
    "            r'$RMSE=%.4f$' % (rmse_score_alb, ),\n",
    "            r'$R^2=%.4f$' % (r2_scores_alb, )))\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=14,\n",
    "                verticalalignment='top', bbox=props)\n",
    "        \"\"\"\n",
    "        # Compute quantiles and remove duplicates\n",
    "        quantiles = np.quantile(df_validation_y_alb.flatten(), [0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "        quantiles = np.unique(quantiles)  # ensures unique edges\n",
    "\n",
    "        # Dynamically generate labels\n",
    "        bin_labels = [f\"Q{i+1}\" for i in range(len(quantiles)-1)]\n",
    "\n",
    "        # Assign to bins using the valid (unique) quantiles and labels\n",
    "        bin_indices = pd.cut(\n",
    "            df_validation_y_alb.flatten(),\n",
    "            bins=quantiles,\n",
    "            labels=bin_labels,\n",
    "            include_lowest=True\n",
    "        )\n",
    "\n",
    "        # Compute RMSE per bin\n",
    "        rmse_by_bin = {}\n",
    "        for label in bin_labels:\n",
    "            mask = bin_indices == label\n",
    "            if np.any(mask):\n",
    "                rmse = np.sqrt(np.mean((predicted_albedos.flatten()[mask] - df_validation_y_alb.flatten()[mask])**2))\n",
    "                rmse_by_bin[label] = rmse\n",
    "\n",
    "        # Print results\n",
    "        for label, rmse in rmse_by_bin.items():\n",
    "            print(f\"{label}: RMSE = {rmse:.4f}\")\n",
    "        \"\"\"\n",
    "        # Scatter plot with density coloring\n",
    "        sc = ax.scatter(x_sorted, y_sorted, c=density_sorted, s=20, cmap='plasma', alpha=0.7)\n",
    "        #cb = plt.colorbar(sc, ax=ax, label=\"Density\")\n",
    "\n",
    "        ax.set_xticks(np.arange(0,1+0.1,0.1))\n",
    "        ax.set_yticks(np.arange(0,1+0.1,0.1))\n",
    "        ax.grid(True)\n",
    "        #plt.savefig(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/alb_emulator.png\")\n",
    "        \"\"\"\n",
    "        metrics_dict[\"ALB\"][\"MAE\"] = mae_score_alb\n",
    "        metrics_dict[\"ALB\"][\"RMSE\"] = rmse_score_alb\n",
    "        metrics_dict[\"ALB\"][\"R2\"] = r2_scores_alb\n",
    "        \n",
    "        #\n",
    "        print(metrics_dict)\n",
    "        \n",
    "        experiment_data = {\n",
    "            \"n_samples\": n_samples,                       # Sample size used\n",
    "            \"train_data\": X_train,              # Typically an xarray.Dataset or pandas.DataFrame\n",
    "            \"test_data\": X_validation,                # Same format\n",
    "            \"predicted_mb\": predictions[0],  # Shape: (N, 1)\n",
    "            \"predicted_tsla\": predictions[1],    # Shape: (N, 62)\n",
    "            \"predicted_alb\": predictions[2],\n",
    "            \"metrics\": metrics_dict,               # MAE, RMSE, loglik, etc.\n",
    "        }\n",
    "        \n",
    "        ## Save to file\n",
    "        \n",
    "        if 'win' in sys.platform:\n",
    "            model.save(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/NN_test/model_{n_samples}_samples.keras\")\n",
    "        else:\n",
    "            model.save(f\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/NN_test/model_{n_samples}v{repeat}_samples.keras\")\n",
    "\n",
    "            with open(f\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/NN_test/model_experiment_{n_samples}v{repeat}_samples.pkl\", \"wb\") as f:\n",
    "                pickle.dump(experiment_data, f)\n",
    "                \n",
    "        #\n",
    "        del model\n",
    "        del history\n",
    "        del metrics_dict\n",
    "        del experiment_data\n",
    "        K.clear_session()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
