{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0d5e413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import salem\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8d24926",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'win' in sys.platform:\n",
    "    path = \"E:/OneDrive - uibk.ac.at/PhD/PhD/Data/Hintereisferner/Climate/CORDEX-DKRZ/\"\n",
    "else:\n",
    "    path = \"/mnt/C4AEBBABAEBB9500/OneDrive - uibk.ac.at/PhD/PhD/Data/Hintereisferner/Climate/CORDEX-DKRZ/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First load snowfall data and fix timestamp\n",
    "ds = xr.open_dataset(path+\"cosmo_1998-2010_1d_prsn.nc\")\n",
    "print(ds.time.values[0], ds.time.values[-1])\n",
    "print(ds.time_bnds.values[0])\n",
    "#temporary store old values\n",
    "oldvals = ds['prsn'].values\n",
    "#clean-up timestamps, given at noon of each day, bounds from 1999-01-01 to 1999-01-02 -> values should be assigned to 01-02 \n",
    "resampled = ds[['prsn']].resample(time=\"1D\").sum()\n",
    "print(resampled) \n",
    "## Ensure they are equal\n",
    "print(np.testing.assert_allclose(oldvals, resampled['prsn'], atol=1e-07))\n",
    "del oldvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29c711",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare pr data\n",
    "pr = xr.open_dataset(path+\"cosmo_1998-2010_1h_pr.nc\")\n",
    "print(pr.time.values[0], pr.time.values[-1])\n",
    "print(pr.time_bnds.values[0])\n",
    "\n",
    "#given at every half-hour - time_bnds from 00h to 01h -> values should be assigned to 01h \n",
    "time_range = pd.date_range(\"1998-11-01T01:00:00\", \"2010-01-01\", freq=\"1H\")\n",
    "\n",
    "pr['time'] = ('time', time_range)\n",
    "## Convert units\n",
    "pr['pr'] = pr['pr'] * 3600 #kg/m2s to kg/m2h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e13ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do a check on magnitude! Noticed one outlier! \n",
    "\n",
    "# Get proj and outlines, dsr is single downloaded file as a placeholder\n",
    "if 'win' in sys.platform:\n",
    "    dsr = salem.open_metum_dataset(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/for_Niklas/cosmo_2009_1h.nc\")\n",
    "    hef = gpd.read_file(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Static/RGI6/HEF_RGI6.shp\")\n",
    "else:\n",
    "    dsr = salem.open_metum_dataset(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/for_Niklas/cosmo_2009_1h.nc\")\n",
    "    hef = gpd.read_file(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Static/RGI6/HEF_RGI6.shp\")\n",
    "reproj_hef = hef.to_crs(dsr.pyproj_srs)\n",
    "bounds = reproj_hef.bounds\n",
    "print(bounds)\n",
    "\n",
    "centroid = reproj_hef.dissolve().centroid\n",
    "print(centroid)\n",
    "\n",
    "idx_lat = np.argmin(np.abs(pr.rlat.values  - centroid.y.values))\n",
    "idx_lon = np.argmin(np.abs(pr.rlon.values  - centroid.x.values))\n",
    "print(idx_lat, idx_lon)\n",
    "\n",
    "# Create single file at centroid from which to distribute using lapse rates \n",
    "x_cords, y_cords = centroid.iloc[0].xy\n",
    "x_cord = x_cords[0]\n",
    "y_cord = y_cords[0]\n",
    "print(x_cord, y_cord)\n",
    "\n",
    "ds_closest = pr.sel(rlat=y_cord, rlon=x_cord, method='nearest')\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(16,9), dpi=150)\n",
    "ax.plot(ds_closest.time, ds_closest['pr'])\n",
    "ax.set_ylabel(\"Total Precipitation rate [kg/m2 h]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38072244",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get idx of outlier event\n",
    "time_max = ds_closest.pr.argmax()\n",
    "print(ds_closest.pr.where(ds_closest.pr > 100, drop=True))\n",
    "## Find timestep and replace it with neighbouring values\n",
    "print(np.nanmax(pr.pr.isel(time=time_max.values).values))\n",
    "time_max\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9058ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_surr = (pr.pr.isel(time=time_max.values-1) + pr.pr.isel(time=time_max.values+1)) / 2\n",
    "## Fix outlier by hand\n",
    "## replace this timestep with mean of previous and next timestep\n",
    "pr[\"pr\"] = xr.where(\n",
    "    (pr.time == np.datetime64(\"2005-03-31T23:00:00\")), mean_surr, pr[\"pr\"]\n",
    ")\n",
    "\n",
    "#repeat plot\n",
    "del ds_closest\n",
    "ds_closest = pr.sel(rlat=y_cord, rlon=x_cord, method='nearest')\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(16,9), dpi=150)\n",
    "ax.plot(ds_closest.time, ds_closest['pr'])\n",
    "ax.set_ylabel(\"Fixed Total Precipitation rate [kg/m2 h]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7090dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store precipitation fields to file! \n",
    "\n",
    "pr.to_netcdf(path+\"cosmo_1998-2010_1hcleaned_pr.nc\")\n",
    "\n",
    "pr_daily = pr[['pr']].resample(time=\"1D\").sum()\n",
    "pr_daily.to_netcdf(path+\"cosmo_1998-2010_1d_pr.nc\")\n",
    "pr_filled = pr_daily.resample(time=\"1H\").pad()\n",
    "pr_filled.to_netcdf(path+\"cosmo_1998-2010_1hffilled_pr.nc\")\n",
    "\n",
    "#Beware last time step 2010-01-01 will consist of just one value! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc6bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Current snowfall values need to be shifted by +1 but previous tests showed that this does not work (see 00-05_CORDEX-DKRZ)\n",
    "## That's why we ignore this in the following steps. We copy the last timestep just in case.\n",
    "\n",
    "## Shift values by +1 and add new timestamp\n",
    "missing_time = resampled.isel(time= [-1]) #sel using list to preserve time dimension\n",
    "print(missing_time)\n",
    "#replace time value with next day\n",
    "timestep = np.datetime64(\"2010-01-01\")\n",
    "missing_time[\"time\"] = (\"time\", np.reshape(timestep, (1)))\n",
    "print(missing_time)\n",
    "fixed = xr.concat([resampled, missing_time], dim=\"time\", data_vars=\"minimal\", coords=\"minimal\")\n",
    "print(fixed)\n",
    "print(fixed['prsn'][-3,:,:].values)\n",
    "\"\"\"\n",
    "#shift fixed values by 1\n",
    "fixed = fixed.shift(time=1)\n",
    "print(fixed)\n",
    "print(fixed['prsn'][-2,:,:].values)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18fede99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert kg/m2s to kg/m2 d\n",
    "fixed['prsn'] = fixed['prsn'] * 3600 * 24 #only run once!\n",
    "\n",
    "# repeat fix for timestep in total precipitation for snowfall to get rid of artefacts\n",
    "mean_prsn = (fixed['prsn'].sel(time=\"2005-03-30\") + fixed['prsn'].sel(time=\"2005-04-01\")) / 2\n",
    "## Fix outlier by hand\n",
    "## replace this timestep with mean of previous and next timestep\n",
    "fixed[\"prsn\"] = xr.where(\n",
    "    (fixed.time == np.datetime64(\"2005-03-31\")), mean_prsn, fixed[\"prsn\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed.to_netcdf(path+\"cosmo_1998-2010_1dcleaned_prsn.nc\")\n",
    "\n",
    "#\n",
    "filled = fixed.resample(time=\"1h\").pad()\n",
    "print(filled)\n",
    "filled.to_netcdf(path+\"cosmo_1998-2010_1hffilled_prsn.nc\")\n",
    "\n",
    "## Check timestep where this super large value occurred\n",
    "print(filled.prsn.isel(time=time_max.values).mean(dim=[\"rlat\",\"rlon\"]).values)\n",
    "print(pr_filled.pr.isel(time=time_max.values).mean(dim=[\"rlat\",\"rlon\"]).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1570514",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filled.prsn.isel(time=time_max.values).mean(dim=[\"rlat\",\"rlon\"]).values)\n",
    "print(pr_filled.pr.isel(time=time_max.values).mean(dim=[\"rlat\",\"rlon\"]).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c8541",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e14b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pr_1h = xr.open_dataset(path+\"cosmo_1998-2010_1hcleaned_pr.nc\") #units in kg/m2/h \n",
    "#pr\n",
    "#pr_daily = xr.open_dataset(path+\"cosmo_1998-2010_1d_pr.nc\") #units in kg/m2/d\n",
    "#pr_daily\n",
    "#pr_filled = xr.open_dataset(path+\"cosmo_1998-2010_1hffilled_pr.nc\") #units in kg/m2/d\n",
    "#pr_filled\n",
    "\n",
    "## Crop data to same timeframe starting from 1999-01-01 ##\n",
    "pr = pr.sel(time=slice(\"1999-01-01\",None))\n",
    "print(len(pr.time))\n",
    "pr_daily = pr_daily.sel(time=slice(\"1999-01-01\",None))\n",
    "print(len(pr_daily.time))\n",
    "pr_filled = pr_filled.sel(time=slice(\"1999-01-01\",None))\n",
    "print(len(pr_filled.time))\n",
    "#Date chosen also because for snowfall we only have data from 1999-01-02 onwards\n",
    "\n",
    "pr_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ac03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test if the thingys worked\n",
    "print(np.testing.assert_allclose(pr_daily.pr[0,1,1].values, pr_filled['pr'][0:24,1,1].mean(), atol=1e-07))\n",
    "print(np.testing.assert_allclose(pr_daily.pr[0,1,1].values, pr.pr[0:24,1,1].sum(), atol=1e-07))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4819dc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## downscale hourly snowfall field\n",
    "## implement temperature-based thresholding\n",
    "if 'win' in sys.platform:\n",
    "    extrapath = \"E:/OneDrive - uibk.ac.at/PhD/PhD/Data/Hintereisferner/Climate/Jennings_etal_2018/\"\n",
    "else:\n",
    "    extrapath = \"/mnt/C4AEBBABAEBB9500/OneDrive - uibk.ac.at/PhD/PhD/Data/Hintereisferner/Climate/Jennings_etal_2018/\"\n",
    "\n",
    "station_locs = pd.read_csv(extrapath+\"jennings_et_al_2018_file1_station_locs_elev.csv\")\n",
    "station_thres = pd.read_csv(extrapath+\"jennings_et_al_2018_file3_temp50_observed_by_station.csv\")\n",
    "\n",
    "merged_thres = pd.merge(station_locs, station_thres)\n",
    "merged_thres.dropna(inplace=True)\n",
    "merged_alps = merged_thres.loc[(merged_thres['Longitude'] >= 6) & (merged_thres['Longitude'] <= 14) & (merged_thres['Latitude'] >= 45.5) & (merged_thres['Latitude'] <= 47.5) & (merged_thres['Elevation'] > 1000)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c32b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_alps.temp50.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0fa29ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'win' in sys.platform:\n",
    "    glaciers = salem.read_shapefile(\"E:/OneDrive - uibk.ac.at/PhD/PhD/Data/Spatial/RGI7/RGI2000-v7.0-G-11_central_europe/RGI2000-v7.0-G-11_central_europe.shp\")\n",
    "    hef = salem.read_shapefile(\"E:/OneDrive - uibk.ac.at/PhD/PhD/Data/Hintereisferner/Static/RGI6/HEF_RGI6.shp\")\n",
    "else:\n",
    "    glaciers = salem.read_shapefile(\"/mnt/C4AEBBABAEBB9500/OneDrive - uibk.ac.at/PhD/PhD/Data/Spatial/RGI7/RGI2000-v7.0-G-11_central_europe/RGI2000-v7.0-G-11_central_europe.shp\")\n",
    "    hef = salem.read_shapefile(\"/mnt/C4AEBBABAEBB9500/OneDrive - uibk.ac.at/PhD/PhD/Data/Hintereisferner/Static/RGI6/HEF_RGI6.shp\")\n",
    "# Convert glacier geometries to PlateCarree if necessary\n",
    "if glaciers.crs is not None and glaciers.crs != \"EPSG:4326\":\n",
    "    glaciers = glaciers.to_crs(\"EPSG:4326\")  # Convert to lat/lon if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c281fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert station data to a GeoDataFrame\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "stations = gpd.GeoDataFrame(merged_alps, geometry=gpd.points_from_xy(merged_alps[\"Longitude\"], merged_alps[\"Latitude\"]), crs=\"EPSG:4326\")\n",
    "\n",
    "# Convert both datasets to a projected CRS (UTM Zone 32N, good for Alps)\n",
    "utm_crs = \"EPSG:32632\"\n",
    "stations_reproj = stations.to_crs(utm_crs)\n",
    "hef_reproj = hef.to_crs(utm_crs)\n",
    "\n",
    "# Get the single geometry (assuming only one feature in the shapefile)\n",
    "glacier_geom = hef_reproj.geometry.iloc[0]\n",
    "\n",
    "# Compute distances from each station to the glacier\n",
    "stations_reproj[\"distance_km\"] = stations_reproj.geometry.distance(glacier_geom) / 1000  # Convert meters to km\n",
    "\n",
    "# Find the closest stations (e.g., top 5)\n",
    "closest_stations = stations_reproj.nsmallest(10, \"distance_km\")\n",
    "\n",
    "# Print the closest stations\n",
    "print(closest_stations[[\"Latitude\", \"Longitude\", \"temp50\", \"distance_km\",\"Elevation\"]])\n",
    "print(closest_stations['temp50'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d2045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raster using salem\n",
    "raster = xr.open_dataset(extrapath+\"jennings_et_al_2018_file4_temp50_raster.tif\")  # Opens as xarray dataset\n",
    "raster = raster.isel(band=0)\n",
    "## crop raster to extent (roughly)\n",
    "crop_raster = raster.sel(x=slice(3, 17), y=slice(49.5, 43.5))\n",
    "\n",
    "# Extract temperature at glacier centroid\n",
    "glacier_temp = raster.sel(x=hef.centroid.x.item(), y=hef.centroid.y.item(), method=\"nearest\").band_data.values\n",
    "print(glacier_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9426da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract temperature at station locations\n",
    "stations[\"raster_temp\"] = stations.apply(lambda row: \n",
    "    crop_raster.sel(x=row.geometry.x, y=row.geometry.y, method=\"nearest\").band_data.item(), axis=1)\n",
    "\n",
    "print(f\"Temperature at glacier centroid: {glacier_temp}\")\n",
    "print(stations[[\"Latitude\", \"Longitude\", \"temp50\", \"raster_temp\"]])\n",
    "\n",
    "# --- PLOTTING ---\n",
    "fig, ax = plt.subplots(figsize=(16, 9), dpi=150, subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "\n",
    "vmin = min(crop_raster.band_data.min().values, stations[\"temp50\"].min())  # Get min temperature\n",
    "vmax = max(crop_raster.band_data.max().values, stations[\"temp50\"].max())  # Get max temperature\n",
    "\n",
    "# Plot raster with common colormap\n",
    "img = crop_raster.band_data.plot.imshow(ax=ax, cmap=\"plasma\", vmin=vmin, vmax=vmax, alpha=0.7, add_colorbar=False)\n",
    "\n",
    "# Plot stations using the same colormap and limits\n",
    "sc = ax.scatter(stations.geometry.x, stations.geometry.y, c=stations[\"temp50\"], cmap=\"plasma\",\n",
    "                edgecolors=\"k\", s=50, transform=ccrs.PlateCarree(), vmin=vmin, vmax=vmax, label=\"Stations\")\n",
    "\n",
    "# Add gridlines\n",
    "gl = ax.gridlines(draw_labels=True, linestyle=\"--\", linewidth=0.5, color=\"gray\")\n",
    "\n",
    "# Customize gridline labels\n",
    "gl.top_labels = False  # Hide labels at the top\n",
    "gl.right_labels = False  # Hide labels on the right\n",
    "gl.xlabel_style = {\"size\": 10, \"color\": \"black\"}  # Customize x-axis labels\n",
    "gl.ylabel_style = {\"size\": 10, \"color\": \"black\"}  # Customize y-axis labels\n",
    "\n",
    "# Plot glacier\n",
    "ax.add_geometries([hef.iloc[0].geometry], crs=ccrs.PlateCarree(), edgecolor=\"black\", facecolor=\"red\", linewidth=2, label=\"Glacier\")\n",
    "\n",
    "# Add features\n",
    "ax.set_extent([6, 14, 45.5, 47.5], crs=ccrs.PlateCarree())\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=\":\")\n",
    "ax.add_feature(cfeature.LAND, edgecolor=\"black\", facecolor=\"lightgray\")\n",
    "\n",
    "# Add colorbar (shared with both raster & stations)\n",
    "cbar = plt.colorbar(sc, ax=ax, orientation=\"horizontal\", label=\"Temperature (°C)\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd389f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build percentages, where division by 0 not possible, set to 0\n",
    "#pr['pr_perc'] = np.divide(pr['pr'], pr_filled['pr'], out=np.zeros_like(pr_filled['pr']),\n",
    "#                          where=pr_filled['pr']!=0)\n",
    "pr['pr_daily'] = pr_filled['pr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43563ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load snowfall data as comparison\n",
    "sf = fixed.copy()\n",
    "print(sf)\n",
    "#sf_daily = xr.open_dataset(path+\"cosmo_1998-2010_1d_prsn.nc\").sel(time=slice(\"1999-01-02\",None)) #kg/m2/d\n",
    "#print(sf_daily)\n",
    "sf_filled = filled.copy()\n",
    "print(sf_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e14d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.testing.assert_allclose(sf['prsn'][0,1,1], sf_filled['prsn'][0:24,1,1].mean(), atol=1e-07))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e405291",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check daily precipitation vs snowfall sum\n",
    "#Set NaN where no precipitation is happening\n",
    "pr['pr_onlyprec'] = (('time','rlat','rlon'), np.where(pr.pr_daily == 0, np.nan, pr.pr_daily))\n",
    "sf_filled['prsn_onlyprec'] = (('time','rlat','rlon'), np.where(sf_filled.prsn == 0, np.nan, sf_filled.prsn))\n",
    "\n",
    "## kg/m2 d \n",
    "dif_field = (pr.pr_onlyprec) - (sf_filled.prsn_onlyprec)\n",
    "print(dif_field.argmin(dim=\"time\"))\n",
    "dif_field.min(dim=\"time\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d6dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, figsize=(16,9))\n",
    "dif_field.plot.hist(bins=np.arange(round(np.nanmin(dif_field)-1), np.nanmax(dif_field)+1, 1), ax=ax[0], edgecolor='black', density=True)\n",
    "ax[0].set_xlabel(\"Tot PR - SF [kg/m2 d]\")\n",
    "#ax[0].set_xlim(-30,50)\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "dif_field.where(dif_field < 0).plot.hist(bins=np.arange(round(np.nanmin(dif_field)-1), np.nanmax(dif_field)+1, 1), ax=ax[1], edgecolor='black', density=True)\n",
    "ax[1].set_xlabel(\"(Tot PR - SF [kg/m2 d]) < 0\")\n",
    "# Manually setting xticks to have one for each bar\n",
    "ax[1].set_xticks(np.arange(round(np.nanmin(dif_field)-1), np.nanmax(dif_field)+1, 1))\n",
    "ax[1].set_xlim(-20,1)\n",
    "ax[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "#Why does it look so different now?\n",
    "## Treat negative values as they are and just use percentages derived from total precipitation percentages\n",
    "## Calculate percentage of occurrences\n",
    "## For distribution use 1°C first, go towards 1.5°C - distribute mismatched snowfall amounts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537f24f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count occurrences, where there is more snowfall than precipitation\n",
    "test = np.where(dif_field < 0, dif_field, np.nan)\n",
    "print(np.count_nonzero(~np.isnan(test)))\n",
    "print(np.count_nonzero(~np.isnan(dif_field)))\n",
    "print(\"Percentage of negative values =\", np.count_nonzero(~np.isnan(test)) / np.count_nonzero(~np.isnan(dif_field)))\n",
    "print(\"\\n--------------------\")\n",
    "\n",
    "test = np.where(dif_field < -20, dif_field, np.nan)\n",
    "print(np.count_nonzero(~np.isnan(test)))\n",
    "print(np.count_nonzero(~np.isnan(dif_field)))\n",
    "print(\"Percentage of values < -20 =\", np.count_nonzero(~np.isnan(test)) / np.count_nonzero(~np.isnan(dif_field)))\n",
    "print(\"\\n--------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a58296",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repeat just over HEF cells\n",
    "if 'win' in sys.platform:\n",
    "    hef_crop = xr.open_dataset(\"E:/OneDrive - uibk.ac.at/PhD/PhD/Data/Hintereisferner/Climate/COSMO/COSMO_HEF_crop.nc\")\n",
    "else:\n",
    "    hef_crop = xr.open_dataset(\"/mnt/C4AEBBABAEBB9500/OneDrive - uibk.ac.at/PhD/PhD/Data/Hintereisferner/Climate/COSMO/COSMO_HEF_crop.nc\")\n",
    "print(hef_crop)\n",
    "cropdif = dif_field.sel(rlat=slice(-0.23, -0.17), rlon=slice(0.49, 0.55))\n",
    "test = np.where(cropdif < 0, cropdif, np.nan)\n",
    "print(np.count_nonzero(~np.isnan(test)))\n",
    "print(np.count_nonzero(~np.isnan(cropdif)))\n",
    "print(\"Percentage =\", np.count_nonzero(~np.isnan(test)) / np.count_nonzero(~np.isnan(cropdif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a424c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what happens if we exclude last timestep - basically the same (tested it, not shown)\n",
    "# repeat figure over HEF \n",
    "fig, ax = plt.subplots(2,1, figsize=(16,9))\n",
    "cropdif.plot.hist(bins=np.arange(round(np.nanmin(cropdif)-1), np.nanmax(cropdif)+1, 1), ax=ax[0], edgecolor='black', density=True)\n",
    "ax[0].set_xlabel(\"Tot PR - SF [kg/m2 d]\")\n",
    "#ax[0].set_xlim(-30,50)\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "ax[0].set_title(\"Over 3x3 HEF\")\n",
    "\n",
    "cropdif.where(cropdif < 0).plot.hist(bins=np.arange(round(np.nanmin(cropdif)-1), np.nanmax(cropdif)+1, 1), ax=ax[1], edgecolor='black', density=True)\n",
    "ax[1].set_xlabel(\"(Tot PR - SF [kg/m2 d]) < 0\")\n",
    "# Manually setting xticks to have one for each bar\n",
    "ax[1].set_xticks(np.arange(round(np.nanmin(cropdif)-1), np.nanmax(cropdif)+1, 1))\n",
    "ax[1].set_xlim(-20,1)\n",
    "ax[1].set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec56be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.where(cropdif < 0, cropdif, np.nan)\n",
    "print(np.count_nonzero(~np.isnan(test)))\n",
    "print(np.count_nonzero(~np.isnan(cropdif)))\n",
    "print(\"Percentage of negative values =\", np.count_nonzero(~np.isnan(test)) / np.count_nonzero(~np.isnan(cropdif)))\n",
    "print(\"\\n--------------------\")\n",
    "\n",
    "test = np.where(cropdif < -20, cropdif, np.nan)\n",
    "print(np.count_nonzero(~np.isnan(test)))\n",
    "print(np.count_nonzero(~np.isnan(cropdif)))\n",
    "print(\"Percentage of values < -20 =\", np.count_nonzero(~np.isnan(test)) / np.count_nonzero(~np.isnan(cropdif)))\n",
    "print(\"\\n--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3dba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Hantel et al., parameterisation to derive temperature-dependent function\n",
    "#load temperature field\n",
    "temp = xr.open_dataset(path+\"cosmo_1998-2010_1h_tas.nc\").sel(time=slice(\"1999-01-01\",None))\n",
    "## Shift values by +1 and add new timestamp\n",
    "missing_time = temp.isel(time= [-1]) #sel using list to preserve time dimension\n",
    "#replace time value with next day\n",
    "timestep = np.datetime64(\"2010-01-01T00:00:00\")\n",
    "missing_time[\"time\"] = (\"time\", np.reshape(timestep, (1)))\n",
    "print(missing_time)\n",
    "temp_fix = xr.concat([temp, missing_time], dim=\"time\", data_vars=\"minimal\", coords=\"minimal\")\n",
    "\n",
    "zero_temperature = 273.15 #K\n",
    "center_snow_transfer_function = glacier_temp\n",
    "spread_snow_transfer_function = 1.0 #from Hantel et al.\n",
    "# Compute temperature-dependent snowfall fraction\n",
    "\n",
    "f_snow = 0.5 * (-np.tanh((temp_fix.tas - zero_temperature - center_snow_transfer_function) * spread_snow_transfer_function) + 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b28e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f446034",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_field = np.zeros_like(pr['pr'].data)\n",
    "empty_field.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a601534",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'win' in sys.platform:\n",
    "    empty_field = np.zeros_like(pr['pr'].data)\n",
    "\n",
    "    for t in range(empty_field.shape[0]):\n",
    "        empty_field[t,:,:] = np.divide(pr['pr'][t,:,:], pr_filled['pr'][t,:,:], out=np.zeros_like(pr_filled['pr'][t,:,:]),\n",
    "                            where=pr_filled['pr'][t,:,:]!=0)\n",
    "\n",
    "    pr['pr_perc'] = (('time','rlat','rlon'), empty_field)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9954de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Derive hourly snowfall fields\n",
    "option = 1\n",
    "# Option 1: Use total precipitation, split into snow/rain based on temperature transfer function\n",
    "# Option 2: Distribute daily snowfall amounts based on hourly total precipitation percentages and temperature\n",
    "if option == 1:\n",
    "    print(\"Distributing snowfall based on temperature transfer function\")\n",
    "    #Unit conversion? pr in kg/m2 h - snowfall needs to be in m for COSIPY -> current unit then in SWE but need to transfer it using density_fresh_snow/water_density?\n",
    "    #SNOWFALL = (RRR[t]/1000.0)*(water_density/density_fresh_snow)*(0.5*(-np.tanh(((T2[t]-zero_temperature) - center_snow_transfer_function) * spread_snow_transfer_function) + 1.0))\n",
    "    #RAIN = RRR[t]-SNOWFALL*(density_fresh_snow/water_density) * 1000.0\n",
    "    if 'win' in sys.platform:\n",
    "        sf_hourly_old = pr['pr_perc'] * sf_filled['prsn']\n",
    "    sf_hourly = pr['pr'] * f_snow\n",
    "    hourly_rain = pr['pr'] - sf_hourly\n",
    "\n",
    "    sf_filled['prsn_1h'] = sf_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be7d547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if option == 2:\n",
    "    print(\"Using daily snowfall fields.\")\n",
    "    ## use percentage to redistribute, technically should only do that where more or equal amount\n",
    "    ## more does not make sense if pr says 0 but snowfall says something else .., percentage to daily will be 0 then .. can't change it\n",
    "\n",
    "    #sf_filled['prsn_1h'] = (('time','rlat','rlon'),\n",
    "    #                        np.where(sf_filled['prsn'] >= pr_1h['pr_daily'],\n",
    "    #                                 sf_filled['prsn'] * pr_1h['pr_perc'], sf_filled['prsn']))\n",
    "\n",
    "    ## Can we implement this with a temperature treshold? \n",
    "\n",
    "    # Compute unnormalized hourly snowfall\n",
    "    sf_hourly_old = pr['pr_perc'] * sf_filled['prsn']\n",
    "    sf_hourly_unnormalized = pr['pr_perc'] * sf_filled['prsn'] * f_snow\n",
    "\n",
    "    # Compute daily sums for renormalization\n",
    "    sum_hourly_snow_frac = sf_hourly_unnormalized.groupby(sf_filled.time.dt.date).sum(dim=\"time\")\n",
    "\n",
    "    # Normalize each hourly value to preserve daily snowfall total #sel date=ds.time.dt.date repeats values\n",
    "    sf_hourly = sf_hourly_unnormalized / sum_hourly_snow_frac.sel(date=sf_filled.time.dt.date) * sf_filled.prsn\n",
    "\n",
    "    # Replace NaNs (if any) from division by zero where no snowfall occurs\n",
    "    sf_hourly = sf_hourly.fillna(0)\n",
    "\n",
    "    sf_filled['prsn_1h'] = sf_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f62bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sf_hourly.resample(time=\"1D\").sum()\n",
    "print(np.allclose(sf.prsn, test.data, atol=1e-4))  # Should return True\n",
    "\n",
    "## does not match, bceause total prec. and snowfall dataset also didnt match so pr perc is not exactly correct either?\n",
    "diff = test-sf['prsn']\n",
    "diff.min(dim=['rlat','rlon']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f777b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'win' in sys.platform:\n",
    "    test = sf_hourly_old.resample(time=\"1D\").sum()\n",
    "    print(np.allclose(sf.prsn, test.data, atol=1e-4))  # Should return True\n",
    "\n",
    "    ## does not match, bceause total prec. and snowfall dataset also didnt match so pr perc is not exactly correct either?\n",
    "    diff = test-sf['prsn']\n",
    "    diff.min(dim=['rlat','rlon']).plot()\n",
    "    \n",
    "    print(sf_hourly_old[0:24,20,13].sum())\n",
    "    print(sf_filled['prsn'][0:24,20,13].mean())\n",
    "    print(sf_filled['prsn_1h'][0:24,20,13].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do a quick check at one example\n",
    "print(np.where( (sf_filled['prsn'] >= pr['pr_daily']) & (pr['pr_daily'] > 0) ))\n",
    "print(sf_filled['prsn_1h'][0:24,20,13].values)\n",
    "print(pr['pr'][0:24,20,13].values)\n",
    "print(\"\\n----------------------------\")\n",
    "print(sf_filled['prsn_1h'][0:24,20,13].sum())\n",
    "print(sf_filled['prsn'][0:24,20,13].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb98c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look at sf_filled vals to see if it actually happened as we wanted it\n",
    "test1 = sf_filled['prsn_1h'].isel(time=slice(None,-1)).resample(time=\"1D\").sum()\n",
    "test2 = sf_filled['prsn'].isel(time=slice(None,-1)).resample(time=\"1D\").mean()\n",
    "\n",
    "if 'win' in sys.platform:\n",
    "    test3 = sf_hourly_old.isel(time=slice(None,-1)).resample(time=\"1D\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc647e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'win' in sys.platform:\n",
    "    print(np.allclose(test3, test1, atol=1e-2))  # Should return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c26b4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check why does it not match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd64355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'win' in sys.platform:\n",
    "    #compare to old approach\n",
    "    diff_old_new = np.abs(test1 - test3)\n",
    "    print(np.nanmax(diff_old_new))\n",
    "    diff_old_new.max(dim=['rlat','rlon']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef1c8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'win' in sys.platform:\n",
    "    np.testing.assert_allclose(test1, test3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49725ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6,3), dpi=300)\n",
    "ax.plot(test1.time, (test1-test2).mean(dim=['rlat','rlon']))\n",
    "ax.set_ylabel(\"new_prsn - prsn [kg/m2 d]\")\n",
    "\n",
    "print(np.nancumsum((test1-test2).mean(dim=['rlat','rlon'])))\n",
    "## units is still in [mm] we're okay with that difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c44ab5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'win' in sys.platform:\n",
    "    ## Check RAIN amounts, where negative - need to fix values to 0\n",
    "    rain = pr['pr'] - sf_filled['prsn_1h']\n",
    "    print(rain)\n",
    "\n",
    "    fig, ax = plt.subplots(1,1, figsize=(6,3), dpi=300)\n",
    "    ax.plot(rain.time, rain.mean(dim=['rlat','rlon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7c2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix differences ... (don't do! Results in bad issues)\n",
    "#sf_filled['prsn_1h'] = ('time','rlat','rlon'), np.where(pr['pr'] < sf_filled['prsn_1h'], pr['pr'], sf_filled['prsn_1h'])\n",
    "#instead plot timeseries around HEF\n",
    "fig, ax  = plt.subplots(1,1, figsize=(6,3), dpi=300)\n",
    "ax.plot( (sf_filled.sel(rlat=slice(-0.24, -0.16), rlon=slice(0.46, 0.56))['prsn_1h'].mean(dim=['rlat','rlon']))/1000 )\n",
    "ax.set_ylabel(\"Snowfall [m w.e.]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "88878670",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save files\n",
    "try:\n",
    "    sf_filled[['prsn_1h']].rename({'prsn_1h':'SNOWFALL'}).drop_vars('date').to_netcdf(path+\"cosmo_1999_2010_1h_SNOWFALL.nc\")\n",
    "except:\n",
    "    sf_filled[['prsn_1h']].rename({'prsn_1h':'SNOWFALL'}).to_netcdf(path+\"cosmo_1999_2010_1h_SNOWFALL.nc\")\n",
    "    \n",
    "pr[['pr']].rename({'pr':'RRR'}).to_netcdf(path+\"cosmo_1999_2010_1h_RRR.nc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c70b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'win' in sys.platform:\n",
    "    snowfall_flat = sf_hourly.values.flatten()\n",
    "    snowfall_other = (sf_filled['prsn'] * pr['pr_perc']).values.flatten()\n",
    "\n",
    "    snowfall_flat = np.where(snowfall_flat > 0, snowfall_flat, np.nan)\n",
    "\n",
    "    temp_flat = temp_fix.tas.values.flatten()\n",
    "    temp_flat = np.where(np.isnan(snowfall_flat), np.nan, temp_flat)\n",
    "    snowfall_other = np.where(np.isnan(snowfall_flat), np.nan, snowfall_other)\n",
    "\n",
    "    snowfall_flat = snowfall_flat[~np.isnan(snowfall_flat)]\n",
    "    temp_flat = temp_flat[~np.isnan(temp_flat)]\n",
    "    snowfall_other = snowfall_other[~np.isnan(snowfall_other)]\n",
    "\n",
    "    # Define temperature bins (e.g., every 0.5K)\n",
    "    temp_bins = np.arange(temp_flat.min(), temp_flat.max(), 0.5)\n",
    "    bin_centers = (temp_bins[:-1] + temp_bins[1:]) / 2  # Midpoints for plotting\n",
    "\n",
    "    # Compute mean snowfall per temperature bin\n",
    "    df = pd.DataFrame({\"temperature\": temp_flat, \"snowfall\": snowfall_flat, \"snowfall_other\": snowfall_other})\n",
    "    df[\"temp_bin\"] = pd.cut(df[\"temperature\"], bins=temp_bins, labels=bin_centers)\n",
    "    mean_snowfall_per_bin = df.groupby(\"temp_bin\")[\"snowfall\"].mean()\n",
    "    mean_snowfallother_per_bin = df.groupby(\"temp_bin\")[\"snowfall_other\"].mean()\n",
    "\n",
    "    # Plot binned mean snowfall vs. temperature\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(bin_centers, mean_snowfall_per_bin, marker=\"o\", linestyle=\"-\", color=\"blue\", label=\"Binned Mean Snowfall\")\n",
    "    plt.plot(bin_centers, mean_snowfallother_per_bin, marker=\"o\", linestyle=\"-\", color=\"red\", label=\"Binned Mean Other Snowfall\")\n",
    "\n",
    "    # Overlay transfer function shape for comparison\n",
    "    temp_range = np.linspace(temp_flat.min(), temp_flat.max(), 100)\n",
    "    f_snow_curve = 0.5 * (-np.tanh((temp_range - zero_temperature) * spread_snow_transfer_function) + 1.0)\n",
    "    #f_snow_curve *= mean_snowfall_per_bin.max()  # Scale for comparison\n",
    "\n",
    "    #plt.plot(temp_range, f_snow_curve, color=\"black\", linestyle=\"dashed\", label=\"Transfer Function Shape\")\n",
    "\n",
    "    # Labels & legend\n",
    "    plt.xlabel(\"Temperature (K)\")\n",
    "    plt.ylabel(\"Mean Snowfall (mm)\")\n",
    "    plt.title(\"Binned Mean Snowfall vs. Temperature\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xlim(267,280)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1627873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'win' in sys.platform:\n",
    "    plt.figure()\n",
    "    f_snow_curve = 0.5 * (-np.tanh((temp_range - zero_temperature -center_snow_transfer_function) * spread_snow_transfer_function) + 1.0)\n",
    "    plt.plot(temp_range, f_snow_curve, 'k--', label=\"Transfer Function\")\n",
    "    plt.scatter(temp_fix.isel(rlat=slice(20,30), rlon=slice(19,30)).tas.values.flatten(), f_snow.isel(rlat=slice(20,30), rlon=slice(19,30)).values.flatten(), s=1, alpha=0.5, label=\"Computed f_snow\")\n",
    "    plt.scatter(temp_fix.isel(rlat=slice(20,30), rlon=slice(19,30)).tas.values.flatten(), sf_hourly.isel(rlat=slice(20,30), rlon=slice(19,30)).values.flatten(), s=1, alpha=0.5, label=\"Snowfall\")\n",
    "\n",
    "    plt.xlabel(\"Temperature (K)\")\n",
    "    plt.ylabel(\"Transfer Function Value\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5083e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_snow_transfer_function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
