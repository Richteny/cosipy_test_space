{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patheffects as path_effects\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.stats import norm, spearmanr, ks_2samp, gaussian_kde, truncnorm\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "import pickle\n",
    "\n",
    "# Requires output of the second LHS run with spotpy (narrow)\n",
    "\n",
    "if 'win' in sys.platform:\n",
    "    path = \"E:/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/LHS/\"\n",
    "    filepath = \"E:/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/\"\n",
    "    tsla = pd.read_csv(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/snowlines/HEF-snowlines-1999-2010_manual_filtered.csv\")\n",
    "    figpath = \"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/\"\n",
    "else:\n",
    "    path = \"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/LHS/\"\n",
    "    filepath = \"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/\"\n",
    "    tsla = pd.read_csv(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/snowlines/HEF-snowlines-1999-2010_manual_filtered.csv\")\n",
    "    figpath = \"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path+\"LHS-narrow_1D20m_1999_2010_fullprior.csv\", index_col=0)\n",
    "if np.min(df.index) == 3000:\n",
    "    print(\"Faulty index, reset\")\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create summary of min/max bounds for individual parameters\n",
    "#with df\n",
    "if 'win' in sys.platform:\n",
    "    df_wide = pd.read_csv(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/LHS/LHS-wide_1D20m_1999_2010_fullprior.csv\", index_col=0)\n",
    "else:\n",
    "    df_wide = pd.read_csv(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/LHS/LHS-wide_1D20m_1999_2010_fullprior.csv\", index_col=0)\n",
    "param_names = [\"rrr_factor\", \"alb_ice\", \"alb_snow\", \"alb_firn\", \"albedo_aging\", \"albedo_depth\", \"roughness_ice\"]\n",
    "df_wide_sub = df_wide[param_names]\n",
    "\n",
    "summary_stats = pd.DataFrame({\n",
    "    'min': df_wide_sub.min(),\n",
    "    'max': df_wide_sub.max(),\n",
    "    'mean': df_wide_sub.mean(),\n",
    "    'std': df_wide_sub.std()\n",
    "})\n",
    "\n",
    "print(summary_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_narrow_sub = df[param_names]\n",
    "\n",
    "summary_stats = pd.DataFrame({\n",
    "    'min': df_narrow_sub.min(),\n",
    "    'max': df_narrow_sub.max(),\n",
    "    'mean': df_narrow_sub.mean(),\n",
    "    'std': df_narrow_sub.std()\n",
    "})\n",
    "\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start_dt = pd.to_datetime(\"2000-01-01\") #config starts with spinup - need to add 1year\n",
    "time_end_dt = pd.to_datetime(\"2009-12-31\")\n",
    "\n",
    "tsla_true_obs = tsla.copy()\n",
    "tsla_true_obs['LS_DATE'] = pd.to_datetime(tsla_true_obs['LS_DATE'])\n",
    "print(\"Start date:\", time_start_dt)\n",
    "print(\"End date:\", time_end_dt)\n",
    "tsla_true_obs = tsla_true_obs.loc[(tsla_true_obs['LS_DATE'] > time_start_dt) & (tsla_true_obs['LS_DATE'] <= time_end_dt)]\n",
    "tsla_true_obs.set_index('LS_DATE', inplace=True)\n",
    "#Normalize standard deviation if necessary\n",
    "tsla_true_obs['SC_stdev'] = (tsla_true_obs['SC_stdev']) / (tsla_true_obs['glacier_DEM_max'] - tsla_true_obs['glacier_DEM_min'])\n",
    "\n",
    "thres_unc = (20) / (tsla_true_obs['glacier_DEM_max'].iloc[0] - tsla_true_obs['glacier_DEM_min'].iloc[0])\n",
    "print(thres_unc)\n",
    "\n",
    "## Set observational uncertainty where smaller to atleast model resolution (20m) and where larger keep it\n",
    "sc_norm = np.where(tsla_true_obs['SC_stdev'] < thres_unc, thres_unc, tsla_true_obs['SC_stdev'])\n",
    "tsla_true_obs['SC_stdev'] = sc_norm\n",
    "tsla_true_obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load MB data\n",
    "rgi_id = \"RGI60-11.00897\"\n",
    "if 'win' in sys.platform:\n",
    "    geod_ref = pd.read_csv(\"E:/OneDrive/PhD/PhD/Data/Hugonnet_21_MB/dh_11_rgi60_pergla_rates.csv\")\n",
    "else:\n",
    "    geod_ref = pd.read_csv(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hugonnet_21_MB/dh_11_rgi60_pergla_rates.csv\")\n",
    "geod_ref = geod_ref.loc[geod_ref['rgiid'] == rgi_id]\n",
    "geod_ref = geod_ref.loc[geod_ref['period'] == \"2000-01-01_2010-01-01\"]\n",
    "geod_ref = geod_ref[['dmdtda', 'err_dmdtda']]\n",
    "print(geod_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare loglike calculations - snowline metrics: Maximum TSLA + Std and normal timeseries? Rate of change?\n",
    "tsla_true_obs['year'] = tsla_true_obs.index.year\n",
    "\n",
    "max_tsl_data = tsla_true_obs.loc[tsla_true_obs.groupby('year')[\"TSL_normalized\"].idxmax()]\n",
    "#max_tsl_data['SC_stdev'] = tsla_true_obs['SC_stdev']\n",
    "\n",
    "max_tsl_with_uncertainty = max_tsl_data[[\"TSL_normalized\", 'SC_stdev']]\n",
    "\n",
    "matching_row_numbers = [tsla_true_obs.index.get_loc(date) for date in max_tsl_with_uncertainty.index if date in tsla_true_obs.index]\n",
    "print(\"Matching row numbers:\", matching_row_numbers)\n",
    "\n",
    "max_tsl_with_uncertainty['rownumber'] = matching_row_numbers \n",
    "print(max_tsl_with_uncertainty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modtsls = df.iloc[:,13:13+58].transpose()\n",
    "print(modtsls)\n",
    "\n",
    "max_modtsls = modtsls.iloc[max_tsl_with_uncertainty['rownumber']]\n",
    "print(max_modtsls)\n",
    "\n",
    "mb_mod = df.iloc[:,[12]].transpose()\n",
    "print(mb_mod)\n",
    "\n",
    "def loglike_tsla_func(sim_tsla, eval_tsla, sigma_tsla):\n",
    "    loglike_tsla = -0.5 * np.sum(np.log(2 * np.pi * sigma_tsla**2) + ((eval_tsla-sim_tsla)**2 / sigma_tsla**2))\n",
    "    avg_loglike_tsla = loglike_tsla / len(eval_tsla)\n",
    "    return avg_loglike_tsla\n",
    "\n",
    "\n",
    "#def loglike_tsla_func(sim_tsla, eval_tsla, sigma_tsla, n_tsla):\n",
    "#    loglike_tsla = -0.5 * np.sum(np.log(2 * np.pi * sigma_tsla**2) + ((eval_tsla - sim_tsla)**2 / sigma_tsla**2)) / n_tsla\n",
    "#    return loglike_tsla\n",
    "\n",
    "def loglike_mb_func(sim_mb, eval_mb, sigma_mb):\n",
    "    loglike_mb = -0.5 * (np.log(2 * np.pi * sigma_mb**2) + ( ((eval_mb-sim_mb)**2) / sigma_mb**2))\n",
    "    return loglike_mb\n",
    "\n",
    "def bias_mb_func(sim_mb, eval_mb):\n",
    "    # For a single point, the bias is the simple difference.\n",
    "    bias = sim_mb - eval_mb\n",
    "    return bias\n",
    "\n",
    "def bias_tsla_func(sim_tsla, eval_tsla):\n",
    "    # The logic is identical to the albedo function.\n",
    "    mean_bias = np.mean(sim_tsla - eval_tsla)\n",
    "    return mean_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mb_res = mb_mod.apply(me, obs_vals = geod_mb, obs_sigma=geod_ref['err_dmdtda'].values, axis=0) #\n",
    "mb_logp = mb_mod.transpose().apply(loglike_mb_func, eval_mb = geod_ref['dmdtda'].values, sigma_mb= geod_ref['err_dmdtda'].values, axis=0)\n",
    "print(mb_logp)\n",
    "\n",
    "#tsl_res = modtsls.apply(mae, obs_vals = tsla_synth['Med_TSL'].values, obs_sigma=tsla_synth['Std_TSL'].values, axis=0) #\n",
    "tsl_logp = modtsls.apply(loglike_tsla_func, eval_tsla =tsla_true_obs['TSL_normalized'].values, sigma_tsla= tsla_true_obs['SC_stdev'].values, axis=0)\n",
    "print(tsl_logp)\n",
    "\n",
    "mb_bias = mb_mod.transpose().apply(bias_mb_func, eval_mb = geod_ref['dmdtda'].values, axis=0)\n",
    "tsla_bias = modtsls.apply(bias_tsla_func, eval_tsla =tsla_true_obs['TSL_normalized'].values, axis=0)\n",
    "\n",
    "#tslmax_logp = max_modtsls.apply(loglike_tsla_func, eval_tsla =max_tsl_with_uncertainty['TSL_normalized'].values, sigma_tsla= max_tsl_with_uncertainty['SC_stdev'].values, axis=0)\n",
    "#print(tsl_logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load ALBEDO observations \n",
    "if 'win' in sys.platform:\n",
    "    alb_obs_data = xr.open_dataset(r\"E:\\OneDrive\\PhD\\PhD\\Data\\Hintereisferner\\Climate\\HEF_processed_HRZ-30CC-filter_albedos.nc\")\n",
    "else:\n",
    "    alb_obs_data = xr.open_dataset(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Climate/HEF_processed_HRZ-30CC-filter_albedos.nc\")\n",
    "#has nans where no glacier -> can build glacier-wide mean albedo for additional logp\n",
    "alb_obs_data = alb_obs_data.sortby(\"time\")\n",
    "alb_obs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tsl_logp = modtsls.apply(loglike_tsla_func, eval_tsla =tsla_true_obs['TSL_normalized'].values, sigma_tsla= tsla_true_obs['SC_stdev'].values, axis=0)\n",
    "#print(tsl_logp)\n",
    "sigma_alb = alb_obs_data['sigma_albedo'] #alb_obs_std.albedo / np.sqrt(alb_obs_sample_size.albedo) #Standard Error of the Mean assuming no autocorrelation (which is faulty but just a first order approximation), error super small\n",
    "eval_alb = alb_obs_data['median_albedo']\n",
    "\n",
    "list_logp_alb = []\n",
    "list_sim_alb = []\n",
    "list_bias_alb = []\n",
    "\n",
    "for i,r in df.iterrows():\n",
    "    if i % 300 == 0:\n",
    "        print(f\"Processing file {i}/2500\")\n",
    "    rrr_factor = round(r['rrr_factor'],4)\n",
    "    alb_ice = round(r['alb_ice'],4)\n",
    "    alb_snow = round(r['alb_snow'],4)\n",
    "    alb_firn = round(r['alb_firn'],4)\n",
    "    alb_aging = round(r['albedo_aging'],4)\n",
    "    alb_depth = round(r['albedo_depth'],4)\n",
    "    roughness_ice = round(r['roughness_ice'], 4)\n",
    "\n",
    "    filename = f\"HEF_COSMO_1D20m_1999_2010_HORAYZON_IntpPRES_LHS-narrow_19990101-20091231_RRR-{rrr_factor}_{alb_snow}_{alb_ice}_{alb_firn}_{alb_aging}_{alb_depth}_0.24_{roughness_ice}_4.0_0.0026_num2.nc\"\n",
    "    if 'win' in sys.platform:\n",
    "        sim_alb = xr.open_dataarray(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Output/albedo_files/LHS/alb_only/\"+\\\n",
    "            filename)\n",
    "    else:\n",
    "        sim_alb = xr.open_dataarray(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Output/albedo_files/LHS/alb_only/\"+\\\n",
    "            filename)\n",
    "\n",
    "    \n",
    "    #sort by time \n",
    "    sim_alb = sim_alb.sortby(\"time\")\n",
    "    sim_alb = sim_alb.sel(time=alb_obs_data.time)\n",
    "        \n",
    "    ## compute logp albedo for file\n",
    "    list_sim_alb.append(sim_alb.data)\n",
    "    logp_alb_all = -0.5 * np.sum(np.log(2 * np.pi * sigma_alb**2) + ((sim_alb.data-eval_alb)**2 / sigma_alb**2))\n",
    "    avg_logp_alb = logp_alb_all / len(sim_alb)\n",
    "    list_logp_alb.append(avg_logp_alb.item())\n",
    "    #bias \n",
    "    bias_alb_all = bias_tsla_func(sim_alb.data, eval_alb)\n",
    "    list_bias_alb.append(bias_alb_all.item())\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "list_logp_albrel = []\n",
    "list_logp_albhigh = []\n",
    "list_logp_alblow = []\n",
    "\n",
    "indices = []\n",
    "for i,r in df.iterrows():\n",
    "    if i % 300 == 0:\n",
    "        print(f\"Processing file {i}/3000\")\n",
    "    rrr_factor = round(r['rrr_factor'],4)\n",
    "    alb_ice = round(r['alb_ice'],4)\n",
    "    alb_snow = round(r['alb_snow'],4)\n",
    "    alb_firn = round(r['alb_firn'],4)\n",
    "    alb_aging = round(r['albedo_aging'],4)\n",
    "    alb_depth = round(r['albedo_depth'],4)\n",
    "    if condition == \"fullprior\":\n",
    "        filename = f\"HEF_COSMO_1D20m_1999_2010_HORAYZON_LHS_19990101-20091231_RRR-{rrr_factor}_{alb_snow}_{alb_ice}_{alb_firn}_{alb_aging}_{alb_depth}_0.24_1.7_4.0_0.0026_num2.nc\"\n",
    "        if 'win' in sys.platform:\n",
    "            sim_alb = xr.open_dataset(\"E:/OneDrive - uibk.ac.at/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/LHS/AWScomp/LHS/\"+\\\n",
    "                filename)\n",
    "        else:\n",
    "            sim_alb = xr.open_dataset(\"/mnt/C4AEBBABAEBB9500/OneDrive - uibk.ac.at/PhD/PhD/Data/Hintereisferner/Output/albedo_files/\"+\\\n",
    "                filename)\n",
    "    else:\n",
    "        filename = f\"HEF_COSMO_1D20m_1999_2010_HORAYZON_LHSnoRRR_19990101-20091231_RRR-{rrr_factor}_{alb_snow}_{alb_ice}_{alb_firn}_{alb_aging}_{alb_depth}_0.24_1.7_4.0_0.0026_num2AWSmetrics.csv\"\n",
    "        if 'win' in sys.platform:\n",
    "            sim_alb = xr.open_dataset(\"E:/OneDrive - uibk.ac.at/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/LHS/AWScomp/LHS_lowsnow/\"+\\\n",
    "                filename)\n",
    "        else:\n",
    "            sim_alb = xr.open_dataset(\"/mnt/C4AEBBABAEBB9500/OneDrive - uibk.ac.at/PhD/PhD/Data/Hintereisferner/Output/albedo_files/\"+\\\n",
    "                filename)\n",
    "            \n",
    "    ## Process albedo \n",
    "    #ensure same order\n",
    "    sim_alb = sim_alb.isel(lon=0).sortby(\"HGT\")\n",
    "    \n",
    "    # prepare data\n",
    "    rel, high, low = prepare_logp_alb(sim_alb, alb_obs_data)\n",
    "    list_logp_albrel.append(rel)\n",
    "    list_logp_albhigh.append(high)\n",
    "    list_logp_alblow.append(low)\n",
    "\"\"\"\n",
    "\n",
    "## keep albedo out for now, some studies say glacier-average albedo is a useful predictor - could implement this as a metric as well    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alb_logp = pd.Series(list_logp_alb)\n",
    "alb_bias = pd.Series(list_bias_alb)\n",
    "alb_logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_list = [f\"sim{i+1}\" for i in range(58)]\n",
    "print(sim_list)\n",
    "\n",
    "#get their maximums aligned\n",
    "mb_logp.max()\n",
    "\n",
    "#full_var_list = ['rrr_factor', 'alb_ice', 'alb_snow', 'alb_firn', 'albedo_aging', 'albedo_depth', 'mb'] + sim_list + ['ME_acc', 'ME_abl', 'LE_acc', 'LE_abl', 'H_acc',\n",
    "#                                                                                                                      'H_abl', 'B_acc', 'B_abl', 'TS_acc', 'TS_abl']\n",
    "#df = df[full_var_list]\n",
    "df['mb_logp'] = mb_logp\n",
    "df['tsla_logp'] = tsl_logp  #+ (mb_logp.max().item() - tsl_logp.max().item()) #we scale by /10 -> does that make sense, then most of impact is on MB ... I think we offset by best-performance difference so that they have equal score for best performance?\n",
    "df['alb_logp'] = alb_logp.values\n",
    "df['joint_like'] = df['mb_logp'] + df['tsla_logp'] + df['alb_logp']\n",
    "#\n",
    "df['bias_mb'] = mb_bias\n",
    "df['bias_tsla'] = tsla_bias\n",
    "df['bias_alb'] = alb_bias\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just a quick test - show simulations that perform best for mb_logp in terms of pf\n",
    "print(df.sort_values(by=\"mb_logp\", ascending=False).head(250)['rrr_factor'].median())\n",
    "print(df.sort_values(by=\"mb_logp\", ascending=False).head(250)['albedo_aging'].median())\n",
    "print(df.sort_values(by=\"mb_logp\", ascending=False).head(250)['alb_snow'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check ranges of logp to ensure equal weight..\n",
    "print(\"MB Logp Range:\")\n",
    "print(df.mb_logp.min(), df.mb_logp.max())\n",
    "print(\"TSLA Logp Range:\")\n",
    "print(df.tsla_logp.min(), df.tsla_logp.max())\n",
    "print(\"Alb Logp Range:\")\n",
    "print(df.alb_logp.min(), df.alb_logp.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset = df.loc[df['joint_like'] > df.joint_like.max()-2*df.joint_like.std()]\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "subset = df.copy()\n",
    "## Create correlation histograms after Rounce et al.\n",
    "## Correlation matrix flawed because it comes from a random uniform sampling - should see ery little correlation initially\n",
    "\n",
    "correlation_matrix = subset[['rrr_factor','alb_ice','alb_snow','alb_firn','albedo_aging','albedo_depth','roughness_ice','bias_mb','bias_tsla','bias_alb']].corr(method=\"spearman\")\n",
    "\n",
    "y_label_dict = {'rrr_factor': r'$p_{f}$', 'alb_ice': r'$\\alpha_{ice}$', 'alb_snow': r'$\\alpha_{fs}$','alb_firn': r'$\\alpha_{firn}$', 'albedo_aging': r'$\\alpha_{aging}$',\n",
    "                'albedo_depth': r'$\\alpha_{depth}$','roughness_ice': r'$z0_{ice}$', 'mb_logp': r'$\\mathcal{L}(MB|\\theta)$',\n",
    "                'tsla_logp': r'$\\mathcal{L}(TSLA|\\theta)$', 'alb_logp': r'$\\mathcal{L}(ALB|\\theta)$', 'joint_like': r'$\\mathcal{L}(total|\\theta)$',\n",
    "                'bias_mb': r'$\\Delta$MB', 'bias_tsla': r'$\\Delta$TSLA', 'bias_alb': r'$\\Delta$ALB'}\n",
    "#rename columns and index\n",
    "new_index = [y_label_dict[x] for x in correlation_matrix.index]\n",
    "correlation_matrix.index = new_index\n",
    "correlation_matrix.columns = new_index\n",
    "\n",
    "# Plot the correlation heatmap\n",
    "plt.figure(figsize=(16, 9), dpi=150)\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First step, filter data\n",
    "df_copy = df.copy()\n",
    "\n",
    "def compute_nroy_threshold(model_outputs, observations, sigmas, scale=0, delta_sigma=3, return_mask=False, clip=False):\n",
    "    \"\"\"\n",
    "    Computes a log-likelihood threshold corresponding to a full delta_sigma deviation.\n",
    "    Optionally returns a mask of runs considered Not-Ruled-Out-Yet (NROY).\n",
    "\n",
    "    Parameters:\n",
    "    - model_outputs: np.ndarray, shape (n_runs, n_points)\n",
    "    - observations: np.ndarray, shape (n_points,)\n",
    "    - sigmas: np.ndarray, shape (n_points,) or (n_runs, n_points)\n",
    "    - scale: float, optional log-likelihood adjustment\n",
    "    - delta_sigma: float, e.g. 3 for ±3σ shift\n",
    "    - return_mask: bool, whether to return accepted run indices\n",
    "\n",
    "    Returns:\n",
    "    - threshold: float, LL threshold (average LL)\n",
    "    - best_avg_ll: float, best average LL score\n",
    "    - nroy_mask: np.ndarray, optional boolean array of accepted runs\n",
    "    \"\"\"\n",
    "    model_outputs = np.asarray(model_outputs)\n",
    "    observations = np.asarray(observations)\n",
    "    sigmas = np.asarray(sigmas)\n",
    "\n",
    "    n_runs, n_points = model_outputs.shape\n",
    "\n",
    "    if sigmas.ndim == 1:\n",
    "        sigmas = np.tile(sigmas, (n_runs, 1))  # (n_runs, n_points)\n",
    "\n",
    "    # Compute LL for all runs\n",
    "    ll_all = norm.logpdf(model_outputs, loc=observations, scale=sigmas)\n",
    "    avg_ll_per_run = np.mean(ll_all, axis=1) + scale\n",
    "\n",
    "    # Best run index and score\n",
    "    best_idx = np.argmax(avg_ll_per_run)\n",
    "    best_ll = avg_ll_per_run[best_idx]\n",
    "\n",
    "    # Shift observations by ±delta_sigma * sigma and re-evaluate LL\n",
    "    shifted_high = model_outputs[best_idx] + delta_sigma * sigmas[best_idx]\n",
    "    shifted_low  = model_outputs[best_idx] - delta_sigma * sigmas[best_idx]\n",
    "\n",
    "    # Clip if physical bounds (e.g. snowlines can’t be negative)\n",
    "    #for snowlines we'd need to clip it to physical range (0,1)\n",
    "    if clip:\n",
    "        shifted_high = np.clip(shifted_high, 0, 1)\n",
    "        shifted_low  = np.clip(shifted_low, 0, 1)\n",
    "\n",
    "    ll_shifted_high = norm.logpdf(shifted_high, loc=observations, scale=sigmas[best_idx])\n",
    "    ll_shifted_low  = norm.logpdf(shifted_low,  loc=observations, scale=sigmas[best_idx])\n",
    "\n",
    "    drop_high = best_ll - (np.mean(ll_shifted_high) + scale)\n",
    "    drop_low  = best_ll - (np.mean(ll_shifted_low)  + scale)\n",
    "    drop_avg  = (drop_high + drop_low) / 2.0\n",
    "\n",
    "    # Define threshold\n",
    "    threshold = best_ll - drop_avg\n",
    "\n",
    "    if return_mask:\n",
    "        nroy_mask = avg_ll_per_run >= threshold\n",
    "        return threshold, best_ll, nroy_mask\n",
    "\n",
    "    return drop_high, drop_low, drop_avg, best_idx, best_ll\n",
    "\n",
    "### Beware of this drop_avg ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### repeat analysis but first subsample into relevant 2 sigma space! ### \n",
    "#Take scoring threshold (not as arbitrary as best ten percent) derived from +- 2 sigma variation around best solution -> does it work if there are multiple solutions with similar performance?\n",
    "drop_high, drop_low, drop_avg, best_idx, best_ll = compute_nroy_threshold(\n",
    "    modtsls.transpose(), tsla_true_obs['TSL_normalized'], tsla_true_obs['SC_stdev'], scale=0, delta_sigma=3, return_mask=False, clip=False)\n",
    "\n",
    "print(f\"Best run index: {best_idx}\")\n",
    "print(f\"Best avg log-likelihood: {best_ll:.4f}\")\n",
    "print(f\"Avg. Log-likelihood drop at +3σ: {drop_avg:.4f}\")\n",
    "print(f\"High Log-likelihood drop at +3σ: {drop_high:.4f}\")\n",
    "print(f\"Low Log-likelihood drop at +3σ: {drop_low:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logit, expit\n",
    "epsilon = 1e-6\n",
    "clipped_modtsls = np.clip(modtsls, epsilon, 1 - epsilon)\n",
    "clipped_obs = np.clip(tsla_true_obs['TSL_normalized'], epsilon, 1 - epsilon)\n",
    "\n",
    "# 1. Transform data to logit space\n",
    "logit_modtsls = logit(clipped_modtsls)\n",
    "logit_obs = logit(clipped_obs)\n",
    "\n",
    "# 2. Propagate error to logit space (this is crucial)\n",
    "# Each point in each run has a different scaling factor for its sigma\n",
    "stdev_array = tsla_true_obs['SC_stdev']\n",
    "\n",
    "# Reshape the 1D sigma array into a 2D column vector (n_points, 1)\n",
    "# This allows it to be broadcast correctly across the (n_points, n_runs) array.\n",
    "stdev_reshaped = stdev_array.values.reshape(-1, 1) # or stdev_array[:, np.newaxis]\n",
    "\n",
    "# The division now works element-wise as intended\n",
    "logit_sigmas = stdev_reshaped / (clipped_modtsls * (1 - clipped_modtsls))\n",
    "\n",
    "# 3. Run the analysis in logit space (NO CLIPPING NEEDED)\n",
    "drop_high, drop_low, drop_avg, best_idx, best_ll = compute_nroy_threshold(\n",
    "    logit_modtsls.transpose(),\n",
    "    logit_obs,\n",
    "    logit_sigmas.transpose(), # Ensure sigmas are correctly shaped\n",
    "    delta_sigma=3,\n",
    "    clip=False # IMPORTANT\n",
    ")\n",
    "\n",
    "print(f\"Best run index: {best_idx}\")\n",
    "print(f\"Best avg log-likelihood: {best_ll:.4f}\")\n",
    "print(f\"Avg. Log-likelihood drop at +3σ: {drop_avg:.4f}\")\n",
    "print(f\"High Log-likelihood drop at +3σ: {drop_high:.4f}\")\n",
    "print(f\"Low Log-likelihood drop at +3σ: {drop_low:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_high, drop_low, drop_avg, best_idx, best_ll = compute_nroy_threshold(\n",
    "    mb_mod.transpose(), geod_ref['dmdtda'], geod_ref['err_dmdtda'], scale=0, delta_sigma=3, return_mask=False, clip=False)\n",
    "\n",
    "print(f\"Best run index: {best_idx}\")\n",
    "print(f\"Best avg log-likelihood: {best_ll:.4f}\")\n",
    "print(f\"Avg. Log-likelihood drop at +3σ: {drop_avg:.4f}\")\n",
    "print(f\"High Log-likelihood drop at +3σ: {drop_high:.4f}\")\n",
    "print(f\"Low Log-likelihood drop at +3σ: {drop_low:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_high, drop_low, drop_avg, best_idx, best_ll = compute_nroy_threshold(\n",
    "    np.asarray(list_sim_alb), eval_alb.data, sigma_alb.data, scale=0, delta_sigma=3, return_mask=False, clip=False)\n",
    "\n",
    "print(f\"Best run index: {best_idx}\")\n",
    "print(f\"Best avg log-likelihood: {best_ll:.4f}\")\n",
    "print(f\"Avg. Log-likelihood drop at +3σ: {drop_avg:.4f}\")\n",
    "print(f\"High Log-likelihood drop at +3σ: {drop_high:.4f}\")\n",
    "print(f\"Low Log-likelihood drop at +3σ: {drop_low:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to use thresholds to adjust it for min/max? \n",
    "#normalize threshold score\n",
    "## construct conservative cutoff-threshold - 3sigma\n",
    "mb_cutoff = 0.4243 - 4.5\n",
    "tsla_cutoff = -26.5530 + -4.5\n",
    "alb_cutoff = 1.3205 - 4.5 #4.5 \n",
    "\n",
    "# 2 sigma cutoff\n",
    "#mb_cutoff = 0.4243 - 2.0025\n",
    "#tsla_cutoff = -23.9576 + -3.1491\n",
    "#alb_cutoff = 0.7417 - 2.8157\n",
    "\n",
    "cutoff_thres = mb_cutoff + tsla_cutoff + alb_cutoff\n",
    "    \n",
    "filtered_data = df_copy.loc[df_copy['joint_like'] >= cutoff_thres]\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_logps = True\n",
    "\n",
    "if scale_logps:\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Simulated log-likelihood scores (you already have these from real data)\n",
    "    # =============================================================================\n",
    "\n",
    "    L_snow = filtered_data['tsla_logp'].copy()\n",
    "    L_mass = filtered_data['mb_logp'].copy()\n",
    "    L_albedo = filtered_data['alb_logp'].copy()\n",
    "\n",
    "    # =============================================================================\n",
    "    # Descriptive statistics\n",
    "    # =============================================================================\n",
    "    def describe_scores(scores, label):\n",
    "        stats = {\n",
    "            'min': np.min(scores),\n",
    "            'max': np.max(scores),\n",
    "            'median': np.median(scores),\n",
    "            'mean': np.mean(scores),\n",
    "            'std': np.std(scores),\n",
    "            'range': np.max(scores) - np.min(scores)\n",
    "        }\n",
    "        print(f\"Statistics for {label}:\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\" {key:>6}: {value:8.2f}\")\n",
    "        print()\n",
    "        return stats\n",
    "\n",
    "    print(\"Raw Statistics:\")\n",
    "    stats_snow   = describe_scores(L_snow,  \"Snowline\")\n",
    "    stats_mass   = describe_scores(L_mass,  \"Mass Balance\")\n",
    "    stats_albedo = describe_scores(L_albedo,\"Albedo\")\n",
    "\n",
    "    # =============================================================================\n",
    "    # Median shift\n",
    "    # =============================================================================\n",
    "    L_snow_shifted   = L_snow - stats_snow['mean']\n",
    "    L_mass_shifted   = L_mass - stats_mass['mean']\n",
    "    L_albedo_shifted = L_albedo - stats_albedo['mean']\n",
    "\n",
    "    print(\"After Median Shift:\")\n",
    "    describe_scores(L_snow_shifted, \"Snowline (Shifted)\")\n",
    "    describe_scores(L_mass_shifted, \"Mass Balance (Shifted)\")\n",
    "    describe_scores(L_albedo_shifted, \"Albedo (Shifted)\")\n",
    "\n",
    "    # =============================================================================\n",
    "    # Standardization: shift + scale (std and MAD versions)\n",
    "    # =============================================================================\n",
    "    L_snow_std   = L_snow_shifted / stats_snow['std']\n",
    "    L_mass_std   = L_mass_shifted / stats_mass['std']\n",
    "    L_albedo_std = L_albedo_shifted / stats_albedo['std']\n",
    "\n",
    "    # =============================================================================\n",
    "    # Plotting: raw and standardized\n",
    "    # =============================================================================\n",
    "    plt.figure(figsize=(18, 12))  # Increase height to accommodate the third row\n",
    "\n",
    "    # Row 1 - Original\n",
    "    plt.subplot(3, 3, 1)\n",
    "    sns.histplot(L_snow, bins=40, kde=True, color='skyblue')\n",
    "    plt.title(\"TSLA (Raw)\")\n",
    "\n",
    "    plt.subplot(3, 3, 2)\n",
    "    sns.histplot(L_mass, bins=40, kde=True, color='lightgreen')\n",
    "    plt.title(\"MB (Raw)\")\n",
    "\n",
    "    plt.subplot(3, 3, 3)\n",
    "    sns.histplot(L_albedo, bins=40, kde=True, color='salmon')\n",
    "    plt.title(\"Alb (Raw)\")\n",
    "\n",
    "    plt.subplot(3, 3, 4)\n",
    "    sns.histplot(L_snow_shifted, bins=40, kde=True, color='skyblue')\n",
    "    plt.title(\"TSLA (Shifted)\")\n",
    "\n",
    "    plt.subplot(3, 3, 5)\n",
    "    sns.histplot(L_mass_shifted, bins=40, kde=True, color='lightgreen')\n",
    "    plt.title(\"MB (Shifted)\")\n",
    "\n",
    "    plt.subplot(3, 3, 6)\n",
    "    sns.histplot(L_albedo_shifted, bins=40, kde=True, color='salmon')\n",
    "    plt.title(\"Alb (Shifted)\")\n",
    "    \n",
    "    # Row 3 - Standardized by STD\n",
    "    plt.subplot(3, 3, 7)\n",
    "    sns.histplot(L_snow_std, bins=40, kde=True, color='skyblue')\n",
    "    plt.title(\"TSLA (Standardized+Shifted)\")\n",
    "\n",
    "    plt.subplot(3, 3, 8)\n",
    "    sns.histplot(L_mass_std, bins=40, kde=True, color='lightgreen')\n",
    "    plt.title(\"MB (Standardized+Shifted)\")\n",
    "\n",
    "    plt.subplot(3, 3, 9)\n",
    "    sns.histplot(L_albedo_std, bins=40, kde=True, color='salmon')\n",
    "    plt.title(\"Alb (Standardized+Shifted)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Optional: MAD-standardized plots if you want to compare\n",
    "    # Comment out the above plot and uncomment the block below if you prefer MAD\n",
    "\n",
    "    \n",
    "    filtered_data.loc[:,'tsla_logp'] = L_snow_std\n",
    "    filtered_data.loc[:,'mb_logp'] = L_mass_std\n",
    "    filtered_data.loc[:,'alb_logp'] = L_albedo_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store stats to load in pymc\n",
    "# Bundle them together\n",
    "all_stats = {\n",
    "    \"snow\": stats_snow,\n",
    "    \"mass\": stats_mass,\n",
    "    \"albedo\": stats_albedo,\n",
    "}\n",
    "\n",
    "# Save to pickle\n",
    "if 'win' in sys.platform:\n",
    "    with open(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/LHS/LHS/loglike_stats.pkl\", \"wb\") as f:\n",
    "        pickle.dump(all_stats, f)\n",
    "else:\n",
    "    with open(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/COSIPY/MiscTests/LHS/LHS/loglike_stats.pkl\", \"wb\") as f:\n",
    "        pickle.dump(all_stats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check ranges of logp to ensure equal weight..\n",
    "print(\"MB Logp Range:\")\n",
    "print(filtered_data.mb_logp.min(), filtered_data.mb_logp.max())\n",
    "print(\"TSLA Logp Range:\")\n",
    "print(filtered_data.tsla_logp.min(), filtered_data.tsla_logp.max())\n",
    "print(\"Alb Logp Range:\")\n",
    "print(filtered_data.alb_logp.min(), filtered_data.alb_logp.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns for correlation\n",
    "cols_to_correlate = ['rrr_factor','alb_ice','alb_snow','alb_firn','albedo_aging','albedo_depth','roughness_ice','bias_mb','bias_tsla','bias_alb']\n",
    "data_for_corr = filtered_data[cols_to_correlate]\n",
    "\n",
    "corr_matrix, p_matrix = spearmanr(data_for_corr)\n",
    "correlation_df = pd.DataFrame(corr_matrix, index=data_for_corr.columns, columns=data_for_corr.columns)\n",
    "pval_df = pd.DataFrame(p_matrix, index=data_for_corr.columns, columns=data_for_corr.columns)\n",
    "\n",
    "# --- Significance mask ---\n",
    "alpha_threshold = 0.01\n",
    "significance_mask = (pval_df < alpha_threshold).astype(float)\n",
    "\n",
    "# --- Your plotting script (with minor changes) ---\n",
    "y_label_dict = {'rrr_factor': r'$p_{f}$', 'alb_ice': r'$\\alpha_{ice}$', 'alb_snow': r'$\\alpha_{fs}$','alb_firn': r'$\\alpha_{firn}$', 'albedo_aging': r'$\\alpha_{aging}$',\n",
    "                'albedo_depth': r'$\\alpha_{depth}$','roughness_ice': r'$z0_{ice}$', 'mb_logp': r'$\\mathcal{L}(MB|\\theta)$',\n",
    "                'tsla_logp': r'$\\mathcal{L}(TSLA|\\theta)$', 'alb_logp': r'$\\mathcal{L}(ALB|\\theta)$', 'joint_like': r'$\\mathcal{L}(total|\\theta)$',\n",
    "                'bias_mb': r'$\\Delta{B_{geod}}$', 'bias_tsla': r'$\\Delta$SLA', 'bias_alb': r'$\\Delta\\bar{\\alpha}$'}\n",
    "\n",
    "new_labels = [y_label_dict[x] for x in correlation_df.index]\n",
    "correlation_df.index = new_labels\n",
    "correlation_df.columns = new_labels\n",
    "\n",
    "# --- Plot ---\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 9), dpi=300)\n",
    "#cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "\n",
    "# Base heatmap\n",
    "sns.heatmap(\n",
    "    correlation_df,\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1, vmax=1,\n",
    "    annot=True, fmt=\".2f\",\n",
    "    mask=None,\n",
    "    ax=ax,\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray',\n",
    "    alpha=1  # we'll modulate alpha manually below\n",
    ")\n",
    "\n",
    "# Optional: overlay transparency by reducing color intensity where not significant\n",
    "# This part modulates transparency by re-plotting a blank mask on top:\n",
    "#ax = plt.gca()\n",
    "\n",
    "for i in range(len(correlation_df)):\n",
    "    for j in range(len(correlation_df)):\n",
    "        if pval_df.iloc[i, j] >= alpha_threshold:\n",
    "            ax.add_patch(plt.Rectangle((j, i), 1, 1, color='white', alpha=0.6, zorder=3))\n",
    "\n",
    "# Optional: Outline significant cells (contours)\n",
    "for i in range(len(correlation_df)):\n",
    "    for j in range(len(correlation_df)):\n",
    "        if pval_df.iloc[i, j] < alpha_threshold:\n",
    "            ax.add_patch(plt.Rectangle((j, i), 1, 1, fill=False, edgecolor='black', lw=1.5))\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "if 'win' in sys.platform:\n",
    "    pass\n",
    "else:\n",
    "    pass\n",
    "    #plt.savefig(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/Fig03_correlation_3sigma_filter.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.rcParams.update({'font.size': 20})\n",
    "### Subset first -- repeat correlation plot to show correlation between parameters which warrants PCA to reduce complex 7D space\n",
    "filtered_data.loc[:, 'joint_like'] = filtered_data['tsla_logp'] + filtered_data['mb_logp'] + filtered_data['alb_logp']\n",
    "correlation_matrix = filtered_data[['rrr_factor','alb_ice','alb_snow','alb_firn','albedo_aging','albedo_depth','roughness_ice','joint_like','bias_mb','bias_tsla','bias_alb']].corr(method=\"spearman\")\n",
    "\n",
    "y_label_dict = {'rrr_factor': r'$p_{f}$', 'alb_ice': r'$\\alpha_{ice}$', 'alb_snow': r'$\\alpha_{fs}$','alb_firn': r'$\\alpha_{firn}$', 'albedo_aging': r'$\\alpha_{aging}$',\n",
    "                'albedo_depth': r'$\\alpha_{depth}$','roughness_ice': r'$z0_{ice}$', 'mb_logp': r'$\\mathcal{L}(MB|\\theta)$',\n",
    "                'tsla_logp': r'$\\mathcal{L}(TSLA|\\theta)$', 'alb_logp': r'$\\mathcal{L}(ALB|\\theta)$', 'joint_like': r'$\\mathcal{L}(total|\\theta)$',\n",
    "                'bias_mb': r'$\\Delta$MB', 'bias_tsla': r'$\\Delta$SLA', 'bias_alb': r'$\\Delta\\bar{\\alpha}$'}\n",
    "#rename columns and index\n",
    "new_index = [y_label_dict[x] for x in correlation_matrix.index]\n",
    "correlation_matrix.index = new_index\n",
    "correlation_matrix.columns = new_index\n",
    "\"\"\"\n",
    "# Plot the correlation heatmap6\n",
    "fig, ax = plt.subplots(1,1, figsize=(16,9), dpi=150)\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, ax=ax)\n",
    "fig.tight_layout()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA\n",
    "param_features = ['rrr_factor','alb_ice','alb_snow','alb_firn','albedo_aging','albedo_depth','roughness_ice'] #only parameters!\n",
    "# Standardize the parameters\n",
    "scaler2 = StandardScaler()\n",
    "param_scaled = scaler2.fit_transform(filtered_data[param_features])\n",
    "\n",
    "# Run PCA with all components\n",
    "pca = PCA(n_components=len(param_features))\n",
    "pca_data = pca.fit_transform(param_scaled)\n",
    "\n",
    "# Create a DataFrame with the PCA results\n",
    "pca_df = pd.DataFrame(pca_data, columns=[f'PC{i+1}' for i in range(len(param_features))])\n",
    "\n",
    "explained_variances = pca.explained_variance_ratio_\n",
    "loadings = pca.components_\n",
    "\n",
    "plt.figure(figsize=(16, 9), dpi=150)\n",
    "plt.bar(range(1, len(explained_variances) + 1), explained_variances, alpha=0.7, align='center', label='Individual Explained Variance')\n",
    "plt.step(range(1, len(explained_variances) + 1), explained_variances.cumsum(), where='mid', label='Cumulative Explained Variance')\n",
    "plt.xlabel('Principal Component Index')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming `pca` is your fitted PCA object and `df_params` is your LHS input DataFrame\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    index=filtered_data[param_features].columns,\n",
    "    columns=[f\"PC{i+1}\" for i in range(filtered_data[param_features].shape[1])]\n",
    ")\n",
    "print(loadings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Continued after clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform clustering, overlay cluster color on PCA plots.\n",
    "# Extract log-likelihood columns\n",
    "loglikelihoods = filtered_data[['tsla_logp','alb_logp','mb_logp']] # Last 3 columns: mass balance, albedo, snowlines\n",
    "#loglikelihoods = filtered_data[['joint_like']]\n",
    "#loglikelihoods = filtered_data[['rrr_factor', 'alb_ice', 'alb_snow', 'alb_firn', 'albedo_aging', 'albedo_depth', 'roughness_ice']]\n",
    "#loglikelihoods = filtered_data[['alb_logp']]\n",
    "# Standardize log-likelihood values (important for clustering)\n",
    "scaler = StandardScaler()\n",
    "loglikelihoods_scaled = scaler.fit_transform(loglikelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "K_range = range(1, 10)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=3)\n",
    "    kmeans.fit(loglikelihoods_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.plot(K_range, wcss, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "bic_scores = []\n",
    "for k in range(1, 10):\n",
    "    gmm = GaussianMixture(n_components=k, random_state=42)\n",
    "    gmm.fit(loglikelihoods_scaled)\n",
    "    bic_scores.append(gmm.bic(loglikelihoods_scaled))\n",
    "\n",
    "# Plot BIC scores\n",
    "plt.plot(range(1, 10), bic_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('BIC Score')\n",
    "plt.title('BIC for GMM')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k = 4  # Choose based on the elbow method\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "\n",
    "clusters = kmeans.fit_predict(loglikelihoods_scaled)\n",
    "\n",
    "# Add the cluster labels to your dataframe\n",
    "try:\n",
    "    filtered_data.drop('KMeans_Cluster', axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "filtered_data.loc[:,'KMeans_Cluster'] = clusters\n",
    "\n",
    "\"\"\"\n",
    "optimal_gmm_k = 3 # Choose based on BIC\n",
    "gmm = GaussianMixture(n_components=optimal_gmm_k, random_state=42)\n",
    "subset[\"GMM_Cluster\"] = gmm.fit_predict(loglikelihoods_scaled)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,5, dpi=150, figsize=(16,9))\n",
    "list_axis = [ax[0,0], ax[0,1], ax[0,2], ax[0,3], ax[0,4], ax[1,0], ax[1,1], ax[1,2], ax[1,3], ax[1,4]]\n",
    "i = 0\n",
    "for col in ['rrr_factor','alb_ice','alb_snow','alb_firn','albedo_aging','albedo_depth','roughness_ice','mb_logp', 'tsla_logp', 'alb_logp']:  # Exclude log-likelihoods\n",
    "    sns.boxplot(x=filtered_data[\"KMeans_Cluster\"], y=filtered_data[col], ax=list_axis[i])\n",
    "    i+=1\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_silhouette = silhouette_score(loglikelihoods_scaled, filtered_data[\"KMeans_Cluster\"])\n",
    "print(f\"GMM Silhouette Score: {gmm_silhouette:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repeat based on joint_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loglikelihoods = filtered_data[['tsla_logp','alb_logp','mb_logp']] # Last 3 columns: mass balance, albedo, snowlines\n",
    "loglikelihoods = filtered_data[['joint_like']]\n",
    "# Standardize log-likelihood values (important for clustering)\n",
    "scaler3 = StandardScaler()\n",
    "loglikelihoods_scaled = scaler3.fit_transform(loglikelihoods)\n",
    "\n",
    "wcss = []\n",
    "K_range = range(1, 10)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=3)\n",
    "    kmeans.fit(loglikelihoods_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.plot(K_range, wcss, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "bic_scores = []\n",
    "for k in range(1, 10):\n",
    "    gmm = GaussianMixture(n_components=k, random_state=42)\n",
    "    gmm.fit(loglikelihoods_scaled)\n",
    "    bic_scores.append(gmm.bic(loglikelihoods_scaled))\n",
    "\n",
    "# Plot BIC scores\n",
    "plt.plot(range(1, 10), bic_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('BIC Score')\n",
    "plt.title('BIC for GMM')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k = 4  # Choose based on the elbow method\n",
    "kmeans2 = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "\n",
    "clusters2 = kmeans2.fit_predict(loglikelihoods_scaled)\n",
    "\n",
    "# Add the cluster labels to your dataframe\n",
    "try:\n",
    "    filtered_data.drop('KMeans_Cluster2', axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "filtered_data.loc[:,'KMeans_Cluster2'] = clusters2\n",
    "\n",
    "\"\"\"\n",
    "optimal_gmm_k = 3 # Choose based on BIC\n",
    "gmm = GaussianMixture(n_components=optimal_gmm_k, random_state=42)\n",
    "subset[\"GMM_Cluster\"] = gmm.fit_predict(loglikelihoods_scaled)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to cluster on\n",
    "score_columns = ['mb_logp', 'tsla_logp', 'alb_logp']\n",
    "n_clusters = 4  # Number of clusters\n",
    "\n",
    "# Add cluster info to DataFrame (for plotting only)\n",
    "temp_df = filtered_data.copy()\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 9), dpi=150, sharey=False)\n",
    "\n",
    "for i, col in enumerate(score_columns):\n",
    "    X = filtered_data[[col]].values\n",
    "\n",
    "    # Run KMeans on single column\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "    temp_df[f'KMeans_{col}'] = cluster_labels\n",
    "\n",
    "    # Plot score distributions colored by cluster\n",
    "    sns.stripplot(\n",
    "        x=temp_df[f'KMeans_{col}'],\n",
    "        y=temp_df[col],\n",
    "        palette='Set1',\n",
    "        ax=axes[i],\n",
    "        alpha=0.6,\n",
    "        jitter=0.1,\n",
    "        size=4\n",
    "    )\n",
    "\n",
    "    # Plot cluster centers\n",
    "    centers = kmeans.cluster_centers_.flatten()\n",
    "    for j, center in enumerate(centers):\n",
    "        axes[i].axhline(center, color='black', linestyle='--', linewidth=1)\n",
    "    \n",
    "    axes[i].set_title(f\"KMeans on {col}\")\n",
    "    axes[i].set_xlabel(\"Cluster\")\n",
    "    axes[i].set_ylabel(\"Log-likelihood\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'win' in sys.platform:\n",
    "    dic_best_scores = {'mb_logp': 1, 'tsla_logp': 2, 'alb_logp': 2}\n",
    "else:\n",
    "    dic_best_scores = {'mb_logp': 1, 'tsla_logp': 3, 'alb_logp': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'win' in sys.platform:\n",
    "    print(len(filtered_data.loc[filtered_data['KMeans_Cluster2'] == 2]))\n",
    "else:\n",
    "    print(len(filtered_data.loc[filtered_data['KMeans_Cluster2'] == 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 22})\n",
    "cols = ['rrr_factor','alb_ice','alb_snow', 'alb_firn', 'albedo_aging','albedo_depth', 'roughness_ice', 'mb_logp',\n",
    "        'tsla_logp','alb_logp','joint_like']\n",
    "colors = {\"mb_logp\": \"#D81B1B\", \"alb_logp\": \"#1E80E5\", \"tsla_logp\": \"#A5781B\"}\n",
    "best_keys = {\"mb_logp\": \"KMeans_mb_logp\", \"alb_logp\": \"KMeans_alb_logp\", \"tsla_logp\": \"KMeans_tsla_logp\"}\n",
    "y_label_dict = {'rrr_factor': r'$p_{f}$', 'alb_ice': r'$\\alpha_{ice}$', 'alb_snow': r'$\\alpha_{fs}$','alb_firn': r'$\\alpha_{firn}$', 'albedo_aging': r'$\\alpha_{aging}$',\n",
    "                'albedo_depth': r'$\\alpha_{depth}$','roughness_ice': r'$z0_{ice}$', 'mb_logp': r'$\\mathcal{L}(B_{geod}|\\theta,{X})$',\n",
    "                'tsla_logp': r'$\\mathcal{L}(SLA|\\theta,{X})$', 'alb_logp': r'$\\mathcal{L}(\\bar{\\alpha}|\\theta,{X})$', 'joint_like': r'$\\mathcal{L}(total|\\theta,{X})$'}\n",
    "\n",
    "fig, ax = plt.subplots(2, 6, dpi=300, figsize=(20, 12), sharex=False)\n",
    "list_axis = [ax[0, 0], ax[0, 1], ax[0, 2], ax[0, 3], ax[0, 4], ax[1, 0], ax[1, 1], ax[1, 2], ax[1, 3], ax[1, 4], ax[1, 5]]\n",
    "plt.subplots_adjust(wspace=0.9, hspace=0.22)\n",
    "\n",
    "## KS-test not super useful here, just set alpha level to extremely high value so we keep it all clean and don't have to bother with KS-test\n",
    "alpha_level = 0.99 #0.05\n",
    "nonsignificant_alpha = 0.3\n",
    "main_color = '#066555'\n",
    "\n",
    "ks_results_list = []\n",
    "boxplot_results_list = []\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    axis = list_axis[i]\n",
    "    axis.grid(False)\n",
    "    if col not in filtered_data.columns:\n",
    "        axis.axis('off')\n",
    "        continue\n",
    "\n",
    "    cluster_labels = sorted(filtered_data['KMeans_Cluster2'].unique())\n",
    "    data_by_cluster = [filtered_data.loc[filtered_data['KMeans_Cluster2'] == c, col] for c in cluster_labels]\n",
    "    \n",
    "    ### MODIFICATION HERE: Define the total \"raw\" data for the parameter\n",
    "    raw_data_for_param = filtered_data[col]\n",
    "    \n",
    "    p_values = []\n",
    "    for c in cluster_labels:\n",
    "        cluster_data = filtered_data.loc[filtered_data['KMeans_Cluster2'] == c, col]\n",
    "        \n",
    "        ks_stat, p_val = -1, 1.0\n",
    "        \n",
    "        # Check for sufficient data\n",
    "        if len(cluster_data) > 1:\n",
    "            ### MODIFICATION HERE: The KS-test is now between the cluster and the total raw data\n",
    "            ks_stat, p_val = ks_2samp(cluster_data, raw_data_for_param)\n",
    "            \n",
    "            median = np.median(cluster_data)\n",
    "            q1, q3 = np.percentile(cluster_data, [25, 75])\n",
    "            iqr = q3 - q1\n",
    "            # Determine whisker positions\n",
    "            whisker_low_limit = q1 - 1.5 * iqr\n",
    "            whisker_high_limit = q3 + 1.5 * iqr\n",
    "            data_within_whiskers = cluster_data[(cluster_data >= whisker_low_limit) & (cluster_data <= whisker_high_limit)]\n",
    "            whisker_low = data_within_whiskers.min()\n",
    "            whisker_high = data_within_whiskers.max()\n",
    "            fliers = cluster_data[(cluster_data < whisker_low_limit) | (cluster_data > whisker_high_limit)].values\n",
    "            \n",
    "            boxplot_results_list.append({\n",
    "                    'Parameter': col,\n",
    "                    'Cluster': c,\n",
    "                    'median': median,\n",
    "                    'q1': q1,\n",
    "                    'q3': q3,\n",
    "                    'whisker_low': whisker_low,\n",
    "                    'whisker_high': whisker_high,\n",
    "                    'p_value': p_val,\n",
    "                    'fliers': fliers\n",
    "                    })\n",
    "\n",
    "        p_values.append(p_val)\n",
    "\n",
    "        ks_results_list.append({\n",
    "            'Parameter': col,\n",
    "            'Cluster': c,\n",
    "            'Compared_Against': 'Total_Raw_Data', # Added for clarity\n",
    "            'KS_Statistic': ks_stat,\n",
    "            'P_Value': p_val,\n",
    "            'Is_Significant': p_val < alpha_level\n",
    "        })\n",
    "\n",
    "    # Drawing the boxplots (logic remains the same)\n",
    "    bp = axis.boxplot(data_by_cluster, patch_artist=True, widths=0.6, vert=True,\n",
    "                      boxprops=dict(facecolor=main_color, edgecolor='black'),\n",
    "                      medianprops=dict(color='black'),\n",
    "                      whiskerprops=dict(color='black'),\n",
    "                      capprops=dict(color='black'),\n",
    "                      flierprops=dict(marker='o', color='black', alpha=0.5))\n",
    "    \n",
    "    axis.axhline(filtered_data[col].median(), color=\"black\", zorder=-1)\n",
    "    axis.set_title(y_label_dict.get(col, col))\n",
    "    \n",
    "    # Applying transparency (logic remains the same)\n",
    "    for j, box in enumerate(bp['boxes']):\n",
    "        if p_values[j] >= alpha_level:\n",
    "            box.set_alpha(nonsignificant_alpha)\n",
    "            # ... (rest of the alpha setting logic)\n",
    "            bp['medians'][j].set_alpha(nonsignificant_alpha)\n",
    "            bp['whiskers'][2*j].set_alpha(nonsignificant_alpha)\n",
    "            bp['whiskers'][2*j+1].set_alpha(nonsignificant_alpha)\n",
    "            bp['caps'][2*j].set_alpha(nonsignificant_alpha)\n",
    "            bp['caps'][2*j+1].set_alpha(nonsignificant_alpha)\n",
    "            if j < len(bp['fliers']):\n",
    "                 bp['fliers'][j].set_alpha(nonsignificant_alpha * 0.8)\n",
    "\n",
    "    ax_inset = inset_axes(list_axis[i], width=\"50%\", height=\"100%\", loc='upper left',\n",
    "                        bbox_to_anchor=(1, 0, 0.6, 1), bbox_transform=list_axis[i].transAxes, borderpad=0)\n",
    "\n",
    "    # Loop through each score type (MB, ALB, TSLA) to plot on the same inset\n",
    "    for score_key, color in colors.items():\n",
    "        # --- Data Selection (from your script) ---\n",
    "        if score_key not in temp_df.columns: continue\n",
    "        cluster_key = best_keys[score_key]\n",
    "        if cluster_key not in temp_df.columns: continue\n",
    "        best_cluster = dic_best_scores.get(score_key)\n",
    "        if best_cluster is None: continue\n",
    "        \n",
    "        # Select the specific data for this KDE plot\n",
    "        data = temp_df.loc[temp_df[cluster_key] == best_cluster, col]\n",
    "        \n",
    "        # Skip if there's no data to plot\n",
    "        if data.empty or data.isna().all():\n",
    "            continue\n",
    "\n",
    "        # --- Plot the KDE (from your script) ---\n",
    "        sns.kdeplot(y=data, ax=ax_inset, fill=True, cut=0, color=color, alpha=0.3, linewidth=1.0)\n",
    "        \n",
    "        # --- ADDITION: Calculate and plot percentile lines ---\n",
    "        # 1. Check if there's enough data for percentile calculation\n",
    "        if len(data) > 1:\n",
    "            # 2. Calculate the 25th, 50th, and 75th percentiles\n",
    "            q25, q50, q75 = np.percentile(data, [25, 50, 75])\n",
    "            \n",
    "            # 3. Create a KDE function to find the density (x-value) at each percentile\n",
    "            kde_func = gaussian_kde(data)\n",
    "            \n",
    "            # 4. Define the percentiles, their styles, and get their density values\n",
    "            percentiles = [q25, q50, q75]\n",
    "            styles = ['--', '-', '--']\n",
    "            kde_values = kde_func(percentiles)\n",
    "\n",
    "            # 5. Plot a horizontal line for each percentile\n",
    "            # The line goes from x=0 to x=kde_value at the height (y) of the percentile\n",
    "            for q, kde_v, style in zip(percentiles, kde_values, styles):\n",
    "                ax_inset.plot([0, kde_v], [q, q], linestyle=style, color=color, linewidth=1.2)\n",
    "  \n",
    "        \n",
    "    ylim1, ylim2 = ax_inset.get_ylim()\n",
    "    if i != 9:\n",
    "        list_axis[i].set_ylim(ylim1, ylim2)\n",
    "    else:\n",
    "        list_axis[i].set_ylim(ylim1-4, ylim2)\n",
    "        ax_inset.set_ylim(ylim1-4, ylim2)\n",
    "    ax_inset.grid(False)\n",
    "    ax_inset.axis(\"off\")\n",
    "    # ... (end of inset plotting code) ...\n",
    "\n",
    "# --- Axis formatting (remains unchanged) ---\n",
    "ax[0, 1].set_yticks(np.arange(0.12, 0.24 + 0.01, 0.03))\n",
    "for a in ax[0]:\n",
    "    a.set_xticklabels([])\n",
    "    a.set_xlabel(\"\")\n",
    "    \n",
    "for a in ax[1]:\n",
    "    a.set_xlabel(\"Cluster\")\n",
    "\n",
    "\"\"\"\n",
    "### MODIFICATION: Create a custom legend for the significance colors\n",
    "legend_patches = [\n",
    "    Patch(facecolor=significant_color, edgecolor='black', label=f'Significant (p < {alpha})'),\n",
    "    Patch(facecolor=nonsignificant_color, edgecolor='black', linestyle='--', label=f'Not Significant (p ≥ {alpha})')\n",
    "]\n",
    "# Place legend in the empty subplot space (ax[1,5] is used for the explainer, so find another spot)\n",
    "ax[0, 5].legend(handles=legend_patches, loc='center', fontsize=16)\n",
    "ax[0, 5].axis('off') # Turn off the axis for the legend plot\n",
    "\"\"\"\n",
    "# Dummy data\n",
    "np.random.seed(42)\n",
    "dummy_data = np.random.normal(loc=10, scale=5, size=100)\n",
    "ax_explainer = ax[0, 5]\n",
    "\n",
    "# Plot a simple vertical boxplot\n",
    "bp = ax_explainer.boxplot(dummy_data, vert=True, patch_artist=True, widths=0.6,\n",
    "               boxprops=dict(facecolor='#066555', color='black'),\n",
    "               medianprops=dict(color='black'),\n",
    "               whiskerprops=dict(color='black'),\n",
    "               capprops=dict(color='black'),\n",
    "               flierprops=dict(marker='o', color='black', alpha=0.3))\n",
    "\n",
    "# Clean up axis for better visuals\n",
    "ax_explainer.set_xticks([])\n",
    "ax_explainer.set_yticks([])\n",
    "\n",
    "# Annotation positions\n",
    "Q1 = np.percentile(dummy_data, 25)\n",
    "Q2 = np.median(dummy_data)\n",
    "Q3 = np.percentile(dummy_data, 75)\n",
    "whisker_low = np.min(dummy_data[dummy_data > Q1 - 1.5 * (Q3 - Q1)])\n",
    "whisker_high = np.max(dummy_data[dummy_data < Q3 + 1.5 * (Q3 - Q1)])\n",
    "outlier = max(dummy_data)\n",
    "ax_explainer.axhline(y=Q3+2.0, color=\"black\")\n",
    "# Annotate parts\n",
    "ax_explainer.annotate(\"LHS Median\", xy=(1, Q3+2.0), xytext=(0.71, Q3+2.5),\n",
    "                      fontsize=14, va='center',\n",
    "                      bbox=dict(facecolor='None', edgecolor='None', boxstyle='round,pad=0.01'))\n",
    "ax_explainer.annotate(\"Q1\", xy=(1, Q1), xytext=(0.75, Q1-0.68),\n",
    "                      #arrowprops=dict(arrowstyle=\"->\"),\n",
    "                      fontsize=14, va='center',\n",
    "                      bbox=dict(facecolor='white', edgecolor='white', boxstyle='round,pad=0.01'))\n",
    "ax_explainer.annotate(\"Median\", xy=(1, Q2), xytext=(0.75, Q2),\n",
    "                      #arrowprops=dict(arrowstyle=\"->\"),\n",
    "                      fontsize=14, va='center',\n",
    "                      bbox=dict(facecolor='#066555', edgecolor='#066555', boxstyle='round,pad=0.01'))\n",
    "ax_explainer.annotate(\"Q3\", xy=(1, Q3), xytext=(0.75, Q3+0.68),\n",
    "                      #arrowprops=dict(arrowstyle=\"->\"),\n",
    "                      fontsize=14, va='center',\n",
    "                      bbox=dict(facecolor='white', edgecolor='white', boxstyle='round,pad=0.01'))\n",
    "ax_explainer.annotate(\"Q1 - 1.5xIQR\", xy=(1, whisker_low), xytext=(0.55, whisker_low - 0.9),\n",
    "                      #arrowprops=dict(arrowstyle=\"->\"),\n",
    "                      fontsize=14, va='center',\n",
    "                      bbox=dict(facecolor='white', edgecolor='white', boxstyle='round,pad=0.01'))\n",
    "\n",
    "ax_explainer.annotate(\"Q3 + 1.5xIQR\", xy=(1, whisker_high), xytext=(0.55, whisker_high + 0.75),\n",
    "                      #arrowprops=dict(arrowstyle=\"->\"),\n",
    "                      fontsize=14, va='center',\n",
    "                      bbox=dict(facecolor='white', edgecolor='white', boxstyle='round,pad=0.01'))\n",
    "ax_explainer.set_ylim(-4,22)\n",
    "\"\"\"\n",
    "# Optional: annotate outlier\n",
    "\n",
    "if outlier > whisker_high + 0.2:  # only if clearly visible\n",
    "    ax_explainer.annotate(\"Outlier\", xy=(1, outlier), xytext=(1.2, outlier),\n",
    "                          #arrowprops=dict(arrowstyle=\"->\"),\n",
    "                          fontsize=14, va='center')\n",
    "\"\"\"\n",
    "# Create small top inset for each metric's best cluster\n",
    "ax_inset = inset_axes(ax_explainer, width=\"60%\", height=\"100%\", loc='upper left',\n",
    "                        bbox_to_anchor=(1, 0, 0.6, 1), bbox_transform=ax_explainer.transAxes, borderpad=0) #bbox starts at first two values, + next two values\n",
    "# Dummy data for KDEs (three distributions)\n",
    "dummy_mb = np.random.normal(loc=12, scale=2.5, size=100)\n",
    "dummy_tsla = np.random.normal(loc=22, scale=2.5, size=100)\n",
    "dummy_alb = np.random.normal(loc=2, scale=2.5, size=100)\n",
    "\n",
    "# Plot vertical KDEs\n",
    "#sns.kdeplot(y=dummy_mb, ax=ax_inset, label=\"MB\", fill=True, color=\"#D81B1B\", linewidth=1.1)\n",
    "#sns.kdeplot(y=dummy_tsla, ax=ax_inset, label=\"TSLA\", fill=True, color=\"#A5781B\", linewidth=1.1)\n",
    "#\n",
    "# sns.kdeplot(y=dummy_alb, ax=ax_inset, label=\"ALB\", fill=True, color=\"#1E80E5\", linewidth=1.1)\n",
    "\n",
    "dummy_dic = {\"MB\": dummy_mb, \"TSLA\": dummy_tsla, \"ALB\": dummy_alb}\n",
    "color_dic = {\"MB\": \"#D81B1B\", \"ALB\": \"#1E80E5\", \"TSLA\": \"#A5781B\"}\n",
    "# 3. Compute percentiles\n",
    "for score_key, color in color_dic.items():\n",
    "    q25 = np.percentile(dummy_dic[score_key], 25)\n",
    "    q50 = np.median(dummy_dic[score_key])\n",
    "    q75 = np.percentile(dummy_dic[score_key], 75)\n",
    "    # Plot KDE vertically\n",
    "    kde_line = sns.kdeplot(y=dummy_dic[score_key], ax=ax_inset, fill=True, color=color, label=score_key, linewidth=1.1)\n",
    "\n",
    "    kde_func = gaussian_kde(dummy_dic[score_key])\n",
    "    kde_vals = kde_func(q25), kde_func(q50), kde_func(q75)\n",
    "\n",
    "    # 5. Plot horizontal lines at each percentile\n",
    "    for q, kde_v, style in zip([q25, q50, q75], kde_vals, ['--', '-', '--']):\n",
    "        ax_inset.plot([0, kde_v[0]], [q, q], linestyle=style, color=color, linewidth=1.0)\n",
    "\n",
    "ax_inset.text(0.01, 12, r\"$B_{geod}$\", color=\"#D81B1B\", fontsize=24, fontweight='bold', va='center')\n",
    "ax_inset.text(0.01, 22, r\"$SLA$\", color=\"#A5781B\", fontsize=24, fontweight='bold', va='center')\n",
    "ax_inset.text(0.01, 2, r\"$\\bar{\\alpha}$\", color=\"#1E80E5\", fontsize=24, fontweight='bold', va='center')\n",
    "\n",
    "## add labels\n",
    "fig.text(0.09, 0.9, 'a)', transform=fig.transFigure, fontsize=24)\n",
    "fig.text(0.79, 0.9, 'f)', transform=fig.transFigure, fontsize=24)\n",
    "\n",
    "# Automatically space b–e evenly between 0.1 and 0.8\n",
    "x_positions = np.linspace(0.09, 0.79, 6)  # gives positions for a–f\n",
    "labels = ['b)', 'c)', 'd)', 'e)']\n",
    "\n",
    "for x, label in zip(x_positions[1:-1], labels):  # skip first (a) and last (f)\n",
    "    fig.text(x, 0.9, label, transform=fig.transFigure, fontsize=24)\n",
    "#\n",
    "fig.text(0.09, 0.48, 'g)', transform=fig.transFigure, fontsize=24)\n",
    "fig.text(0.79, 0.48, 'l)', transform=fig.transFigure, fontsize=24)\n",
    "\n",
    "# Automatically space b–e evenly between 0.1 and 0.8\n",
    "x_positions = np.linspace(0.09, 0.79, 6)  # gives positions for a–f\n",
    "labels = ['h)', 'i)', 'j)', 'k)']\n",
    "\n",
    "for x, label in zip(x_positions[1:-1], labels):  # skip first (a) and last (f)\n",
    "    fig.text(x, 0.48, label, transform=fig.transFigure, fontsize=24)\n",
    "\n",
    "ax_inset.axis('off')  # Clean look\n",
    "if 'win' in sys.platform:\n",
    "    plt.savefig(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/Fig04_boxplot_distributions_clusters.pdf\", bbox_inches=\"tight\")\n",
    "else:\n",
    "    plt.savefig(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/Fig04_boxplot_distributions_clusters.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_results_df = pd.DataFrame(boxplot_results_list)\n",
    "bp_results_df = bp_results_df.sort_values(by=['Parameter', 'Cluster'])\n",
    "bp_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_results_df = pd.DataFrame(ks_results_list)\n",
    "ks_results_df = ks_results_df.sort_values(by=['Parameter', 'Cluster'])\n",
    "ks_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to plot\n",
    "cols = ['rrr_factor','alb_ice','alb_snow', 'alb_firn', 'albedo_aging','albedo_depth', 'roughness_ice', 'mb_logp',\n",
    "        'tsla_logp','alb_logp','joint_like']\n",
    "\n",
    "colors = {\"mb_logp\": \"#D81B1B\", \"alb_logp\": \"#1E80E5\", \"tsla_logp\": \"#A5781B\"}\n",
    "best_keys = {\"mb_logp\": \"KMeans_mb_logp\", \"alb_logp\": \"KMeans_alb_logp\", \"tsla_logp\": \"KMeans_tsla_logp\"}\n",
    "\n",
    "# Create boxplots manually\n",
    "fig, ax = plt.subplots(2,6, dpi=300, figsize=(20,12), sharex=False)\n",
    "list_axis = [ax[0,0], ax[0,1], ax[0,2], ax[0,3], ax[0,4], ax[1,0], ax[1,1], ax[1,2], ax[1,3], ax[1,4], ax[1,5]]\n",
    "plt.subplots_adjust(wspace=.9, hspace=0.2)\n",
    "for i, col in enumerate(cols):\n",
    "    axis = list_axis[i]\n",
    "    data_by_cluster = [filtered_data.loc[filtered_data['KMeans_Cluster2'] == c, col] for c in sorted(filtered_data['KMeans_Cluster2'].unique())]\n",
    "    \n",
    "    bp = axis.boxplot(data_by_cluster, patch_artist=True, widths=0.6, vert=True,\n",
    "                   boxprops=dict(\n",
    "                       facecolor='#066555',\n",
    "                       color='black',\n",
    "                       linestyle='--'  # Dashed box outline\n",
    "                       ),\n",
    "               medianprops=dict(color='black'),\n",
    "               whiskerprops=dict(color='black'),\n",
    "               capprops=dict(color='black'),\n",
    "               flierprops=dict(marker='o', color='black', alpha=0.3))\n",
    "    \n",
    "    axis.set_title(y_label_dict[col])\n",
    "    \n",
    "    \n",
    "    # Create small top inset for each metric's best cluster\n",
    "    ax_inset = inset_axes(list_axis[i], width=\"50%\", height=\"100%\", loc='upper left',\n",
    "                          bbox_to_anchor=(1, 0, 0.6, 1), bbox_transform=list_axis[i].transAxes, borderpad=0) #bbox starts at first two values, + next two values\n",
    "    #sns.kdeplot(y=temp_df.loc[temp_df['KMeans_mb_logp'] == dic_best_scores['mb_logp'], col], fill=True, ax=ax_inset, color=\"#D81B1B\", alpha=0.3, label=\"test\", linewidth=1.)\n",
    "    #sns.kdeplot(y=temp_df.loc[temp_df['KMeans_alb_logp'] == dic_best_scores['alb_logp'], col], fill=True, ax=ax_inset, color=\"#1E80E5\", alpha=0.3, label=\"test\", linewidth=1.)\n",
    "    #sns.kdeplot(y=temp_df.loc[temp_df['KMeans_tsla_logp'] == dic_best_scores['tsla_logp'], col], fill=True, ax=ax_inset, color=\"#A5781B\", alpha=0.3, label=\"test\", linewidth=1.)\n",
    "    \n",
    "    for score_key, color in colors.items():\n",
    "            cluster_key = best_keys[score_key]\n",
    "            data = temp_df.loc[temp_df[cluster_key] == dic_best_scores[score_key], col]\n",
    "\n",
    "            # 3. Compute percentiles\n",
    "            q25 = np.percentile(data, 25)\n",
    "            q50 = np.median(data)\n",
    "            q75 = np.percentile(data, 75)\n",
    "            # Plot KDE vertically\n",
    "            kde_line = sns.kdeplot(y=data, ax=ax_inset, fill=True, cut=0, color=color, alpha=0.3, linewidth=1.)\n",
    "\n",
    "            kde_func = gaussian_kde(data)\n",
    "            kde_vals = kde_func(q25), kde_func(q50), kde_func(q75)\n",
    "\n",
    "            # 5. Plot horizontal lines at each percentile\n",
    "            for q, kde_v, style in zip([q25, q50, q75], kde_vals, ['--', '-', '--']):\n",
    "                ax_inset.plot([0, kde_v[0]], [q, q], linestyle=style, color=color, linewidth=1.0)\n",
    "    \n",
    "    list_axis[i].set_ylim(ax_inset.get_ylim())\n",
    "    ax_inset.axis(\"off\")\n",
    "ax[0,1].set_yticks(np.arange(0.12,0.24+0.01,0.03))\n",
    "    \n",
    "for a in ax[0]:  # ax[0] is the first row\n",
    "    a.set_xticklabels([])\n",
    "    a.set_xlabel(\"\")  # Optional: also hide the label\n",
    "    \n",
    "## design axis[0,5] by hand\n",
    "# Dummy data\n",
    "np.random.seed(42)\n",
    "dummy_data = np.random.normal(loc=10, scale=5, size=100)\n",
    "ax_explainer = ax[0, 5]\n",
    "\n",
    "# Plot a simple vertical boxplot\n",
    "bp = ax_explainer.boxplot(dummy_data, vert=True, patch_artist=True, widths=0.6,\n",
    "               boxprops=dict(facecolor='#066555', color='black'),\n",
    "               medianprops=dict(color='black'),\n",
    "               whiskerprops=dict(color='black'),\n",
    "               capprops=dict(color='black'),\n",
    "               flierprops=dict(marker='o', color='black', alpha=0.3))\n",
    "\n",
    "# Clean up axis for better visuals\n",
    "ax_explainer.set_xticks([])\n",
    "ax_explainer.set_yticks([])\n",
    "\n",
    "# Annotation positions\n",
    "Q1 = np.percentile(dummy_data, 25)\n",
    "Q2 = np.median(dummy_data)\n",
    "Q3 = np.percentile(dummy_data, 75)\n",
    "whisker_low = np.min(dummy_data[dummy_data > Q1 - 1.5 * (Q3 - Q1)])\n",
    "whisker_high = np.max(dummy_data[dummy_data < Q3 + 1.5 * (Q3 - Q1)])\n",
    "outlier = max(dummy_data)\n",
    "\n",
    "# Annotate parts\n",
    "ax_explainer.annotate(\"Q1\", xy=(1, Q1), xytext=(0.75, Q1-0.68),\n",
    "                      #arrowprops=dict(arrowstyle=\"->\"),\n",
    "                      fontsize=14, va='center',\n",
    "                      bbox=dict(facecolor='white', edgecolor='white', boxstyle='round,pad=0.01'))\n",
    "ax_explainer.annotate(\"Median\", xy=(1, Q2), xytext=(0.75, Q2),\n",
    "                      #arrowprops=dict(arrowstyle=\"->\"),\n",
    "                      fontsize=14, va='center',\n",
    "                      bbox=dict(facecolor='#066555', edgecolor='#066555', boxstyle='round,pad=0.01'))\n",
    "ax_explainer.annotate(\"Q3\", xy=(1, Q3), xytext=(0.75, Q3+0.68),\n",
    "                      #arrowprops=dict(arrowstyle=\"->\"),\n",
    "                      fontsize=14, va='center',\n",
    "                      bbox=dict(facecolor='white', edgecolor='white', boxstyle='round,pad=0.01'))\n",
    "ax_explainer.annotate(\"Q1 - 1.5xIQR\", xy=(1, whisker_low), xytext=(0.55, whisker_low - 0.9),\n",
    "                      #arrowprops=dict(arrowstyle=\"->\"),\n",
    "                      fontsize=14, va='center',\n",
    "                      bbox=dict(facecolor='white', edgecolor='white', boxstyle='round,pad=0.01'))\n",
    "\n",
    "ax_explainer.annotate(\"Q3 + 1.5xIQR\", xy=(1, whisker_high), xytext=(0.55, whisker_high + 0.75),\n",
    "                      #arrowprops=dict(arrowstyle=\"->\"),\n",
    "                      fontsize=14, va='center',\n",
    "                      bbox=dict(facecolor='white', edgecolor='white', boxstyle='round,pad=0.01'))\n",
    "ax_explainer.set_ylim(-4,22)\n",
    "\"\"\"\n",
    "# Optional: annotate outlier\n",
    "\n",
    "if outlier > whisker_high + 0.2:  # only if clearly visible\n",
    "    ax_explainer.annotate(\"Outlier\", xy=(1, outlier), xytext=(1.2, outlier),\n",
    "                          #arrowprops=dict(arrowstyle=\"->\"),\n",
    "                          fontsize=14, va='center')\n",
    "\"\"\"\n",
    "# Create small top inset for each metric's best cluster\n",
    "ax_inset = inset_axes(ax_explainer, width=\"60%\", height=\"100%\", loc='upper left',\n",
    "                        bbox_to_anchor=(1, 0, 0.6, 1), bbox_transform=ax_explainer.transAxes, borderpad=0) #bbox starts at first two values, + next two values\n",
    "# Dummy data for KDEs (three distributions)\n",
    "dummy_mb = np.random.normal(loc=12, scale=2.5, size=100)\n",
    "dummy_tsla = np.random.normal(loc=22, scale=2.5, size=100)\n",
    "dummy_alb = np.random.normal(loc=2, scale=2.5, size=100)\n",
    "\n",
    "# Plot vertical KDEs\n",
    "#sns.kdeplot(y=dummy_mb, ax=ax_inset, label=\"MB\", fill=True, color=\"#D81B1B\", linewidth=1.1)\n",
    "#sns.kdeplot(y=dummy_tsla, ax=ax_inset, label=\"TSLA\", fill=True, color=\"#A5781B\", linewidth=1.1)\n",
    "#\n",
    "# sns.kdeplot(y=dummy_alb, ax=ax_inset, label=\"ALB\", fill=True, color=\"#1E80E5\", linewidth=1.1)\n",
    "\n",
    "dummy_dic = {\"MB\": dummy_mb, \"TSLA\": dummy_tsla, \"ALB\": dummy_alb}\n",
    "color_dic = {\"MB\": \"#D81B1B\", \"ALB\": \"#1E80E5\", \"TSLA\": \"#A5781B\"}\n",
    "# 3. Compute percentiles\n",
    "for score_key, color in color_dic.items():\n",
    "    q25 = np.percentile(dummy_dic[score_key], 25)\n",
    "    q50 = np.median(dummy_dic[score_key])\n",
    "    q75 = np.percentile(dummy_dic[score_key], 75)\n",
    "    # Plot KDE vertically\n",
    "    kde_line = sns.kdeplot(y=dummy_dic[score_key], ax=ax_inset, fill=True, color=color, label=score_key, linewidth=1.1)\n",
    "\n",
    "    kde_func = gaussian_kde(dummy_dic[score_key])\n",
    "    kde_vals = kde_func(q25), kde_func(q50), kde_func(q75)\n",
    "\n",
    "    # 5. Plot horizontal lines at each percentile\n",
    "    for q, kde_v, style in zip([q25, q50, q75], kde_vals, ['--', '-', '--']):\n",
    "        ax_inset.plot([0, kde_v[0]], [q, q], linestyle=style, color=color, linewidth=1.0)\n",
    "\n",
    "ax_inset.text(0.01, 12, r\"$B_{geod}$\", color=\"#D81B1B\", fontsize=24, fontweight='bold', va='center')\n",
    "ax_inset.text(0.01, 22, r\"$SLA$\", color=\"#A5781B\", fontsize=24, fontweight='bold', va='center')\n",
    "ax_inset.text(0.01, 2, r\"$\\bar{\\alpha}$\", color=\"#1E80E5\", fontsize=24, fontweight='bold', va='center')\n",
    "\n",
    "ax_inset.axis('off')  # Clean look\n",
    "#if 'win' in sys.platform:\n",
    "#    plt.savefig(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/boxplot_distributions_clusters.png\", bbox_inches=\"tight\")\n",
    "#else:\n",
    "#    plt.savefig(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/boxplot_distributions_clusters.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_silhouette = silhouette_score(loglikelihoods_scaled, filtered_data[\"KMeans_Cluster2\"])\n",
    "print(f\"GMM Silhouette Score: {gmm_silhouette:.2f}\")\n",
    "\n",
    "sns.pairplot(filtered_data, hue=\"KMeans_Cluster2\", vars=['rrr_factor','alb_ice','alb_snow','alb_firn','albedo_aging','albedo_depth','roughness_ice','alb_logp','mb_logp','tsla_logp','joint_like'])\n",
    "plt.suptitle(\"Pairplot of Model Parameters by Cluster\", y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute IQR and CI ratios between best performing cluster and all data sets as a measure of spread\n",
    "\n",
    "def compute_cir_iqr_ratios(full_df, best_df, param_features, ci=0.95):\n",
    "    \"\"\"\n",
    "    Compute CIR and IQR ratios for each parameter.\n",
    "\n",
    "    Parameters:\n",
    "    df_all : DataFrame\n",
    "        Full parameter set.\n",
    "    df_top : DataFrame\n",
    "        Subset of best-performing simulations (e.g., top 10%).\n",
    "    ci : float\n",
    "        Credible interval level (default: 0.95).\n",
    "\n",
    "    Returns:\n",
    "    DataFrame with CIR and IQR ratios for each parameter.\n",
    "    \"\"\"\n",
    "    ratios = {}\n",
    "    lower_q = (1 - ci) / 2\n",
    "    upper_q = 1 - lower_q\n",
    "\n",
    "    for col in full_df[param_features].columns:\n",
    "        # CIR\n",
    "        full_ci = np.quantile(full_df[col], upper_q) - np.quantile(full_df[col], lower_q)\n",
    "        top_ci = np.quantile(best_df[col], upper_q) - np.quantile(best_df[col], lower_q)\n",
    "        cir = top_ci / full_ci if full_ci != 0 else np.nan\n",
    "\n",
    "        # IQR Ratio\n",
    "        full_iqr = np.percentile(full_df[col], 75) - np.percentile(full_df[col], 25)\n",
    "        top_iqr = np.percentile(best_df[col], 75) - np.percentile(best_df[col], 25)\n",
    "        iqr_ratio = top_iqr / full_iqr if full_iqr != 0 else np.nan\n",
    "\n",
    "        ratios[col] = {\"CIR\": cir, \"IQR_ratio\": iqr_ratio}\n",
    "\n",
    "    return pd.DataFrame(ratios).T\n",
    "\n",
    "if 'win' in sys.platform:\n",
    "    results = compute_cir_iqr_ratios(filtered_data, filtered_data.loc[filtered_data['KMeans_Cluster2'] == 3], param_features=param_features)\n",
    "else:\n",
    "    results = compute_cir_iqr_ratios(filtered_data, filtered_data.loc[filtered_data['KMeans_Cluster2'] == 3], param_features=param_features)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blabla = results.reset_index()\n",
    "# Dictionary for LaTeX-style labels\n",
    "y_label_dict_short = {\n",
    "    'rrr_factor': r'$p_{f}$', \n",
    "    'alb_ice': r'$\\alpha_{ice}$', \n",
    "    'alb_snow': r'$\\alpha_{fs}$',\n",
    "    'alb_firn': r'$\\alpha_{firn}$', \n",
    "    'albedo_aging': r'$\\alpha_{aging}$',\n",
    "    'albedo_depth': r'$\\alpha_{depth}$',\n",
    "    'roughness_ice': r'$z0_{ice}$'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'Parameter' column in the DataFrame\n",
    "blabla['index'] = blabla['index'].map(y_label_dict)\n",
    "blabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "\n",
    "# CIR Plot\n",
    "sns.barplot(x='CIR', y='index', data=blabla, ax=axs[0], palette='viridis')\n",
    "axs[0].axvline(1.0, color='red', linestyle='--')\n",
    "axs[0].set_title('Credible Interval Ratio')\n",
    "axs[0].set_xlim(0, 1.2)\n",
    "axs[0].set_ylabel(\"\")\n",
    "\n",
    "# IQR Ratio Plot\n",
    "sns.barplot(x='IQR_ratio', y='index', data=blabla, ax=axs[1], palette='plasma')\n",
    "axs[1].axvline(1.0, color='red', linestyle='--')\n",
    "axs[1].set_title('Interquartile Range Ratio')\n",
    "axs[1].set_xlim(0, 1.2)\n",
    "\n",
    "# Layout tweaks\n",
    "plt.tight_layout()\n",
    "#if 'win' in sys.platform:\n",
    "#    plt.savefig(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/cir_iqr_ratios.png\", bbox_inches=\"tight\")\n",
    "#else:\n",
    "#    plt.savefig(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/cir_iqr_ratios.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Investigate loadings\n",
    "loadings = pca.components_.T\n",
    "n_components_to_keep = 6  # based on your earlier comment\n",
    "\n",
    "# Communalities: variance of each parameter explained by the top components\n",
    "communalities = np.sum(loadings[:, :n_components_to_keep]**2, axis=1)\n",
    "communalities_df = pd.DataFrame({\n",
    "    'Parameter': param_features,\n",
    "    'Communality': communalities\n",
    "}).sort_values(by='Communality', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(np.arange(1, len(explained_variances)+1), explained_variances*100, 'o-', linewidth=2)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance (%)')\n",
    "plt.title('Scree Plot')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "loadings_df = pd.DataFrame(loadings[:, :6], columns=[f'PC{i+1}' for i in range(6)], index=param_features)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(loadings_df, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('PCA Loadings for First 6 Components')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communalities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = pd.concat([pca_df, filtered_data[score_columns + ['joint_like']].reset_index(drop=True)], axis=1)\n",
    "# e.g. plot PC1 vs PC2 colored by joint LL\n",
    "corr = df_pca.corr(method=\"spearman\").loc[['PC1','PC2','PC3','PC4','PC5','PC6','PC7'],\n",
    "                         ['mb_logp','tsla_logp','alb_logp','joint_like']]\n",
    "# Step 5: Visualize the correlations\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca.loc[:,'cluster'] = clusters2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newcolors_sorted = [\"gray\", \"#008b8b\", \"#6b8e23\", \"#6a5acd\" ]\n",
    "\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list(\"custom_cmap\", newcolors_sorted, N=256)\n",
    "\n",
    "gradient = np.linspace(0, 1, 256).reshape(1, -1)\n",
    "plt.figure(figsize=(6, 1))\n",
    "plt.imshow(gradient, aspect=\"auto\", cmap=cmap)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Adjusted colormap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = df_pca['joint_like'].min()\n",
    "vmax = df_pca['joint_like'].max()\n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "\n",
    "def plot_pca_axis(axis, data, x_data, y_data, loading_index, ylabel):\n",
    "    \n",
    "    markers = ['o', 's', 'D', 'H']  # expand if needed\n",
    "    unique_clusters = data['cluster'].unique()\n",
    "    \n",
    "    sc = None  # Initialize for scatter return\n",
    "\n",
    "    # Plot each cluster with a different marker\n",
    "    for i, cluster in enumerate(unique_clusters):\n",
    "        print(i, cluster)\n",
    "        subset = data[data['cluster'] == cluster]\n",
    "        sc = axis.scatter(\n",
    "            subset[x_data], subset[y_data],\n",
    "            c=subset['joint_like'], cmap='jet',\n",
    "            marker=marker_map[cluster], #markers[i % len(markers)],\n",
    "            vmin=vmin, vmax=vmax,\n",
    "            edgecolor='k', linewidth=0.5,\n",
    "            label=f'Cluster {cluster}', s=30,\n",
    "            alpha=0.8\n",
    "        )\n",
    "\n",
    "    # Correlation annotation\n",
    "    pc1_corr = corr.loc[x_data]\n",
    "    pc2_corr = corr.loc[y_data]\n",
    "    text = (\n",
    "        f\"{x_data} Corr — \"+r\"$B_{geod}$: \"+f\"{pc1_corr['mb_logp']:.2f}, SLA: {pc1_corr['tsla_logp']:.2f}, \"+r\"$\\bar{\\alpha}$: \"+f\"{pc1_corr['alb_logp']:.2f}, total: {pc1_corr['joint_like']:.2f}\\n\"\n",
    "        f\"{y_data} Corr — \"+r\"$B_{geod}$: \"+f\"{pc2_corr['mb_logp']:.2f}, SLA: {pc2_corr['tsla_logp']:.2f}, \"+r\"$\\bar{\\alpha}$: \"+f\"{pc2_corr['alb_logp']:.2f}, total: {pc2_corr['joint_like']:.2f}\"\n",
    "    )\n",
    "    if axis in [axes[4],axes[5]]:\n",
    "        axis.text(0.01, -0.13, text, transform=axis.transAxes, fontsize=14, va='top')\n",
    "    else:\n",
    "        axis.text(0.01, -0.05, text, transform=axis.transAxes, fontsize=14, va='top')\n",
    "\n",
    "    # Add PCA loadings (scaled for visibility)\n",
    "    for i, param in enumerate(param_features):  # param_names = list of original parameter names\n",
    "        loading_x = pca.components_[0, i]\n",
    "        loading_y = pca.components_[loading_index, i]\n",
    "        axis.arrow(0, 0, loading_x * 3, loading_y * 3, color='black', alpha=1, head_width=0.05)\n",
    "        new_param_name = y_label_dict[param]\n",
    "        axis.text(loading_x * 3., loading_y * 3., param, fontsize=18, ha='center', va='center')\n",
    "\n",
    "    axis.axhline(0, color='black', lw=0.5)\n",
    "    axis.axvline(0, color='black', lw=0.5)\n",
    "    axis.grid(True)\n",
    "\n",
    "    # Labels and legend\n",
    "    #axis.set_xlabel('PC 1')\n",
    "    axis.set_ylabel(ylabel)\n",
    "    axis.axhline(0, color='gray', linestyle='--', linewidth=0.7)\n",
    "    axis.axvline(0, color='gray', linestyle='--', linewidth=0.7)\n",
    "    #axis.legend()\n",
    "    return sc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_cluster = 3\n",
    "\n",
    "# Define the marker shapes\n",
    "highlight_marker = '^'\n",
    "other_markers = ['8', 'p', 'D'] # Circle, Square, Diamond\n",
    "\n",
    "unique_clusters = sorted(df_pca['cluster'].unique())\n",
    "marker_map = {}\n",
    "other_marker_idx = 0\n",
    "for c in unique_clusters:\n",
    "    if c == highlight_cluster:\n",
    "        marker_map[c] = highlight_marker\n",
    "    else:\n",
    "        # Correctly assign a different marker to each of the other clusters\n",
    "        marker_map[c] = other_markers[other_marker_idx % len(other_markers)]\n",
    "        other_marker_idx += 1\n",
    "\n",
    "# calc ll diff\n",
    "max_ll = df_pca['joint_like'].max()\n",
    "df_pca['ll_difference'] = max_ll - df_pca['joint_like']\n",
    "\n",
    "pc_cols_for_dist = [f'PC{i+1}' for i in range(7)] # e.g., ['PC1', 'PC2', ..., 'PC7']\n",
    "\n",
    "best_run_coords = df_pca.loc[df_pca['joint_like'].idxmax(), pc_cols_for_dist].values\n",
    "\n",
    "all_coords = df_pca[pc_cols_for_dist].values\n",
    "\n",
    "# Calculate Mahalanobis distance for each point from the best run\n",
    "inv_cov_matrix = np.diag(1 / pca.explained_variance_[:len(pc_cols_for_dist)])\n",
    "df_pca['param_distance'] = [mahalanobis(point, best_run_coords, inv_cov_matrix) for point in all_coords]\n",
    "\n",
    "# Update global parameters\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "\n",
    "y_label_dict_short = {\n",
    "    'rrr_factor': r'$p_{f}$', \n",
    "    'alb_ice': r'$\\alpha_{ice}$', \n",
    "    'alb_snow': r'$\\alpha_{fs}$',\n",
    "    'alb_firn': r'$\\alpha_{firn}$', \n",
    "    'albedo_aging': r'$\\alpha_{aging}$',\n",
    "    'albedo_depth': r'$\\alpha_{depth}$',\n",
    "    'roughness_ice': r'$z0_{ice}$'\n",
    "    }\n",
    "\n",
    "def plot_pca_axis(axis, data, x_data, y_data, loading_index, ylabel, marker_map,\n",
    "                  color_mode='log_likelihood', highlight_cluster=None):\n",
    "    \n",
    "    # This logic block correctly defines the variables to use\n",
    "    if color_mode == 'll_difference':\n",
    "        color_col = 'll_difference'\n",
    "        cmap = 'inferno_r'\n",
    "        cbar_label = 'Log-Likelihood Difference from Best'\n",
    "        vmin, vmax = 0, data[color_col].quantile(0.99)\n",
    "    elif color_mode == 'param_distance':\n",
    "        color_col = 'param_distance'\n",
    "        cmap = 'inferno_r'\n",
    "        cbar_label = 'Parameter Distance from Best Run'\n",
    "        vmin, vmax = 0, data[color_col].quantile(0.99)\n",
    "    else: # Default to original 'log_likelihood'\n",
    "        color_col = 'joint_like'\n",
    "        cmap = 'jet'\n",
    "        cbar_label = 'Joint Log-Likelihood'\n",
    "        vmin, vmax = data[color_col].min(), data[color_col].max()\n",
    "\n",
    "    sc = None\n",
    "    \n",
    "    clusters_to_plot = sorted(data['cluster'].unique())\n",
    "    if highlight_cluster is not None and highlight_cluster in clusters_to_plot:\n",
    "        clusters_to_plot.remove(highlight_cluster)\n",
    "        clusters_to_plot.append(highlight_cluster)\n",
    "\n",
    "    for cluster in clusters_to_plot:\n",
    "        subset = data[data['cluster'] == cluster]\n",
    "        is_highlight = (cluster == highlight_cluster)\n",
    "        \n",
    "        if is_highlight:\n",
    "            marker_size, edge_color, line_w, z_order, alpha = 60, 'black', 0.8, 10, 0.9\n",
    "        else:\n",
    "            marker_size, edge_color, line_w, z_order, alpha = 40, 'grey', 0.6, 5, 0.7\n",
    "\n",
    "        sc = axis.scatter(\n",
    "            subset[x_data], subset[y_data],\n",
    "            c=subset[color_col],\n",
    "            cmap=cmap,\n",
    "            marker=marker_map.get(cluster, 'x'),\n",
    "            s=marker_size,\n",
    "            vmin=vmin, vmax=vmax,\n",
    "            edgecolor=edge_color,\n",
    "            linewidth=line_w,\n",
    "            zorder=z_order,\n",
    "            alpha=alpha\n",
    "        )\n",
    "    \n",
    "    # Correlation annotation\n",
    "    var_x = pca.explained_variance_ratio_[0]  # x_data is always PC1\n",
    "    var_y = pca.explained_variance_ratio_[loading_index]\n",
    "    \n",
    "    x_loadings = pca.components_[0]\n",
    "    top_x_indices = np.argsort(np.abs(x_loadings))[-3:] # Get indices of 2 largest absolute loadings\n",
    "    top_x_params = [\n",
    "    f\"{y_label_dict_short.get(param_features[i], param_features[i])} ({x_loadings[i]:+.2f})\" \n",
    "    for i in top_x_indices\n",
    "    ]\n",
    "\n",
    "    y_loadings = pca.components_[loading_index]\n",
    "    top_y_indices = np.argsort(np.abs(y_loadings))[-3:] # Get indices of 2 largest absolute loadings\n",
    "    top_y_params = [\n",
    "        f\"{y_label_dict_short.get(param_features[i], param_features[i])} ({y_loadings[i]:+.2f})\" \n",
    "        for i in top_y_indices\n",
    "    ]\n",
    "    text = (\n",
    "        f\"{x_data} (Var: {var_x:.1%}) | Loadings: {', '.join(top_x_params)}\\n\"\n",
    "        f\"{y_data} (Var: {var_y:.1%}) | Loadings: {', '.join(top_y_params)}\"\n",
    "    )\n",
    "    if axis in [axes[4],axes[5]]:\n",
    "        axis.text(0.01, -0.13, text, transform=axis.transAxes, fontsize=13, va='top')\n",
    "    else:\n",
    "        axis.text(0.01, -0.05, text, transform=axis.transAxes, fontsize=13, va='top')\n",
    "\n",
    "\n",
    "    text_effect = [path_effects.Stroke(linewidth=2, foreground='white'), path_effects.Normal()]\n",
    "    padding = 0.05  # Adjust this value to control the gap between the arrow and the text\n",
    "\n",
    "    for i, param in enumerate(param_features):\n",
    "        loading_x = pca.components_[0, i] * 3  # scale by 3\n",
    "        loading_y = pca.components_[loading_index, i] * 3\n",
    "        \n",
    "        arrow = axis.arrow(0, 0, loading_x, loading_y, color='black', alpha=1, \n",
    "                head_width=0.07, zorder=11, length_includes_head=True)\n",
    "        arrow.set_path_effects(text_effect)\n",
    "\n",
    "        # Determine horizontal alignment\n",
    "        if loading_x > 0:\n",
    "            ha = 'left'\n",
    "            text_x = loading_x + padding\n",
    "        else:\n",
    "            ha = 'right'\n",
    "            text_x = loading_x - padding\n",
    "\n",
    "        # Determine vertical alignment\n",
    "        if loading_y > 0:\n",
    "            va = 'bottom'\n",
    "            text_y = loading_y + padding\n",
    "        else:\n",
    "            va = 'top'\n",
    "            text_y = loading_y - padding\n",
    "\n",
    "        # Place the text with the new alignment and padded coordinates\n",
    "        txt = axis.text(text_x, text_y, y_label_dict_short[param], fontsize=20, \n",
    "                        ha=ha, va=va, zorder=12)\n",
    "        txt.set_path_effects(text_effect)\n",
    "\n",
    "    axis.set_ylabel(ylabel)\n",
    "    axis.axhline(0, color='gray', linestyle='--', linewidth=0.7)\n",
    "    axis.axvline(0, color='gray', linestyle='--', linewidth=0.7)\n",
    "\n",
    "    axis.axhline(0, color='black', lw=0.5)\n",
    "    axis.axvline(0, color='black', lw=0.5)\n",
    "    axis.grid(True)\n",
    "    axis.set_ylabel(ylabel)\n",
    "    \n",
    "    return sc, cbar_label # Return the colorbar label as well\n",
    "\n",
    "\n",
    "# choose here 'log_likelihood', 'll_difference', or 'param_distance'\n",
    "selected_color_mode = 'll_difference' \n",
    "\n",
    "fig, axes = plt.subplots(3, 2, dpi=300, figsize=(16, 12), sharex=True, constrained_layout=True)\n",
    "axes = axes.flatten()\n",
    "#plt.subplots_adjust(left=0.05, right=0.85, top=0.95, bottom=0.1, hspace=0.2, wspace=0.2)\n",
    "\n",
    "# Call the plotting function with the selected mode\n",
    "sc, cbar_label = plot_pca_axis(axes[0], df_pca, \"PC1\", \"PC2\", 1, \"PC 2\", marker_map=marker_map, color_mode=selected_color_mode, highlight_cluster=3)\n",
    "plot_pca_axis(axes[1], df_pca, \"PC1\", \"PC3\", 2, \"PC 3\", marker_map=marker_map, color_mode=selected_color_mode, highlight_cluster=3)\n",
    "plot_pca_axis(axes[2], df_pca, \"PC1\", \"PC4\", 3, \"PC 4\", marker_map=marker_map, color_mode=selected_color_mode, highlight_cluster=3)\n",
    "plot_pca_axis(axes[3], df_pca, \"PC1\", \"PC5\", 4, \"PC 5\", marker_map=marker_map, color_mode=selected_color_mode, highlight_cluster=3)\n",
    "plot_pca_axis(axes[4], df_pca, \"PC1\", \"PC6\", 5, \"PC 6\", marker_map=marker_map, color_mode=selected_color_mode, highlight_cluster=3)\n",
    "sc, cbar_label = plot_pca_axis(axes[5], df_pca, \"PC1\", \"PC7\", 6, \"PC 7\", marker_map=marker_map, color_mode=selected_color_mode, highlight_cluster=3) # Capture last one for colorbar\n",
    "\n",
    "# Add colorbar using the dynamic label\n",
    "cbar = fig.colorbar(sc, ax=axes.ravel().tolist(), location='right', pad=0.03, aspect=40)\n",
    "cbar.set_label(cbar_label)\n",
    "\n",
    "highlight_cluster = 3\n",
    "highlight_marker = '^'\n",
    "other_markers = ['8', 'p', 'D']\n",
    "unique_clusters = sorted(df_pca['cluster'].unique())\n",
    "marker_map = {}\n",
    "other_marker_idx = 0\n",
    "for c in unique_clusters:\n",
    "    if c == highlight_cluster:\n",
    "        marker_map[c] = highlight_marker\n",
    "    else:\n",
    "        marker_map[c] = other_markers[other_marker_idx % len(other_markers)]\n",
    "        other_marker_idx += 1\n",
    "\n",
    "# Create a legend handle for each unique cluster\n",
    "legend_handles = [\n",
    "    mlines.Line2D([], [], color='black' if c == highlight_cluster else 'grey',\n",
    "                  marker=marker_map[c],\n",
    "                  linestyle='None', markersize=9, \n",
    "                  label=f'Cluster {c+1}{\" (Best)\" if c == 42 else \"\"}')\n",
    "    for c in unique_clusters\n",
    "]\n",
    "\n",
    "# Place the legend\n",
    "fig.legend(handles=legend_handles,\n",
    "           loc='lower center',\n",
    "           bbox_to_anchor=(0.5, -0.07),\n",
    "           ncol=4, # Display all 4 side-by-side\n",
    "           frameon=False)\n",
    " \n",
    "\n",
    "if 'win' in sys.platform:\n",
    "    plt.savefig(\"E:/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/FigS14_pca_loadings_colored.png\", bbox_inches=\"tight\")\n",
    "else:\n",
    "    plt.savefig(\"/mnt/C4AEBBABAEBB9500/OneDrive/PhD/PhD/Data/Hintereisferner/Figures/FigS14_pca_loadings_colored.png\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estimate additional error term based on residuals for MCMC later\n",
    "subset = filtered_data.loc[filtered_data['KMeans_Cluster2'] == 3]\n",
    "subset['mb_res'] = subset['mb'] - geod_ref['dmdtda'].item()\n",
    "\n",
    "#\n",
    "sim_columns = [col for col in subset.columns if col.startswith('sim')]\n",
    "snowline_sims = subset[sim_columns]\n",
    "obs = tsla_true_obs['TSL_normalized']\n",
    "assert snowline_sims.shape[1] == len(obs), \"Mismatch in number of time steps!\"\n",
    "# Calculate residuals (modelled - observed)\n",
    "residuals = np.abs(snowline_sims - obs.values)\n",
    "# Split into seasons (assume the index is datetime)\n",
    "residuals.columns = tsla_true_obs.index\n",
    "is_summer = residuals.columns.month.isin([6, 7, 8, 9])\n",
    "residuals_summer = residuals.loc[:, is_summer]\n",
    "residuals_winter = residuals.loc[:, ~is_summer]\n",
    "# Albedo\n",
    "residuals_alb = []\n",
    "subset_alb_list = [list_sim_alb[x] for x in subset.index]\n",
    "\n",
    "[residuals_alb.append(x - alb_obs_data.median_albedo.values) for x in subset_alb_list]\n",
    "df_albedo_res = pd.DataFrame(residuals_alb)\n",
    "df_abs_albedo_res = df_albedo_res.abs()\n",
    "time_index = pd.to_datetime(alb_obs_data.coords['time'].values)\n",
    "df_abs_albedo_res.columns = time_index\n",
    "\n",
    "is_summer = df_abs_albedo_res.columns.month.isin([6, 7, 8, 9])\n",
    "\n",
    "residuals_albedo_summer = df_abs_albedo_res.loc[:, is_summer]\n",
    "residuals_albedo_winter = df_abs_albedo_res.loc[:, ~is_summer]\n",
    "df_abs_albedo_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create residual plot and look at e.g., standard deviation\n",
    "#Create mean seasonal residuals\n",
    "alb_mean_summer_residual = residuals_albedo_summer.mean(axis=1)\n",
    "alb_mean_winter_residual = residuals_albedo_winter.mean(axis=1)\n",
    "tsla_mean_summer_residual = residuals_summer.mean(axis=1)\n",
    "tsla_mean_winter_residual = residuals_winter.mean(axis=1)\n",
    "mb_mean_residual = subset['mb_res'].copy()\n",
    "mb_mean_residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create histograms\n",
    "fig, ax = plt.subplots(1,3, figsize=(16,9), dpi=300)\n",
    "ax[0].hist(alb_mean_summer_residual, label=\"Summer\", edgecolor=\"black\", alpha=0.7)\n",
    "ax[0].hist(alb_mean_winter_residual, label=\"Winter\", edgecolor=\"black\", alpha=0.7)\n",
    "#\n",
    "ax[1].hist(tsla_mean_summer_residual, label=\"Summer\", edgecolor=\"black\", alpha=0.7)\n",
    "ax[1].hist(tsla_mean_winter_residual, label=\"Winter\", edgecolor=\"black\", alpha=0.7)\n",
    "ax[1].legend()\n",
    "#\n",
    "ax[2].hist(mb_mean_residual, edgecolor=\"black\", alpha=0.7)\n",
    "\n",
    "print(\"Prior scales for systematic uncertainty term in MCMC\")\n",
    "print(\"Prior sigma for mass balance: \", mb_mean_residual.std())\n",
    "print(\"--------------------\")\n",
    "print(\"Prior sigma for summer TSLA: \", tsla_mean_summer_residual.std())\n",
    "print(\"Prior sigma for winter TSLA: \", tsla_mean_winter_residual.std())\n",
    "print(\"--------------------\")\n",
    "print(\"Prior sigma for summer ALB: \", alb_mean_summer_residual.std())\n",
    "print(\"Prior sigma for winter ALB: \", alb_mean_winter_residual.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Based on CRI and IQR ratio, we set constrained priors (IQR range) for albedo aging, depth, precipitation factor while we keep the other parameters wide.\n",
    "blabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_truncnorm_with_iqr(df, iqr_cols):\n",
    "    trunc_params = {}\n",
    "    distr_dict = {}\n",
    "    \n",
    "    # Precompute IQR bounds for specified columns\n",
    "    q025 = df.quantile(0.025)\n",
    "    q975 = df.quantile(0.975)\n",
    "\n",
    "    for col in df.columns:\n",
    "        mu = df[col].mean()\n",
    "        sigma = df[col].std(ddof=1)\n",
    "        if col in iqr_cols:\n",
    "            #lower, upper = df[col].min(), df[col].max()\n",
    "            #lower, upper = q25[col], q75[col]\n",
    "            lower, upper = q025[col], q975[col]\n",
    "        else:\n",
    "            #lower, upper = df[col].min(), df[col].max()\n",
    "            lower, upper = q025[col], q975[col]\n",
    "        \n",
    "        # Standardize bounds\n",
    "        a, b = (lower - mu) / sigma, (upper - mu) / sigma\n",
    "        \n",
    "        # Store parameters and create distribution\n",
    "        distr_dict[col] = (mu, sigma, lower, upper)\n",
    "        trunc_params[col] = truncnorm(a=a, b=b, loc=mu, scale=sigma)\n",
    "    \n",
    "    return trunc_params, distr_dict\n",
    "\n",
    "mcmc_df = subset[[\"rrr_factor\",\"alb_ice\",\"alb_snow\",\"alb_firn\",\n",
    "                  \"albedo_aging\",\"albedo_depth\",\"roughness_ice\"]]\n",
    "\n",
    "# Columns for which to use IQR-based bounds\n",
    "iqr_cols = [\"rrr_factor\", \"albedo_aging\", \"albedo_depth\"]\n",
    "\n",
    "trunc_params, trunc_dists  = fit_truncnorm_with_iqr(mcmc_df, iqr_cols)\n",
    "\n",
    "trunc_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_truncnorm(mcmc_df, trunc_dists):\n",
    "    fig, axes = plt.subplots(len(mcmc_df.columns), 1, figsize=(6, 3 * len(mcmc_df.columns)))\n",
    "\n",
    "    for i, col in enumerate(mcmc_df.columns):\n",
    "        \n",
    "        mu, sigma, lower, upper = trunc_dists[col]\n",
    "            \n",
    "        #mu = mcmc_df[col].mean()\n",
    "        #sigma = mcmc_df[col].std()\n",
    "        if col in [\"alb_ice\", \"alb_snow\", \"alb_firn\"]:\n",
    "            sigma = 0.1\n",
    "        elif col in [\"roughness_ice\"]:\n",
    "            sigma = 9\n",
    "        #lower, upper = mcmc_df[col].min(), mcmc_df[col].max()\n",
    "        # Convert bounds to standard normal space\n",
    "        a, b = (lower - mu) / sigma, (upper - mu) / sigma\n",
    "        \n",
    "        # Generate samples\n",
    "        x = np.linspace(lower, upper, 100)\n",
    "        func = truncnorm(a, b, loc=mu, scale=sigma)\n",
    "        trunc_dists[col] = (mu, sigma, lower, upper)\n",
    "        pdf = func.pdf(x)\n",
    "        \n",
    "        # Plot histogram of actual data\n",
    "        sns.histplot(mcmc_df[col], kde=False, bins=30, stat=\"density\", alpha=0.5, ax=axes[i], color=\"gray\", label=\"Observed Data\")\n",
    "\n",
    "        # Plot fitted truncated normal distribution\n",
    "        axes[i].plot(x, pdf, 'r-', lw=2, label=f'Trunc. Normal Fit\\nμ={mu:.2f}, σ={sigma:.2f}')\n",
    "        \n",
    "        axes[i].set_title(f\"Truncated Normal Fit: {col}\")\n",
    "        axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the function\n",
    "plot_truncnorm(mcmc_df, trunc_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(trunc_dists.keys()):\n",
    "    print(\"Variable: \", key)\n",
    "    print(\"Mu: \", trunc_dists[key][0])\n",
    "    print(\"Sigma: \",trunc_dists[key][1])\n",
    "    print(\"Lower: \",trunc_dists[key][2])\n",
    "    print(\"Upper: \",trunc_dists[key][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
